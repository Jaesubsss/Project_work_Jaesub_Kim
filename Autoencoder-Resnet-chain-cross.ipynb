{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ce831d-3d8b-4bdf-9d5c-8f96227a2b8c",
   "metadata": {},
   "source": [
    "# Package & Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9dff727-3bc9-4647-9d03-3aa3bdeb368e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "from torchmetrics import MeanSquaredError\n",
    "from sklearn.impute import KNNImputer\n",
    "import scripts\n",
    "from functools import lru_cache\n",
    "import optuna\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import warnings\n",
    "from torch.utils.data import Dataset\n",
    "from datetime import datetime\n",
    "import uuid \n",
    "import time\n",
    "# 경고 무시 설정\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6edd50e5-91f8-4df6-8d58-e5e712430d34",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dataset for sim_matrix\n",
    "class OmicsDataset_dict(Dataset): \n",
    "    def __init__(self, omic_dict, drug_dict, data): \n",
    "        self.omic_dict = omic_dict\n",
    "        self.drug_dict = drug_dict\n",
    "        self.cell_mapped_ids = {key:i for i, key in enumerate(self.omic_dict.keys())}\n",
    "        # omic_dict의 키를 고유한 인덱스로 매핑\n",
    "        # enumerate는 키들을 순서대로 열거하여 (인덱스, 키) 형태의 튜플로 반환\n",
    "        # 딕셔너리 컴프레헨션: 각 키를 key로, 각 키의 인덱스를 i로 사용하여 {key:i}형태로 매핑된 딕셔너리 만듬.\n",
    "        self.drug_mapped_ids = {key:i for i, key in enumerate(self.drug_dict.keys())}\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx): # idx = train_data\n",
    "        instance = self.data.iloc[idx] \n",
    "        cell_id = instance.iloc[0]\n",
    "        drug_id = instance.iloc[1]\n",
    "        target = instance.iloc[2]\n",
    "        \n",
    "        #omics_data = { # usage of dictionary here causes a problem or crash with collate_fn function in Dataloader \n",
    "        #    cell_id : {\n",
    "        #        data_type: self.omic_dict[cell_id][data_type] for data_type in self.omic_dict[cell_id].keys()\n",
    "        #    }\n",
    "        #}\n",
    "        \n",
    "        return (torch.cat([self.omic_dict[cell_id][modality] for modality in self.omic_dict[cell_id].keys()]), \n",
    "                self.drug_dict[drug_id],\n",
    "                torch.Tensor([target]),\n",
    "                torch.Tensor([self.cell_mapped_ids[cell_id]]),\n",
    "                torch.Tensor([self.drug_mapped_ids[drug_id]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deaf58b2-acde-4a0b-92c0-c32057378a9e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# MDA model\n",
    "class MultimodalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dims,  hidden_dim_encoders = 150, embed_dim = 75, fusion_dim = 150, dropout_encoders = 0.2):\n",
    "        # get input as a dictionary\n",
    "        super(MultimodalAutoencoder, self).__init__()\n",
    "        # EEEEEEEEEEncoder\n",
    "        self.input_dims = input_dims\n",
    "        self.num_modalities = len(input_dims)\n",
    "        self.do = nn.Dropout(dropout_encoders)\n",
    "\n",
    "        self.omics_encoder = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(mod_input_dim, hidden_dim_encoders), # input \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim_encoders, embed_dim) # encoder hidden layer: 150, 75 as the value from the paper. so we start from this \n",
    "            )                                 # I dont get why they used 150, 75 for dimension, but we can tune it later\n",
    "            for mod_input_dim in self.input_dims\n",
    "        ])\n",
    "        # fused latent feature \n",
    "        self.fusion_layer = nn.Sequential( # I think we need a fusion layer here, to combine the data modalities\n",
    "            nn.Linear(embed_dim * self.num_modalities, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fusion_dim, embed_dim) # This concatenate latent features of all omics data, and fusion them and make its dim final latent dim\n",
    "        )                                     # This is the only way I can think of to fuse omics data\n",
    "        # decoder\n",
    "        self.omics_decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim_encoders),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_encoders, sum(input_dims))\n",
    "        )\n",
    "        # I actually dont understand this step in paper. they said that decoder has symmetric structure as encoder,\n",
    "        # but the data after MDA they provided, has weird dimension(363x90) which makes no sense. this is the point that i cant understand\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_features = [] # get dictionary as an input \n",
    "        start_idx = 0\n",
    "        for i, encoder in enumerate(self.omics_encoder):\n",
    "            mod_input_dim = self.input_dims[i]\n",
    "            x_modality = x[:, start_idx:start_idx + mod_input_dim]\n",
    "            latent_features.append(encoder(self.do(x_modality)))\n",
    "            start_idx += mod_input_dim\n",
    "            \n",
    "        latent_fused = torch.cat(latent_features, dim=1)\n",
    "        latent_final = self.fusion_layer(latent_fused)\n",
    "        decoded = self.omics_decoder(latent_final)\n",
    "        return decoded, latent_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d90f4c86-c210-4d9e-a83b-a8cddf993ace",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Main resnet model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, embed_dim=256, hidden_dim=1024, dropout=0.1, n_layers=6, norm=\"layernorm\"):\n",
    "        super().__init__()\n",
    "        self.mlps = nn.ModuleList()\n",
    "        \n",
    "        # Determine normalization layer\n",
    "        if norm == \"layernorm\":\n",
    "            norm_layer = nn.LayerNorm\n",
    "        elif norm == \"batchnorm\":\n",
    "            norm_layer = nn.BatchNorm1d\n",
    "        else:\n",
    "            norm_layer = nn.Identity\n",
    "        \n",
    "        # Create MLP layers\n",
    "        for _ in range(n_layers):\n",
    "            self.mlps.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(embed_dim, hidden_dim),\n",
    "                    norm_layer(hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(hidden_dim, embed_dim)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.lin = nn.Linear(embed_dim, 1)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.mlps:\n",
    "            \n",
    "            x = (layer(x) + x) / 2  # Residual connection\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "class Main_model(nn.Module):\n",
    "    def __init__(self, embed_dim=256, hidden_dim=1024, dropout=0.1, n_layers=6, norm=\"layernorm\", \n",
    "                 dropout_omics=0.4, dropout_omics_finetuning=0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ResNet as backbone\n",
    "        self.resnet = ResNet(embed_dim, hidden_dim, dropout, n_layers, norm)\n",
    "        \n",
    "        # Modified embed_d: Two-layer MLP with dropout after ReLU\n",
    "        self.embed_d = nn.Sequential(\n",
    "            nn.LazyLinear(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Modified embed_c: Two-layer MLP with dropout after ReLU and before first Linear Layer\n",
    "        self.embed_c = nn.Sequential(\n",
    "            nn.Dropout(dropout_omics_finetuning),\n",
    "            nn.LazyLinear(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_omics),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, c, d):\n",
    "        # Combine embedded inputs and pass through ResNet\n",
    "        return self.resnet(self.embed_d(d) + self.embed_c(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73df737-dd85-452a-9b4b-1ffd3dc25eb9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# get Data function\n",
    "@lru_cache(maxsize=None)\n",
    "def get_data_corr(n_fold = 0, fp_radius = 2, transform_into_corr = True, typ = [\"rnaseq\", \"mutations\", \"cnvs\"],\n",
    "                  #reconstructed = None\n",
    "                 ):\n",
    "    # drug\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    \n",
    "    # loading all datasets\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    \n",
    "    proteomics = pd.read_csv(\"data/proteomics.csv\", index_col=0)\n",
    "    \n",
    "    mutation = pd.read_csv(\"data/binary_mutations.csv\")\n",
    "    mutation.columns = mutation.iloc[0]\n",
    "    mutation = mutation.iloc[2:,:].set_index(\"gene_symbol\")\n",
    "    driver_columns = mutation.columns.isin(driver_genes)\n",
    "    filtered_mut = mutation.loc[:, driver_columns]\n",
    "    filtered_mut = filtered_mut.astype(float)\n",
    "\n",
    "    methylations = pd.read_csv(\"data/methylations.csv\",index_col = 0).sort_index(ascending = True)\n",
    "\n",
    "    cnvs = pd.read_csv(\"data/copy_number_variations.csv\",index_col= 0)\n",
    "\n",
    "    # concatenate all dataset \n",
    "    # inner join based on index: model_ids with NaN are automatically filtered out \n",
    "    data_concat = pd.concat([filtered_rna, proteomics, filtered_mut, methylations, cnvs], axis=1, join='inner')\n",
    "    \n",
    "    \n",
    "    # Filter data by common indices in all modalities\n",
    "    filtered_rna = filtered_rna[filtered_rna.index.isin(data_concat.index)]\n",
    "    proteomics = proteomics[proteomics.index.isin(data_concat.index)]\n",
    "    filtered_mut = filtered_mut[filtered_mut.index.isin(data_concat.index)]\n",
    "    methylations = methylations[methylations.index.isin(data_concat.index)]\n",
    "    cnvs = cnvs[cnvs.index.isin(data_concat.index)]\n",
    "    \n",
    "    # Initialize cell_dict\n",
    "    cell_dict = {}\n",
    "\n",
    "    if not transform_into_corr : #and reconstructed is None:\n",
    "\n",
    "        dims = []\n",
    "        if \"rnaseq\" in typ:\n",
    "            dims.append(filtered_rna.shape[1])\n",
    "        if \"proteomics\" in typ:\n",
    "            dims.append(proteomics.shape[1])\n",
    "        if \"mutations\" in typ:\n",
    "            dims.append(filtered_mut.shape[1])\n",
    "        if \"methylations\" in typ:\n",
    "            dims.append(methylations.shape[1])\n",
    "        if \"cnvs\" in typ:\n",
    "            dims.append(cnvs.shape[1])\n",
    "        \n",
    "        for cell in data_concat.index:\n",
    "            # Initialize a sub-dictionary for each cell\n",
    "            concatenated_data = []\n",
    "            \n",
    "            # Add data for each type specified in typ\n",
    "            if \"rnaseq\" in typ:\n",
    "                concatenated_data.append(filtered_rna.loc[cell].to_numpy())\n",
    "            if \"proteomics\" in typ:\n",
    "                concatenated_data.append(proteomics.loc[cell].to_numpy())\n",
    "            if \"mutations\" in typ:\n",
    "                concatenated_data.append(filtered_mut.loc[cell].to_numpy())\n",
    "            if \"methylations\" in typ:\n",
    "                concatenated_data.append(methylations.loc[cell].to_numpy())\n",
    "            if \"cnvs\" in typ:\n",
    "                concatenated_data.append(cnvs.loc[cell].to_numpy())\n",
    "\n",
    "            cell_dict[cell] = torch.Tensor(np.concatenate(concatenated_data))\n",
    "            \n",
    "#    if reconstructed is not None:\n",
    "#        for cell_idx, cell in enumerate(data_concat.index):\n",
    "#            # cell_dict에 reconstructed 텐서의 각 행(cell 데이터) 저장\n",
    "#            cell_dict[cell] = reconstructed[cell_idx]\n",
    "        \n",
    "\n",
    "    # GDSC\n",
    "    GDSC1 = pd.read_csv(\"data/GDSC1.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = GDSC1.query(\"SANGER_MODEL_ID in @data_concat.index & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "\n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    #test_lines = folds[0] \n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold] \n",
    "\n",
    "        # no change needed, query works fine with some missing\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "    \n",
    "    if transform_into_corr: #and reconstructed is None:\n",
    "        # train, val, test among filtered data\n",
    "        # these are valid train_, val_ and test_data index\n",
    "        \n",
    "        \n",
    "        n_train = len(train_lines)  \n",
    "        n_val = len(validation_lines)      \n",
    "        n_test = len(test_lines)\n",
    "        \n",
    "        # Precompute similarity matrices for each data type\n",
    "        similarity_matrices = {}\n",
    "        dims = []\n",
    "        \n",
    "        if \"rnaseq\" in typ:\n",
    "            exp_com = np.corrcoef(np.vstack([filtered_rna.loc[train_lines], \n",
    "                                             filtered_rna.loc[validation_lines], \n",
    "                                             filtered_rna.loc[test_lines]]), rowvar=True)\n",
    "            train = exp_com[:n_train, :n_train]\n",
    "            val = exp_com[n_train:n_train+n_val, :n_train]\n",
    "            test = exp_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"rnaseq\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"rnaseq\"][0]))\n",
    "        \n",
    "        if \"proteomics\" in typ:\n",
    "            prot_com = np.corrcoef(np.vstack([proteomics.loc[train_lines], \n",
    "                                              proteomics.loc[validation_lines], \n",
    "                                              proteomics.loc[test_lines]]), rowvar=True)\n",
    "            train = prot_com[:n_train, :n_train]\n",
    "            val = prot_com[n_train:n_train+n_val, :n_train]\n",
    "            test = prot_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"proteomics\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"proteomics\"][0]))\n",
    "        \n",
    "        if \"mutations\" in typ:\n",
    "            train_snp = filtered_mut.loc[train_lines].astype(bool)\n",
    "            val_snp = filtered_mut.loc[validation_lines].astype(bool)\n",
    "            test_snp = filtered_mut.loc[test_lines].astype(bool)\n",
    "            \n",
    "            train = 1 - pairwise_distances(train_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            val = 1 - pairwise_distances(val_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            test = 1 - pairwise_distances(test_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "    \n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"mutations\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"mutations\"][0]))\n",
    "        \n",
    "        if \"methylations\" in typ:\n",
    "            methyl_com = np.nan_to_num(np.corrcoef(np.vstack([methylations.loc[train_lines], \n",
    "                                                methylations.loc[validation_lines], \n",
    "                                                methylations.loc[test_lines]]), rowvar=True))\n",
    "            train = methyl_com[:n_train, :n_train]\n",
    "            val = methyl_com[n_train:n_train+n_val, :n_train]\n",
    "            test = methyl_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"methylations\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"methylations\"][0]))\n",
    "        \n",
    "        if \"cnvs\" in typ:\n",
    "            cnv_com = np.nan_to_num(np.corrcoef(np.vstack([cnvs.loc[train_lines], # nan-generation problem fixed \n",
    "                                             cnvs.loc[validation_lines], \n",
    "                                             cnvs.loc[test_lines]]), rowvar=True))\n",
    "            train= cnv_com[:n_train, :n_train]\n",
    "            val= cnv_com[n_train:n_train+n_val, :n_train]\n",
    "            test= cnv_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"cnvs\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"cnvs\"][0]))\n",
    "            \n",
    "        cell_dict = {}\n",
    "\n",
    "        # \n",
    "        for cell in unique_cell_lines:\n",
    "            cell_dict[cell] = {}\n",
    "            for data_type in typ:\n",
    "                sim_matrices = similarity_matrices[data_type]\n",
    "                sim_tensor = torch.Tensor(sim_matrices)\n",
    "                cell_idx = np.where(unique_cell_lines == cell)[0][0]\n",
    "                cell_dict[cell][data_type] = sim_tensor[cell_idx]\n",
    "    \n",
    "        return (OmicsDataset_dict(cell_dict, drug_dict, train_data),\n",
    "        OmicsDataset_dict(cell_dict, drug_dict, validation_data),\n",
    "        OmicsDataset_dict(cell_dict, drug_dict, test_data),\n",
    "        dims)\n",
    "\n",
    "    return (scripts.OmicsDataset(cell_dict, drug_dict, train_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, validation_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, test_data),\n",
    "    dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e757bd2-4119-456e-aaac-944a4f56547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_train_step(autoencoder, resnet, lamda, optimizer, loader, config, device):\n",
    "    \"\"\"\n",
    "    Autoencoder와 ResNet을 동시에 학습하는 End-to-End Training Step\n",
    "    \"\"\"\n",
    "\n",
    "    # 손실 함수 정의\n",
    "    loss_fn_recon = nn.MSELoss()  # Autoencoder의 Reconstruction Loss\n",
    "    loss_fn_pred = nn.MSELoss()   # ResNet의 Prediction Loss\n",
    "    ls = []  # ResNet의 Loss 저장\n",
    "    ls_recon = []  # Autoencoder의 Reconstruction Loss 저장\n",
    "\n",
    "    autoencoder.train()  # Autoencoder 학습 모드\n",
    "    resnet.train()  # ResNet 학습 모드\n",
    "    \n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1️⃣ Autoencoder Forward (Latent Feature 생성)\n",
    "        reconstructed, latent_features = autoencoder(batch[0].to(device))  # (batch_size, latent_dim)\n",
    "        \n",
    "        # 2️⃣ ResNet Forward (Latent Feature + Drug 데이터)\n",
    "        out = resnet(latent_features, batch[1].to(device))\n",
    "\n",
    "        # 3️⃣ Loss 계산\n",
    "        loss_recon = loss_fn_recon(reconstructed, batch[0].to(device))  # Autoencoder Loss\n",
    "        loss_pred = loss_fn_pred(out.squeeze(), batch[2].to(device).squeeze())  # ResNet Loss\n",
    "\n",
    "        # 4️⃣ Total Loss = Reconstruction Loss + Prediction Loss\n",
    "        loss = lamda*loss_recon + (1-lamda)*loss_pred\n",
    "\n",
    "        # 5️⃣ Backpropagation\n",
    "        loss.backward() # 두 losses를 모두 backpropagation한다.\n",
    "        torch.nn.utils.clip_grad_norm_(list(autoencoder.parameters()) + list(resnet.parameters()), config[\"optimizer\"][\"clip_norm\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        # 6️⃣ Loss 저장\n",
    "        ls.append(loss_pred.item())\n",
    "        ls_recon.append(loss_recon.item())\n",
    "\n",
    "    return np.mean(ls), np.mean(ls_recon)\n",
    "\n",
    "\n",
    "def train_auto_resnet_chain(config, train_dataset, lamda=0.5, validation_dataset=None, use_momentum=True, callback_epoch=None):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "        drop_last=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    if validation_dataset is not None:\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            validation_dataset,\n",
    "            batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "            drop_last=False,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    autoencoder = MultimodalAutoencoder(\n",
    "        input_dims=input_dims,\n",
    "        hidden_dim_encoders=config[\"model\"][\"hidden_dim_encoders\"],\n",
    "        embed_dim=config[\"model\"][\"embed_dim\"],\n",
    "        fusion_dim=config[\"model\"][\"fusion_dim\"],\n",
    "        dropout_encoders=config[\"model\"][\"dropout_encoders\"]\n",
    "    )\n",
    "\n",
    "    resnet = Main_model(\n",
    "        embed_dim=config[\"model\"][\"embed_dim\"],\n",
    "        hidden_dim=config[\"model\"][\"hidden_dim\"], \n",
    "        dropout=config[\"model\"][\"dropout\"], \n",
    "        n_layers=config[\"model\"][\"n_layers\"],  \n",
    "        dropout_omics=config[\"model\"][\"dropout_omics\"], \n",
    "        dropout_omics_finetuning=config[\"model\"][\"dropout_omics_finetuning\"],\n",
    "        norm=config[\"model\"][\"norm\"]\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(autoencoder.parameters()) + list(resnet.parameters()), config[\"optimizer\"][\"learning_rate\"])\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)\n",
    "    early_stop = scripts.EarlyStop(config[\"optimizer\"][\"stopping_patience\"])\n",
    "    device = torch.device(config[\"env\"][\"device\"])\n",
    "    autoencoder.to(device)\n",
    "    resnet.to(device)\n",
    "    autoencoder.train()\n",
    "    resnet.train()\n",
    "\n",
    "    metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection({\n",
    "        \"R_cellwise_residuals\": scripts.GroupwiseMetric(\n",
    "            metric=torchmetrics.functional.pearson_corrcoef,\n",
    "            grouping=\"drugs\",\n",
    "            average=\"macro\",\n",
    "            residualize=True\n",
    "        ),\n",
    "        \"R_cellwise\": scripts.GroupwiseMetric(\n",
    "            metric=torchmetrics.functional.pearson_corrcoef,\n",
    "            grouping=\"cell_lines\",\n",
    "            average=\"macro\",\n",
    "            residualize=False\n",
    "        ),\n",
    "        \"MSE\": torchmetrics.MeanSquaredError()\n",
    "    }))\n",
    "    metrics.to(device)\n",
    "\n",
    "    num_epochs = config[\"env\"][\"max_epochs\"]\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        ### 1️⃣ Training Step ###\n",
    "        train_loss_pred, train_loss_recon = chain_train_step(autoencoder, resnet, lamda, optimizer, train_loader, config, device)\n",
    "        total_train_loss = train_loss_pred + train_loss_recon\n",
    "            \n",
    "        # Learning Rate Scheduler 업데이트\n",
    "        lr_scheduler.step(total_train_loss)\n",
    "\n",
    "        ### 2️⃣ Validation Step ###\n",
    "        if validation_dataset is not None:\n",
    "            with torch.no_grad():\n",
    "                validation_metrics = evaluate_step(autoencoder, resnet, val_loader, metrics, device)\n",
    "    \n",
    "                # Momentum을 사용한 Validation Metric 업데이트\n",
    "                if epoch > 0 and use_momentum:\n",
    "                    val_target = 0.2 * val_target + 0.8 * validation_metrics['R_cellwise_residuals']\n",
    "                else:\n",
    "                    val_target = validation_metrics['R_cellwise_residuals']\n",
    "        else:\n",
    "            val_target = None\n",
    "        \n",
    "        # 로그 출력\n",
    "        if callback_epoch is None:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {total_train_loss} | Validation R: {val_target}\")\n",
    "        else:\n",
    "            callback_epoch(epoch, val_target)\n",
    "\n",
    "        # Early Stopping 체크\n",
    "        if early_stop(total_train_loss):\n",
    "            print(\"⏹ Early Stopping Triggered. Stopping Training.\")\n",
    "            break\n",
    "\n",
    "    print(\"✅ Training Complete!\")\n",
    "\n",
    "    return val_target, autoencoder, resnet\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "423a3b62-5c29-4360-8eab-265db337cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation step\n",
    "def evaluate_step(autoencoder, resnet, loader, metrics, device, save_predictions=False, model_name = \"model\", dataset_name = \"dataset\"):\n",
    "    metrics.increment()\n",
    "    autoencoder.eval()  # Autoencoder Evaluation Mode\n",
    "    resnet.eval()  # ResNet Evaluation Mode\n",
    "\n",
    "    predictions = {\"cell_line\": [], \"drug_id\": [], \"prediction\": [], \"target\": []}\n",
    "\n",
    "    for batch in loader:\n",
    "        with torch.no_grad():\n",
    "            _, latent_features = autoencoder(batch[0].to(device))  # Autoencoder에서 Latent Feature 생성\n",
    "            out = resnet(latent_features, batch[1].to(device))  # ResNet 예측\n",
    "\n",
    "            metrics.update(out.squeeze(),\n",
    "                           batch[2].to(device).squeeze(),\n",
    "                           cell_lines=batch[3].to(device).squeeze(),\n",
    "                           drugs=batch[4].to(device).squeeze())\n",
    "\n",
    "            # 결과 저장\n",
    "            predictions[\"cell_line\"].extend(batch[3].squeeze().tolist())  \n",
    "            predictions[\"drug_id\"].extend(batch[4].squeeze().cpu().tolist())    \n",
    "            predictions[\"prediction\"].extend(out.squeeze().tolist()) \n",
    "            predictions[\"target\"].extend(batch[2].squeeze().cpu().tolist())    \n",
    "\n",
    "    metrics_dict = {it[0]: it[1].item() for it in metrics.compute().items()}\n",
    "\n",
    "    # Save predictions to a CSV file if required\n",
    "    if save_predictions:\n",
    "        df = pd.DataFrame(predictions)\n",
    "        filename = generate_filename(model_name, dataset_name, extension=\"csv\")\n",
    "        df.to_csv(\"results/\" + filename, index=False)\n",
    "        print(f\"Predictions saved to: results/{filename}\")\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def generate_filename(model_name=\"model1\", dataset_name=\"dataset\", extension=\"csv\"):\n",
    "    time = datetime.now().strftime(\"%Y%m%d_%H:%M:%S\")\n",
    "    unique_id = uuid.uuid4()\n",
    "    filename = f\"pred_{model_name}_{dataset_name}_{time}_{unique_id}.{extension}\"\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55296b8c-c6d3-4795-a100-5d177ddd4cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Hyperparameters: {'embed_dim': 493, 'hidden_dim_encoders': 868, 'fusion_dim': 592, 'dropout_encoders': 0.2670033232985919, 'pre_training_epochs': 160, 'hidden_dim': 1982, 'dropout': 0.2965716546354523, 'n_layers': 5, 'dropout_omics': 0.8626326149215306, 'dropout_omics_finetuning': 0.5489897453017749, 'batch_size': 261, 'clip_norm': 10, 'learning_rate': 3.3761769665931243e-06, 'lr_pretraining': 1.435539966576822e-06, 'pre_batch_size': 81}\n",
    "# new best config\n",
    "# config\n",
    "config = {\"features\" : {\"fp_radius\":2,\n",
    "                        \"use_correlation_representation\": True,\n",
    "                        \"num_modalities\": 4},\n",
    "          \"optimizer\": {\"batch_size\": 261,\n",
    "                        \"clip_norm\":10,\n",
    "                        \"learning_rate\": 3.3761769665931243e-06,\n",
    "                        \"stopping_patience\":15,\n",
    "                        \"pre_batch_size\": 81,\n",
    "                        \"lr_pretraining\": 1.435539966576822e-06},\n",
    "          \"model\":{\"embed_dim\":493, # shared\n",
    "                 \"hidden_dim\":1982, \n",
    "                 \"dropout\":0.2965716546354523, \n",
    "                 \"n_layers\": 5, \n",
    "                 \"norm\": \"batchnorm\", \n",
    "                 \"hidden_dim_encoders\": 868, # ENCODER\n",
    "                 \"fusion_dim\": 592, # ENCODER\n",
    "                 \"dropout_encoders\": 0.2670033232985919,\n",
    "                 \"dropout_omics\": 0.8626326149215306, # second\n",
    "                 \"dropout_omics_finetuning\": 0.5489897453017749, # first\n",
    "                 \"pre_training_epochs\": 160}, \n",
    "         \"env\": {\"fold\": 0,  \n",
    "                \"device\":\"cuda:0\", \n",
    "                 \"max_epochs\": 100, \n",
    "                 \"search_hyperparameters\":False}} \n",
    "\n",
    "# Best Hyperparameters: {'embed_dim': 215, 'hidden_dim_encoders': 1330, 'fusion_dim': 505, 'dropout_encoders': 0.3626415306459327, 'pre_training_epochs': 184, 'pre_batch_size': 474, 'lr_pretraining': 3.9761097548681355e-06, 'dropout_omics': 0.5398956377457799, 'dropout_omics_finetuning': 0.42563415722955866, 'learning_rate': 0.0006230634247016341, 'batch_size': 469}\n",
    "\n",
    "# Best Hyperparameters: {'embed_dim': 258, 'hidden_dim_encoders': 1236, 'fusion_dim': 447, 'dropout_encoders': 0.44181165428653146, 'pre_training_epochs': 190, 'pre_batch_size': 460, 'lr_pretraining': 2.480792529122e-06, 'dropout_omics': 0.5666807093758374, 'dropout_omics_finetuning': 0.3833537931230297, 'learning_rate': 0.0008108830765296783, 'batch_size': 408}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62307b7-6612-43b1-a2b6-8cc7d48c9f02",
   "metadata": {},
   "source": [
    "# Model training \n",
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af043824-988a-437e-8b6b-df4e9133ce0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:26:41] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# data loading with raw data\n",
    "train_dataset, val_dataset, test_dataset, input_dims= get_data_corr(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"], \n",
    "                                                           transform_into_corr = False,\n",
    "                                                           typ = (\"rnaseq\", \"proteomics\", \"mutations\", \"methylations\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f544e44c-523b-4ee0-99a5-ea8ec63e789f",
   "metadata": {},
   "source": [
    "여기서 Autoencoder의 evaluation은 수행하지 않는다. Autoencoder를 단독으로 평가하는 것이 Test distribution과 맞지 않을 수 있기 때문이다. 여기서 이걸 먼저 평가하게 되면 Test set의 정보가 노출되어 편향된 결과가 나타날 수 있다. \n",
    "\n",
    "또한, 단순 Reconstruction Loss로 이를 중간에 평가하는 것이 애초에 의미가 없을 가능성이 높다. \n",
    "\n",
    "그렇다면, 두 모델을 따로 트레이닝 할 것인지, 한번에 트레이닝 할 것인지 정해야겠다. \n",
    "\n",
    "1. Autoencoder pretraining -> encoder frozen, used as feature extractor -> Resnet training\n",
    "   - Autoencoder가 단순 Reconstruction Loss로 학습되므로, ResNet에 필요한 Feature를 충분히 학습하지 못할 가능성이 있음.\n",
    "3. Autoencoder -> latent feature -> Resnet\n",
    "   - 설계가 복잡해짐.\n",
    "   - 두 모델을 한번에 트레이닝하므로, 학습이 불안정할 수 있음. \n",
    "\n",
    "\n",
    "둘다 해볼건데, 여기서는 2번을 할거다. 간략적인 플로우는 다음과 같다.\n",
    "\n",
    "- autoencoder와 resnet을 동시에 학습.\n",
    "- 이 과정에서 reconstruction loss와 prediction loss를 합산하여 backpropagation한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84584108-f676-4d60-98c1-adccbc220f0a",
   "metadata": {},
   "source": [
    "## Autoencoder training\n",
    "\n",
    "여기서는 Autoencoder와 Resnet을 한 시스템으로 묶어버리도록 하겠다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c552b7a5-c3ad-4c21-b221-88d772f67552",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: FutureWarning: The default value for `maximize` will be changed from `True` to `None` in v1.7.0 of TorchMetrics,will automatically infer the value based on the `higher_is_better` attribute of the metric (if such attribute exists) or raise an error if it does not. If you are explicitly setting the `maximize` argument to either `True` or `False` already, you can ignore this warning.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 20.51217029206564 | Validation R: None\n",
      "Epoch 2/100 | Train Loss: 16.460548546656835 | Validation R: None\n",
      "Epoch 3/100 | Train Loss: 15.217209692574666 | Validation R: None\n",
      "Epoch 4/100 | Train Loss: 12.824020484850204 | Validation R: None\n",
      "Epoch 5/100 | Train Loss: 8.373624313611788 | Validation R: None\n",
      "Epoch 6/100 | Train Loss: 4.5565076613705875 | Validation R: None\n",
      "Epoch 7/100 | Train Loss: 3.198260305564075 | Validation R: None\n",
      "Epoch 8/100 | Train Loss: 2.7481189669926493 | Validation R: None\n",
      "Epoch 9/100 | Train Loss: 2.5187890209760133 | Validation R: None\n",
      "Epoch 10/100 | Train Loss: 2.3790631753957863 | Validation R: None\n",
      "Epoch 11/100 | Train Loss: 2.3007810707490814 | Validation R: None\n",
      "Epoch 12/100 | Train Loss: 2.254846773364327 | Validation R: None\n",
      "Epoch 13/100 | Train Loss: 2.2259324644842455 | Validation R: None\n",
      "Epoch 14/100 | Train Loss: 2.198489167997914 | Validation R: None\n",
      "Epoch 15/100 | Train Loss: 2.1735234575187707 | Validation R: None\n",
      "Epoch 16/100 | Train Loss: 2.1628988655256736 | Validation R: None\n",
      "Epoch 17/100 | Train Loss: 2.1436054204041657 | Validation R: None\n",
      "Epoch 18/100 | Train Loss: 2.1233010491318014 | Validation R: None\n",
      "Epoch 19/100 | Train Loss: 2.109613288139318 | Validation R: None\n",
      "Epoch 20/100 | Train Loss: 2.0945614905388825 | Validation R: None\n",
      "Epoch 21/100 | Train Loss: 2.0768243991209965 | Validation R: None\n",
      "Epoch 22/100 | Train Loss: 2.0667897254403385 | Validation R: None\n",
      "Epoch 23/100 | Train Loss: 2.044000828982798 | Validation R: None\n",
      "Epoch 24/100 | Train Loss: 2.0372484138459406 | Validation R: None\n",
      "Epoch 25/100 | Train Loss: 2.0239865258991543 | Validation R: None\n",
      "Epoch 26/100 | Train Loss: 2.0050748870694393 | Validation R: None\n",
      "Epoch 27/100 | Train Loss: 1.9857345572577194 | Validation R: None\n",
      "Epoch 28/100 | Train Loss: 1.975165915366841 | Validation R: None\n",
      "Epoch 29/100 | Train Loss: 1.9638678332752497 | Validation R: None\n",
      "Epoch 30/100 | Train Loss: 1.95078961922626 | Validation R: None\n",
      "Epoch 31/100 | Train Loss: 1.935610099621882 | Validation R: None\n",
      "Epoch 32/100 | Train Loss: 1.9277239912503625 | Validation R: None\n",
      "Epoch 33/100 | Train Loss: 1.914730099158203 | Validation R: None\n",
      "Epoch 34/100 | Train Loss: 1.9046732920530605 | Validation R: None\n",
      "Epoch 35/100 | Train Loss: 1.898341572660505 | Validation R: None\n",
      "Epoch 36/100 | Train Loss: 1.8910290351973251 | Validation R: None\n",
      "Epoch 37/100 | Train Loss: 1.8886720279444698 | Validation R: None\n",
      "Epoch 38/100 | Train Loss: 1.8847329342120553 | Validation R: None\n",
      "Epoch 39/100 | Train Loss: 1.8729068417965142 | Validation R: None\n",
      "Epoch 40/100 | Train Loss: 1.86435726638938 | Validation R: None\n",
      "Epoch 41/100 | Train Loss: 1.8629805570211577 | Validation R: None\n",
      "Epoch 42/100 | Train Loss: 1.8589669938398596 | Validation R: None\n",
      "Epoch 43/100 | Train Loss: 1.8525143842543326 | Validation R: None\n",
      "Epoch 44/100 | Train Loss: 1.8479867576241842 | Validation R: None\n",
      "Epoch 45/100 | Train Loss: 1.8399031899017322 | Validation R: None\n",
      "Epoch 46/100 | Train Loss: 1.8355072027077772 | Validation R: None\n",
      "Epoch 47/100 | Train Loss: 1.8252157689102235 | Validation R: None\n",
      "Epoch 48/100 | Train Loss: 1.8228653699946193 | Validation R: None\n",
      "Epoch 49/100 | Train Loss: 1.816672554026601 | Validation R: None\n",
      "Epoch 50/100 | Train Loss: 1.8075771601843345 | Validation R: None\n",
      "Epoch 51/100 | Train Loss: 1.8041042009931156 | Validation R: None\n",
      "Epoch 52/100 | Train Loss: 1.7943893580580037 | Validation R: None\n",
      "Epoch 53/100 | Train Loss: 1.7886313574929391 | Validation R: None\n",
      "Epoch 54/100 | Train Loss: 1.7769732663183966 | Validation R: None\n",
      "Epoch 55/100 | Train Loss: 1.774117855688344 | Validation R: None\n",
      "Epoch 56/100 | Train Loss: 1.7680158642642307 | Validation R: None\n",
      "Epoch 57/100 | Train Loss: 1.760471810343678 | Validation R: None\n",
      "Epoch 58/100 | Train Loss: 1.7591887011118998 | Validation R: None\n",
      "Epoch 59/100 | Train Loss: 1.755613931113324 | Validation R: None\n",
      "Epoch 60/100 | Train Loss: 1.7524416186680194 | Validation R: None\n",
      "Epoch 61/100 | Train Loss: 1.7493438854420291 | Validation R: None\n",
      "Epoch 62/100 | Train Loss: 1.7381447856901677 | Validation R: None\n",
      "Epoch 63/100 | Train Loss: 1.7420652240864343 | Validation R: None\n",
      "Epoch 64/100 | Train Loss: 1.7355934172169554 | Validation R: None\n",
      "Epoch 65/100 | Train Loss: 1.7283106526671266 | Validation R: None\n",
      "Epoch 66/100 | Train Loss: 1.7302443578708206 | Validation R: None\n",
      "Epoch 67/100 | Train Loss: 1.7258425495229508 | Validation R: None\n",
      "Epoch 68/100 | Train Loss: 1.7223388051881816 | Validation R: None\n",
      "Epoch 69/100 | Train Loss: 1.716288677277453 | Validation R: None\n",
      "Epoch 70/100 | Train Loss: 1.7158548623935217 | Validation R: None\n",
      "Epoch 71/100 | Train Loss: 1.7166830933163941 | Validation R: None\n",
      "Epoch 72/100 | Train Loss: 1.7128606311911363 | Validation R: None\n",
      "Epoch 73/100 | Train Loss: 1.7053696422283249 | Validation R: None\n",
      "Epoch 74/100 | Train Loss: 1.7106430853042434 | Validation R: None\n",
      "Epoch 75/100 | Train Loss: 1.6981080959269728 | Validation R: None\n",
      "Epoch 76/100 | Train Loss: 1.7000870234892864 | Validation R: None\n",
      "Epoch 77/100 | Train Loss: 1.699563873216204 | Validation R: None\n",
      "Epoch 78/100 | Train Loss: 1.6974174057371105 | Validation R: None\n",
      "Epoch 79/100 | Train Loss: 1.6929301281303955 | Validation R: None\n",
      "Epoch 80/100 | Train Loss: 1.6921733218379036 | Validation R: None\n",
      "Epoch 81/100 | Train Loss: 1.6932533548549473 | Validation R: None\n",
      "Epoch 82/100 | Train Loss: 1.685337983486939 | Validation R: None\n",
      "Epoch 83/100 | Train Loss: 1.6870120550093415 | Validation R: None\n",
      "Epoch 84/100 | Train Loss: 1.6808482545299614 | Validation R: None\n",
      "Epoch 85/100 | Train Loss: 1.6839856316179822 | Validation R: None\n",
      "Epoch 86/100 | Train Loss: 1.6774793734823152 | Validation R: None\n",
      "Epoch 87/100 | Train Loss: 1.67897835386988 | Validation R: None\n",
      "Epoch 88/100 | Train Loss: 1.6763446104666355 | Validation R: None\n",
      "Epoch 89/100 | Train Loss: 1.6767962248094619 | Validation R: None\n",
      "Epoch 90/100 | Train Loss: 1.6731529119602047 | Validation R: None\n",
      "Epoch 91/100 | Train Loss: 1.6704325674653404 | Validation R: None\n",
      "Epoch 92/100 | Train Loss: 1.6674204855458128 | Validation R: None\n",
      "Epoch 93/100 | Train Loss: 1.6660686211764286 | Validation R: None\n",
      "Epoch 94/100 | Train Loss: 1.6648344523833294 | Validation R: None\n",
      "Epoch 95/100 | Train Loss: 1.6644116169895944 | Validation R: None\n",
      "Epoch 96/100 | Train Loss: 1.6669134931567826 | Validation R: None\n",
      "Epoch 97/100 | Train Loss: 1.6594260968595655 | Validation R: None\n",
      "Epoch 98/100 | Train Loss: 1.6595251492041647 | Validation R: None\n",
      "Epoch 99/100 | Train Loss: 1.6546883860116832 | Validation R: None\n",
      "Epoch 100/100 | Train Loss: 1.6509969421932773 | Validation R: None\n",
      "✅ Training Complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_, autoencoder, resnet = train_auto_resnet_chain(config, torch.utils.data.ConcatDataset([train_dataset, val_dataset]),\n",
    "                                                 lamda = 0.01, validation_dataset=None, use_momentum=True, callback_epoch=None)\n",
    "\n",
    "metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(\n",
    "    {\"R_cellwise_residuals\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"drugs\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=True),\n",
    "    \"R_cellwise\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"cell_lines\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=False),\n",
    "    \"MSE\":torchmetrics.MeanSquaredError()}))\n",
    "device = torch.device(config[\"env\"][\"device\"])\n",
    "metrics.to(device)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                       batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                       drop_last=False,\n",
    "                                      shuffle=False,\n",
    "                                      pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2949c0b-2267-4732-a146-4ea178117fdf",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "트레이닝된 Autoencoder의 encoder와, Resnet을 하나의 시스템으로 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f5d20ff-ce7c-414f-8aa2-f36ab25ced43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/kim14/project_work/scripts/models.py:69: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/TensorShape.cpp:3277.)\n",
      "  return torch.linalg.solve(A, Xy).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to: results/pred_auto-resnet-chain_raw_0.01_20250316_02:04:59_4bec1760-84bb-4c8c-97ac-862d84fadb04.csv\n",
      "main model final metrics: {'MSE': 1.6427305936813354, 'R_cellwise': 0.8919653296470642, 'R_cellwise_residuals': 0.27920225262641907}\n"
     ]
    }
   ],
   "source": [
    "autoencoder_resnet_chain = evaluate_step(autoencoder, resnet, test_dataloader, metrics, device,\n",
    "                                        save_predictions=True, model_name = \"auto-resnet-chain\", dataset_name = \"raw_0.01\")\n",
    "print(f\"main model final metrics: {autoencoder_resnet_chain}\")\n",
    "#2907s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a2735-570b-4331-a27b-616ca1182b72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fbb24a9-529a-42db-bc7e-c03f7029833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def train_model_optuna(trial, config):\n",
    "    \"\"\"\n",
    "    Optuna를 활용한 하이퍼파라미터 최적화 함수\n",
    "    - Autoencoder + ResNet을 함께 최적화\n",
    "    \"\"\"\n",
    "\n",
    "    ### 1️⃣ 하이퍼파라미터 샘플링 ###\n",
    "    # Autoencoder 관련\n",
    "    config[\"model\"][\"embed_dim\"] = trial.suggest_int(\"embed_dim\", 64, 512)\n",
    "    config[\"model\"][\"hidden_dim_encoders\"] = trial.suggest_int(\"hidden_dim_encoders\", 64, 1024)\n",
    "    config[\"model\"][\"fusion_dim\"] = trial.suggest_int(\"fusion_dim\", 64, 1024)\n",
    "    config[\"model\"][\"dropout_encoders\"] = trial.suggest_float(\"dropout_encoders\", 0.1, 0.5)\n",
    "    config[\"model\"][\"pre_training_epochs\"] = trial.suggest_int(\"pre_training_epochs\", 10, 200)\n",
    "\n",
    "    # ResNet 관련\n",
    "    config[\"model\"][\"hidden_dim\"] = trial.suggest_int(\"hidden_dim\", 256, 2048)\n",
    "    config[\"model\"][\"dropout\"] = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    config[\"model\"][\"n_layers\"] = trial.suggest_int(\"n_layers\", 2, 10)\n",
    "    config[\"model\"][\"dropout_omics\"] = trial.suggest_float(\"dropout_omics\", 0.1, 0.9)\n",
    "    config[\"model\"][\"dropout_omics_finetuning\"] = trial.suggest_float(\"dropout_omics_finetuning\", 0.1, 0.9)\n",
    "\n",
    "    # Optimizer 관련\n",
    "    config[\"optimizer\"][\"batch_size\"] = trial.suggest_int(\"batch_size\", 64, 512)\n",
    "    config[\"optimizer\"][\"clip_norm\"] = trial.suggest_int(\"clip_norm\", 5, 20)\n",
    "    config[\"optimizer\"][\"learning_rate\"] = trial.suggest_float(\"learning_rate\", 1e-6, 1e-2, log=True)\n",
    "    config[\"optimizer\"][\"lr_pretraining\"] = trial.suggest_float(\"lr_pretraining\", 1e-6, 1e-2, log=True)\n",
    "    config[\"optimizer\"][\"pre_batch_size\"] = trial.suggest_int(\"pre_batch_size\", 64, 512)\n",
    "\n",
    "    ### 2️⃣ 데이터 로딩 ###\n",
    "    #train_dataset, val_dataset, test_dataset, input_dims = get_data_corr(\n",
    "    #    n_fold=config[\"env\"][\"fold\"],\n",
    "    #    fp_radius=config[\"features\"][\"fp_radius\"],\n",
    "    #    transform_into_corr=config[\"features\"][\"use_correlation_representation\"],\n",
    "    #    typ=(\"rnaseq\", \"proteomics\", \"mutations\", \"methylations\")\n",
    "    #)\n",
    "\n",
    "    ### 3️⃣ 모델 학습 (Autoencoder + ResNet 동시 학습) ###\n",
    "    try:\n",
    "        val_target, autoencoder, resnet = train_auto_resnet_chain(\n",
    "            config, train_dataset, val_dataset, use_momentum=True\n",
    "        )\n",
    "        print(f\"Trial {trial.number}: Validation R_cellwise_residuals = {val_target}\")\n",
    "\n",
    "        return val_target  # 최적화 목표 (Validation R값 최대화)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in trial {trial.number}: {e}\")\n",
    "        return -float(\"inf\")  # 실패 시 최소 값 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa8f8e12-d0e9-4395-90a2-bd181ae56cbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 23:06:27,799] Using an existing study with name 'auto_resnet_chain_opt' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 5.489410892630046 | Validation R: 0.01004643552005291\n",
      "Epoch 2/100 | Train Loss: 2.5610766532146823 | Validation R: 0.003304628189653158\n",
      "Epoch 3/100 | Train Loss: 2.2855825964987804 | Validation R: 0.0018826914951205254\n",
      "Epoch 4/100 | Train Loss: 2.1744253857407423 | Validation R: -0.0051840155199170115\n",
      "Epoch 5/100 | Train Loss: 2.1126591061002915 | Validation R: -0.007814185746014118\n",
      "Epoch 6/100 | Train Loss: 2.076366836729303 | Validation R: -0.014115806306898595\n",
      "Epoch 7/100 | Train Loss: 2.0563792280127515 | Validation R: -0.009512321578562261\n",
      "Epoch 8/100 | Train Loss: 2.0433153385082177 | Validation R: -0.009597727194845678\n",
      "Epoch 9/100 | Train Loss: 2.032511304450774 | Validation R: -0.00922377577134371\n",
      "Epoch 10/100 | Train Loss: 2.0214865629322234 | Validation R: -0.007527938079254628\n",
      "Epoch 11/100 | Train Loss: 2.0143513717054002 | Validation R: -0.012388687201622487\n",
      "Epoch 12/100 | Train Loss: 2.016638361956034 | Validation R: -0.014613377227044774\n",
      "Epoch 13/100 | Train Loss: 2.0043111183078945 | Validation R: -0.017103670969338552\n",
      "Epoch 14/100 | Train Loss: 2.00143294249686 | Validation R: -0.01874879726464179\n",
      "Epoch 15/100 | Train Loss: 1.9951490505886949 | Validation R: -0.017174917417157945\n",
      "Epoch 16/100 | Train Loss: 1.9855440004094116 | Validation R: -0.01902883260595585\n",
      "Epoch 17/100 | Train Loss: 1.971386982747807 | Validation R: -0.018343622503971625\n",
      "Epoch 18/100 | Train Loss: 1.9691457922150077 | Validation R: -0.0191589300867509\n",
      "Epoch 19/100 | Train Loss: 1.9472217580072781 | Validation R: -0.019856198232106194\n",
      "Epoch 20/100 | Train Loss: 1.9194998440917879 | Validation R: -0.018246704061971472\n",
      "Epoch 21/100 | Train Loss: 1.8876495706609788 | Validation R: -0.016935245935151253\n",
      "Epoch 22/100 | Train Loss: 1.864183638358604 | Validation R: -0.01336227913578933\n",
      "Epoch 23/100 | Train Loss: 1.8477588291156346 | Validation R: -0.009789104381959807\n",
      "Epoch 24/100 | Train Loss: 1.8311924510874447 | Validation R: -0.008373835458310677\n",
      "Epoch 25/100 | Train Loss: 1.8248617759155752 | Validation R: -0.007381897672802654\n",
      "Epoch 26/100 | Train Loss: 1.8150865628425263 | Validation R: -0.0046825461595834195\n",
      "Epoch 27/100 | Train Loss: 1.806894401052093 | Validation R: -0.003773763338915128\n",
      "Epoch 28/100 | Train Loss: 1.7983569510792077 | Validation R: -0.002590776147483114\n",
      "Epoch 29/100 | Train Loss: 1.785698096046999 | Validation R: -0.0010007927112605818\n",
      "Epoch 30/100 | Train Loss: 1.7824504060423478 | Validation R: -0.0008760056442087223\n",
      "Epoch 31/100 | Train Loss: 1.7723946983159513 | Validation R: 0.00159623662042488\n",
      "Epoch 32/100 | Train Loss: 1.765790171073997 | Validation R: 0.005480670187687615\n",
      "Epoch 33/100 | Train Loss: 1.7596346051498126 | Validation R: 0.008220777810378024\n",
      "Epoch 34/100 | Train Loss: 1.7498003188166273 | Validation R: 0.005415800765308371\n",
      "Epoch 35/100 | Train Loss: 1.7425242914406138 | Validation R: 0.003964145629138277\n",
      "Epoch 36/100 | Train Loss: 1.73938917099968 | Validation R: 0.0031778504994572254\n",
      "Epoch 37/100 | Train Loss: 1.728911916768195 | Validation R: 0.0014388629219842168\n",
      "Epoch 38/100 | Train Loss: 1.725663307167392 | Validation R: 0.0065837538425507595\n",
      "Epoch 39/100 | Train Loss: 1.7184562171183 | Validation R: 0.0056121557689583785\n",
      "Epoch 40/100 | Train Loss: 1.7144708406757068 | Validation R: 0.006471933493750821\n",
      "Epoch 41/100 | Train Loss: 1.7069138684392438 | Validation R: 0.005686125471142438\n",
      "Epoch 42/100 | Train Loss: 1.7059830301647299 | Validation R: 0.002940532329469424\n",
      "Epoch 43/100 | Train Loss: 1.697125542017145 | Validation R: 0.0063512844121037925\n",
      "Epoch 44/100 | Train Loss: 1.692324169385618 | Validation R: 0.00918847764085887\n",
      "Epoch 45/100 | Train Loss: 1.6792586134441725 | Validation R: 0.007626571644447562\n",
      "Epoch 46/100 | Train Loss: 1.6824053030807227 | Validation R: 0.012257451740722323\n",
      "Epoch 47/100 | Train Loss: 1.678156342724981 | Validation R: 0.015696378534599237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-12 23:24:30,230] Trial 163 failed with parameters: {'embed_dim': 508, 'hidden_dim_encoders': 812, 'fusion_dim': 596, 'dropout_encoders': 0.26815411194064864, 'pre_training_epochs': 175, 'hidden_dim': 1994, 'dropout': 0.29737656926914646, 'n_layers': 5, 'dropout_omics': 0.856976151498954, 'dropout_omics_finetuning': 0.5516169494497061, 'batch_size': 280, 'clip_norm': 10, 'learning_rate': 3.068319236416493e-06, 'lr_pretraining': 1.4284574022734964e-06, 'pre_batch_size': 114} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_175440/4219314737.py\", line 15, in <lambda>\n",
      "    study.optimize(lambda trial: train_model_optuna(trial, config), n_trials=40)\n",
      "  File \"/tmp/ipykernel_175440/944425779.py\", line 41, in train_model_optuna\n",
      "    val_target, autoencoder, resnet = train_auto_resnet_chain(\n",
      "  File \"/tmp/ipykernel_175440/2337750768.py\", line 107, in train_auto_resnet_chain\n",
      "    train_loss_pred, train_loss_recon = chain_train_step(autoencoder, resnet, optimizer, train_loader, config, device)\n",
      "  File \"/tmp/ipykernel_175440/2337750768.py\", line 15, in chain_train_step\n",
      "    for batch in loader:\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 628, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 671, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/tmp/ipykernel_175440/3351032323.py\", line 18, in __getitem__\n",
      "    cell_id = instance.iloc[0]\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1191, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1752, in _getitem_axis\n",
      "    self._validate_integer(key, axis)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1683, in _validate_integer\n",
      "    len_axis = len(self.obj._get_axis(axis))\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/core/generic.py\", line 586, in _get_axis\n",
      "    @final\n",
      "KeyboardInterrupt\n",
      "[W 2025-03-12 23:24:30,316] Trial 163 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_175440/4219314737.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mload_if_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_startup_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_model_optuna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mbest_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best Hyperparameters:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0mRaises\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    115\u001b[0m                         )\n\u001b[1;32m    116\u001b[0m                     )\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_optimize_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;31m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;31m# Please refer to the following PR for further details:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# https://github.com/optuna/optuna/pull/325.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_175440/4219314737.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(trial)\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_model_optuna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_175440/944425779.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(trial, config)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Trial {trial.number}: Validation R_cellwise_residuals = {val_target}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mval_target\u001b[0m  \u001b[0;31m# 최적화 목표 (Validation R값 최대화)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error in trial {trial.number}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 실패 시 최소 값 반환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_175440/2337750768.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(config, train_dataset, validation_dataset, use_momentum, callback_epoch)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"env\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m### 1️⃣ Training Step ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtrain_loss_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_recon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss_pred\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_loss_recon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Learning Rate Scheduler 업데이트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_175440/2337750768.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(autoencoder, resnet, optimizer, loader, config, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Autoencoder 학습 모드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ResNet 학습 모드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# 1️⃣ Autoencoder Forward (Latent Feature 생성)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IterableDataset_len_called\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_175440/3351032323.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# idx = train_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mcell_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mdrug_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_deprecated_callable_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_callable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1748\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot index by location index with a non-integer key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1752\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1754\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1679\u001b[0m         \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m         \u001b[0mIndexError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0;34m'key'\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ma\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m'axis'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m         \"\"\"\n\u001b[0;32m-> 1683\u001b[0;31m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, axis)\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0maxis_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0maxis_number\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#if config[\"env\"][\"search_hyperparameters\"]:\n",
    "true = True\n",
    "if true:\n",
    "    study_name = \"auto_resnet_chain_opt\"\n",
    "    storage_name = f\"sqlite:///studies/{study_name}.db\"\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=storage_name,\n",
    "        direction='maximize',  # Validation R 값을 최대화\n",
    "        load_if_exists=True,\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=30, n_warmup_steps=5, interval_steps=5)\n",
    "    )\n",
    "\n",
    "    study.optimize(lambda trial: train_model_optuna(trial, config), n_trials=40)\n",
    "\n",
    "    best_config = study.best_params\n",
    "    print(\"Best Hyperparameters:\", best_config)\n",
    "\n",
    "    # 최적의 하이퍼파라미터를 config에 반영\n",
    "    config[\"model\"][\"embed_dim\"] = best_config[\"embed_dim\"]\n",
    "    config[\"model\"][\"hidden_dim_encoders\"] = best_config[\"hidden_dim_encoders\"]\n",
    "    config[\"model\"][\"fusion_dim\"] = best_config[\"fusion_dim\"]\n",
    "    config[\"model\"][\"dropout_encoders\"] = best_config[\"dropout_encoders\"]\n",
    "    config[\"model\"][\"pre_training_epochs\"] = best_config[\"pre_training_epochs\"]\n",
    "    config[\"model\"][\"hidden_dim\"] = best_config[\"hidden_dim\"]\n",
    "    config[\"model\"][\"dropout\"] = best_config[\"dropout\"]\n",
    "    config[\"model\"][\"n_layers\"] = best_config[\"n_layers\"]\n",
    "    config[\"model\"][\"dropout_omics\"] = best_config[\"dropout_omics\"]\n",
    "    config[\"model\"][\"dropout_omics_finetuning\"] = best_config[\"dropout_omics_finetuning\"]\n",
    "    config[\"optimizer\"][\"batch_size\"] = best_config[\"batch_size\"]\n",
    "    config[\"optimizer\"][\"clip_norm\"] = best_config[\"clip_norm\"]\n",
    "    config[\"optimizer\"][\"learning_rate\"] = best_config[\"learning_rate\"]\n",
    "    config[\"optimizer\"][\"lr_pretraining\"] = best_config[\"lr_pretraining\"]\n",
    "    config[\"optimizer\"][\"pre_batch_size\"] = best_config[\"pre_batch_size\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa17710-e4ea-4534-8e7d-ffb77b0d71c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ 최적 하이퍼파라미터를 적용한 후 Training 실행\n",
    "train_dataset, val_dataset, test_dataset, input_dims = get_data_corr(\n",
    "    n_fold=config[\"env\"][\"fold\"],\n",
    "    fp_radius=config[\"features\"][\"fp_radius\"],\n",
    "    transform_into_corr=config[\"features\"][\"use_correlation_representation\"],\n",
    "    typ=(\"rnaseq\", \"proteomics\", \"mutations\", \"methylations\")\n",
    ")\n",
    "\n",
    "val_target, autoencoder, resnet = train_auto_resnet_chain(config, train_dataset, val_dataset, use_momentum=True)\n",
    "\n",
    "print(f\"Final Training Complete! Final R_cellwise_residuals: {val_target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81e84f3-bb20-4be7-90b2-62b21ebc796c",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc006fe3-50a1-41b5-a75d-b2661c6474ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import RDLogger\n",
    "\n",
    "RDLogger.DisableLog('rdApp.warning')  # RDKit의 경고 메시지 제거\n",
    "RDLogger.DisableLog('rdApp.error') \n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55fb5233-849b-48a7-b342-bb7e51a3b605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Optimizing for Lambda: 0.001 ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[03:13:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: FutureWarning: The default value for `maximize` will be changed from `True` to `None` in v1.7.0 of TorchMetrics,will automatically infer the value based on the `higher_is_better` attribute of the metric (if such attribute exists) or raise an error if it does not. If you are explicitly setting the `maximize` argument to either `True` or `False` already, you can ignore this warning.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 20.708787205282196 | Validation R: None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(folds):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# 데이터셋을 각 폴드별로 분할\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     train_dataset, val_dataset, test_dataset, input_dims \u001b[38;5;241m=\u001b[39m get_data_corr(n_fold\u001b[38;5;241m=\u001b[39mfold, transform_into_corr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     15\u001b[0m                                                                          typ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrnaseq\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproteomics\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmutations\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethylations\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 17\u001b[0m     _, autoencoder, resnet \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_auto_resnet_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConcatDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mlamda\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlambda_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m torchmetrics\u001b[38;5;241m.\u001b[39mMetricTracker(torchmetrics\u001b[38;5;241m.\u001b[39mMetricCollection(\n\u001b[1;32m     21\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR_cellwise_residuals\u001b[39m\u001b[38;5;124m\"\u001b[39m:scripts\u001b[38;5;241m.\u001b[39mGroupwiseMetric(metric\u001b[38;5;241m=\u001b[39mtorchmetrics\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpearson_corrcoef,\n\u001b[1;32m     22\u001b[0m                               grouping\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrugs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m                               residualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m:torchmetrics\u001b[38;5;241m.\u001b[39mMeanSquaredError()}))\n\u001b[1;32m     30\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[6], line 107\u001b[0m, in \u001b[0;36mtrain_auto_resnet_chain\u001b[0;34m(config, train_dataset, lamda, validation_dataset, use_momentum, callback_epoch)\u001b[0m\n\u001b[1;32m    103\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m### 1️⃣ Training Step ###\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     train_loss_pred, train_loss_recon \u001b[38;5;241m=\u001b[39m \u001b[43mchain_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlamda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     total_train_loss \u001b[38;5;241m=\u001b[39m train_loss_pred \u001b[38;5;241m+\u001b[39m train_loss_recon\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# Learning Rate Scheduler 업데이트\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mchain_train_step\u001b[0;34m(autoencoder, resnet, lamda, optimizer, loader, config, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 5️⃣ Backpropagation\u001b[39;00m\n\u001b[1;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# 두 losses를 모두 backpropagation한다.\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclip_norm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 6️⃣ Loss 저장\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:43\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[1;32m     41\u001b[0m     total_norm \u001b[38;5;241m=\u001b[39m norms[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(norms) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39mstack(norms))\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m     total_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, norm_type)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlogical_or(total_norm\u001b[38;5;241m.\u001b[39misnan(), total_norm\u001b[38;5;241m.\u001b[39misinf()):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe total norm of order \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for gradients from \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`parameters` is non-finite, so it cannot be clipped. To disable \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis error and scale the gradients by the non-finite norm anyway, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset `error_if_nonfinite=False`\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lambda_candidates = [0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99, 0.999]\n",
    "\n",
    "# 10-Fold Cross Validation 수행\n",
    "# K-Fold 분할 (10-Fold)\n",
    "folds = 10\n",
    "cv_results = {}\n",
    "\n",
    "for lambda_value in lambda_candidates:\n",
    "    print(f\"====== Optimizing for Lambda: {lambda_value} ======\")\n",
    "    fold_results = []\n",
    "\n",
    "    for fold in range(folds):\n",
    "        # 데이터셋을 각 폴드별로 분할\n",
    "        train_dataset, val_dataset, test_dataset, input_dims = get_data_corr(n_fold=fold, transform_into_corr=False, \n",
    "                                                                             typ = (\"rnaseq\", \"proteomics\", \"mutations\", \"methylations\"))\n",
    "\n",
    "        _, autoencoder, resnet = train_auto_resnet_chain(config, torch.utils.data.ConcatDataset([train_dataset, val_dataset]),\n",
    "                                                 lamda = lambda_value, validation_dataset=None, use_momentum=True, callback_epoch=None)\n",
    "\n",
    "        metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(\n",
    "            {\"R_cellwise_residuals\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                                  grouping=\"drugs\",\n",
    "                                  average=\"macro\",\n",
    "                                  residualize=True),\n",
    "            \"R_cellwise\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                                  grouping=\"cell_lines\",\n",
    "                                  average=\"macro\",\n",
    "                                  residualize=False),\n",
    "            \"MSE\":torchmetrics.MeanSquaredError()}))\n",
    "        device = torch.device(config[\"env\"][\"device\"])\n",
    "        metrics.to(device)\n",
    "        test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                               batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                               drop_last=False,\n",
    "                                              shuffle=False,\n",
    "                                              pin_memory=True)\n",
    "        autoencoder_resnet_chain = evaluate_step(autoencoder, resnet, test_dataloader, metrics, device)\n",
    "\n",
    "        fold_results.append(autoencoder_resnet_chain['R_cellwise_residuals'])\n",
    "\n",
    "    # 각 Lambda에 대해 폴드 평균 성능 계산\n",
    "    cv_results[lambda_value] = np.mean(fold_results)\n",
    "\n",
    "# 최적의 Lambda 값 출력\n",
    "best_lambda = max(cv_results, key=cv_results.get)\n",
    "print(f\"Best Lambda: {best_lambda} with average R_cellwise_residuals: {cv_results[best_lambda]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c576a-1470-4db3-bac0-f10847c91d26",
   "metadata": {},
   "source": [
    "# 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3923d296-ab3b-4dbf-a6a7-d67b45daf8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>R_cellwise</th>\n",
       "      <th>R_cellwise_residuals</th>\n",
       "      <th>Model</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.795264</td>\n",
       "      <td>0.890693</td>\n",
       "      <td>0.333604</td>\n",
       "      <td>baseline_rnaseq</td>\n",
       "      <td>20241206_19:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.977368</td>\n",
       "      <td>0.874748</td>\n",
       "      <td>0.238710</td>\n",
       "      <td>baseline_proteomics</td>\n",
       "      <td>20241206_19:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.400242</td>\n",
       "      <td>0.866647</td>\n",
       "      <td>0.093873</td>\n",
       "      <td>baseline_mutations</td>\n",
       "      <td>20241206_19:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.243853</td>\n",
       "      <td>0.868511</td>\n",
       "      <td>0.167871</td>\n",
       "      <td>baseline_methylations</td>\n",
       "      <td>20241206_19:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.230996</td>\n",
       "      <td>0.868163</td>\n",
       "      <td>0.138434</td>\n",
       "      <td>baseline_cnvs</td>\n",
       "      <td>20241206_19:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.810338</td>\n",
       "      <td>0.886230</td>\n",
       "      <td>0.320672</td>\n",
       "      <td>baseline_concat</td>\n",
       "      <td>20241206_19:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.646177</td>\n",
       "      <td>0.897545</td>\n",
       "      <td>0.327031</td>\n",
       "      <td>baseline_ensemble</td>\n",
       "      <td>20241206_19:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.882182</td>\n",
       "      <td>0.884252</td>\n",
       "      <td>0.265135</td>\n",
       "      <td>encoder_sub-network</td>\n",
       "      <td>20241206_19:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.265149</td>\n",
       "      <td>0.876332</td>\n",
       "      <td>-0.011226</td>\n",
       "      <td>auto-resnet-chain-sim</td>\n",
       "      <td>20250314_02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.594700</td>\n",
       "      <td>0.894793</td>\n",
       "      <td>0.300049</td>\n",
       "      <td>auto-resnet-chain-raw</td>\n",
       "      <td>20250314_02:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.172632</td>\n",
       "      <td>0.880587</td>\n",
       "      <td>-0.012053</td>\n",
       "      <td>auto-resnet-sim</td>\n",
       "      <td>20250314_02:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.630139</td>\n",
       "      <td>0.891810</td>\n",
       "      <td>0.280180</td>\n",
       "      <td>auto-resnet-chain-raw</td>\n",
       "      <td>20250314_03:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.796518</td>\n",
       "      <td>0.889818</td>\n",
       "      <td>0.332300</td>\n",
       "      <td>rnaseq</td>\n",
       "      <td>20250314_15:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.628869</td>\n",
       "      <td>0.892188</td>\n",
       "      <td>0.277903</td>\n",
       "      <td>auto-resnet-chain-raw</td>\n",
       "      <td>20250314_15:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.439976</td>\n",
       "      <td>0.862337</td>\n",
       "      <td>0.094334</td>\n",
       "      <td>baseline_mutations</td>\n",
       "      <td>20250314_16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.151283</td>\n",
       "      <td>0.870389</td>\n",
       "      <td>0.194565</td>\n",
       "      <td>baseline_methylations</td>\n",
       "      <td>20250314_16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.017405</td>\n",
       "      <td>0.875545</td>\n",
       "      <td>0.257444</td>\n",
       "      <td>baseline_proteomics</td>\n",
       "      <td>20250314_16:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.765396</td>\n",
       "      <td>0.887761</td>\n",
       "      <td>0.333191</td>\n",
       "      <td>baseline_concat</td>\n",
       "      <td>20250314_16:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.876814</td>\n",
       "      <td>0.884018</td>\n",
       "      <td>0.286007</td>\n",
       "      <td>encoder_sub-network</td>\n",
       "      <td>20250314_17:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.579047</td>\n",
       "      <td>0.898829</td>\n",
       "      <td>0.320077</td>\n",
       "      <td>auto-resnet-raw</td>\n",
       "      <td>20250314_18:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.621410</td>\n",
       "      <td>0.892856</td>\n",
       "      <td>0.281142</td>\n",
       "      <td>auto-resnet-chain-raw0.4</td>\n",
       "      <td>20250314_19:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.628601</td>\n",
       "      <td>0.891974</td>\n",
       "      <td>0.272781</td>\n",
       "      <td>auto-resnet-chain-raw0.6</td>\n",
       "      <td>20250314_20:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.646025</td>\n",
       "      <td>0.891511</td>\n",
       "      <td>0.268490</td>\n",
       "      <td>auto-resnet-chain-raw0.7</td>\n",
       "      <td>20250314_21:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MSE  R_cellwise  R_cellwise_residuals                     Model  \\\n",
       "0   1.795264    0.890693              0.333604           baseline_rnaseq   \n",
       "1   1.977368    0.874748              0.238710       baseline_proteomics   \n",
       "2   2.400242    0.866647              0.093873        baseline_mutations   \n",
       "3   2.243853    0.868511              0.167871     baseline_methylations   \n",
       "4   2.230996    0.868163              0.138434             baseline_cnvs   \n",
       "5   1.810338    0.886230              0.320672           baseline_concat   \n",
       "6   1.646177    0.897545              0.327031         baseline_ensemble   \n",
       "7   1.882182    0.884252              0.265135       encoder_sub-network   \n",
       "8   2.265149    0.876332             -0.011226     auto-resnet-chain-sim   \n",
       "9   1.594700    0.894793              0.300049     auto-resnet-chain-raw   \n",
       "10  2.172632    0.880587             -0.012053           auto-resnet-sim   \n",
       "11  1.630139    0.891810              0.280180     auto-resnet-chain-raw   \n",
       "12  1.796518    0.889818              0.332300                    rnaseq   \n",
       "13  1.628869    0.892188              0.277903     auto-resnet-chain-raw   \n",
       "14  2.439976    0.862337              0.094334        baseline_mutations   \n",
       "15  2.151283    0.870389              0.194565     baseline_methylations   \n",
       "16  2.017405    0.875545              0.257444       baseline_proteomics   \n",
       "17  1.765396    0.887761              0.333191           baseline_concat   \n",
       "18  1.876814    0.884018              0.286007       encoder_sub-network   \n",
       "19  1.579047    0.898829              0.320077           auto-resnet-raw   \n",
       "20  1.621410    0.892856              0.281142  auto-resnet-chain-raw0.4   \n",
       "21  1.628601    0.891974              0.272781  auto-resnet-chain-raw0.6   \n",
       "22  1.646025    0.891511              0.268490  auto-resnet-chain-raw0.7   \n",
       "\n",
       "              Time  \n",
       "0   20241206_19:25  \n",
       "1   20241206_19:26  \n",
       "2   20241206_19:26  \n",
       "3   20241206_19:26  \n",
       "4   20241206_19:26  \n",
       "5   20241206_19:26  \n",
       "6   20241206_19:26  \n",
       "7   20241206_19:27  \n",
       "8   20250314_02:16  \n",
       "9   20250314_02:23  \n",
       "10  20250314_02:53  \n",
       "11  20250314_03:15  \n",
       "12  20250314_15:22  \n",
       "13  20250314_15:44  \n",
       "14  20250314_16:09  \n",
       "15  20250314_16:09  \n",
       "16  20250314_16:10  \n",
       "17  20250314_16:14  \n",
       "18  20250314_17:00  \n",
       "19  20250314_18:13  \n",
       "20  20250314_19:43  \n",
       "21  20250314_20:27  \n",
       "22  20250314_21:14  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = autoencoder_resnet_chain\n",
    "\n",
    "model_name = \"auto-resnet-chain-raw0.65\"\n",
    "result[\"Model\"] = model_name\n",
    "time = datetime.now().strftime(\"%Y%m%d_%H:%M\")\n",
    "result[\"Time\"] = time\n",
    "\n",
    "result_df = pd.DataFrame([result])\n",
    "\n",
    "ev_table = pd.read_csv(\"results/evaluation_table.csv\")\n",
    "ev_table = pd.concat([ev_table, result_df], ignore_index=True)\n",
    "#ev_table = ev_table.drop_duplicates(subset=[\"Model\"],keep = \"last\")\n",
    "display(ev_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e1fdb24-9d44-4026-a2c4-7a457ef2165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_table.to_csv(\"results/evaluation_table.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "817977db-a4e2-49cf-ab36-d95e92d21bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1275509/86570976.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ev_table['Model'][20] = 'auto-resnet-chain-raw0.3'\n"
     ]
    }
   ],
   "source": [
    "ev_table['Model'][20] = 'auto-resnet-chain-raw0.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac8ec8d-f1fc-4cf3-95cb-d0d6c444d6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
