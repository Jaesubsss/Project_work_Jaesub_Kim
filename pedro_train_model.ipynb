{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdae1d44-c633-40f9-a8f9-4ade3398d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov  5 16:02:12 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-PCIE-16GB           On  | 00000000:21:00.0 Off |                    0 |\n",
      "| N/A   30C    P0              24W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE-16GB           On  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   28C    P0              23W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-PCIE-16GB           On  | 00000000:E2:00.0 Off |                    0 |\n",
      "| N/A   28C    P0              24W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#!pip3 show torch\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62988601-706e-4bd5-b617-630ecf47bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import scripts\n",
    "from functools import lru_cache\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4fd07-fd56-408f-b190-4f83ccd2f95c",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "First we load the data. The basic idea is to create dictionaries with features associated to the drugs and cell-lines. In principle, the splits and the data shouldn't be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b7d3c4-e803-4ba4-ba37-91295eb04378",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize = None)\n",
    "def get_data(n_fold = 0, fp_radius = 2):\n",
    "    # 1\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    # 2\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    tensor_exp = torch.Tensor(filtered_rna.to_numpy())\n",
    "    cell_dict = {cell: tensor_exp[i] for i, cell in enumerate(filtered_rna.index.to_numpy())}\n",
    "    # 3\n",
    "    data = pd.read_csv(\"data/GDSC1.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = data.query(\"SANGER_MODEL_ID in @cell_dict.keys() & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "    # 4 \n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    test_lines = folds[0] # ?? 질문 필요. 0번째 fold를 test로 쓴다면, 이후에도 train_idx에서 0을 제거해야하는것 아닌가? 만약 train_idx에서 n_fold를 제거할 것이라면, trest_lines를 folds[n_fold]로 바꿔야하지 않는가? \n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold] # 아 여기서 오버라이드 됐네? 그럼 위에 test_lines는 왜 있는거지?\n",
    "    # 5\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "\n",
    "    return (scripts.OmicsDataset(cell_dict, drug_dict, train_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, validation_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef20e1c0",
   "metadata": {},
   "source": [
    "데이터를 처리하여 특정 cell line과 약물 간 상호작용을 예측하기 위한 데이터 셋을 구축한다.     \n",
    "여기서는 데이터를 학습, 검증, 테스트용으로 분할하고 이를 `OmicsDataset`형태로 반환한다. \n",
    "\n",
    "1. \n",
    "   - `@lru_cache(maxsize=None)`는 `functools` 모듈에 포함된 데코레이터로 Least Recently Used 캐싱전략을 사용한다. 이 함수는 아래 함수의 결과를 캐시에 저장하여, 동일한 입력에 대해 함수 호출을 최적화한다. 최대 크기를 `None`으로 설정해 캐시가 무한으로 사용된다.\n",
    "   - `fp`에서는 `FingerprintFeaturizer`를 사용하여 molecule fingerprint를 생성하는데, 이때 반경을 지정된 `fp_radius`로 지정하여 SMILES 데이터를 약물 딕셔너리(`drug_dict`)로 변환한다. \n",
    "2. \n",
    "   - `driver_genes` 유전자 심볼을 불러와 데이터에 결측치를 제거하고 리스트로 저장한다. 이후, `rnaseq`에 대한 데이터도 불러온다. `rnaseq.columns.isin(driver_genes)`는 `rnaseq` 의 각 칼럼의 이름이 `driver_genes` 목록에 포함되어 있는지를 `True` 혹은 `False`로 반환한다. 최종적으로 논리적 인덱스 배열을 생성하여 `driver_columns`에 저장한다. 이는 `True`인 유전자 열만 필터링하여 `filtered_rna`에 저장하는데 쓰인다.\n",
    "   - 필터링된 유전자 컬럼은 `tensor`로 변환되어 `cell_dict`에 딕셔너리 형태로 저장된다. `Tensor`는 PyTorch에서 수치 데이터를 저장하고 다루기 위해 사용하는 기본적인 데이터 구조이다. 텐서는 GPU에서 연산이 가능하여 대규모 데이터에 대해 신경망을 이용해 빠른 계산을 수행할 수 있다. \n",
    "   - `filtered_rna`의 로우 인덱스(각 cell line의 이름 또는 ID)를 Numpy 배열로 변환하여, 각 cell line의 이름을 `cell`이라는 key, `tensor_exp[i]`를 RNA 발현값 텐서로 저장하여 딕셔너리로 만든다.   \n",
    "3. \n",
    "   - GDSC 데이터를 불러와, `SANGER_MODEL_ID`가 `cell_dict`에, 그리고 `DRUG_ID`가 `drug_dict`에 존재하는 항목만 남긴다. 또한, 모든 행에서 `SANGER_MODEL_ID` 열만 선택하여, 중복을 제거하여 저장한다. \n",
    "4. \n",
    "   - 일괄적으로 `420`번 시드를 사용하여 `unique_cell_line`을 셔플하고, 데이터를 10개의 fold로 고정적으로 분할한다. 이때, 첫번째 fold를 `test_lines`로 사용한다. \n",
    "   - `train_idxs`에 0부터 9까지의 숫자로 구성된 리스트를 넣는다. 이는 이후 fold의 인덱스를 가리키게 된다. 이후 지정된 \"n_fold\"를 인덱스에서 삭제한다. \n",
    "   - train_idxs의 리스트에서 무작위로 하나의 인덱스를 선택하여 `validation_idx`로 사용한다. \n",
    "   - `folds[idx] for idx in train_idxs`는 train 데이터로 사용할 fold들의 세포주 ID 배열들을 추출하고, `np.concatenate`를 사용하여 추출된 배열들을 하나로 이어붙여 최종 train cell line ID list를 만든다. \n",
    "5. \n",
    "   - query는 데이터 프레임에서 SQL 스타일의 조건식을 통해 데이터를 필터링하는 메서드이다. \n",
    "   - 쿼리를 사용하여, 각 데이터셋에 `SANGER_MODEL_ID`가 포함된 row만 추출한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccda52-600d-4931-b7ca-5c06db7d2d9f",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "we declare the configuration, this is going to be model-specific and we get the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c23383-21e3-4fdd-a6fa-4b181c451af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"features\" : {\"fp_radius\":2}, # chemical의 fingerprint 생성 radius를 2로 설정\n",
    "          \"optimizer\": {\"batch_size\": 220, # 한번에 학습시킬 데이터의 양\n",
    "                        \"clip_norm\":19, # 그라디언트 클리핑에 사용할 최대 norm 값\n",
    "                        \"learning_rate\": 0.0004592646200179472, # 학습률\n",
    "                        \"stopping_patience\":15}, # 개선되지 않는 epoch가 15번 이상 나오면 학습을 중단\n",
    "          \"model\":{\"embed_dim\":485, # input을 embedding할 때 사용할 차원\n",
    "                 \"hidden_dim\":696, # hidden layer의 차원\n",
    "                 \"dropout\":0.48541242824674574, # 40퍼센트의 노드를 랜덤하게 드랍아웃 \n",
    "                 \"n_layers\": 4, # 3개의 hidden layer를 사용\n",
    "                 \"norm\": \"batchnorm\"}, # batch normalization을 사용하여 모델이 학습 중 출력 분포를 정규화하여 학습을 안정화\n",
    "         \"env\": {\"fold\": 0, # 0번째 fold를 사용하여 학습. 이는 음 n_fold에 들어갈 값을 의미하는 듯 하다. \n",
    "                \"device\":\"cuda:2\", # GPU자원을 사용할 장치를 지정한다. \n",
    "                 \"max_epochs\": 100, # 최대 epoch 수 \n",
    "                 \"search_hyperparameters\":False}} # hyper parameter 이미 있으니 안쓴다.\n",
    "\n",
    "# params={'batch_size': 220, 'clip_norm': 19, 'dropout': 0.48541242824674574, 'embed_dim': 485, 'hidden_dim': 696, 'learning_rate': 0.0004592646200179472, 'n_layers': 4, 'norm': 'batchnorm'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f178cd",
   "metadata": {},
   "source": [
    "- Gradient Clipping:\n",
    "  -  Gradient의 크기를 threshold 이하로 제한하는 방식으로, 일반적으로 Norm을 기준으로 수행한다. \n",
    "  -  Norm Clipping은 Gradient vector의 Norm을 계산하고, 이 값이 지정된 threshold보다 크면 Gradient를 threshold로 scaling한다. 즉, 그냥 threshold 보다 큰값을 threshold 값으로 변환한다. \n",
    "  -  이는 Gradient가 지나치게 커지는 그라디엔트 폭주 현상을 막고, 모델이 안정적으로 학습할 수 있도록 돕는다. \n",
    "- Batch Normalization:\n",
    "  - 인공 신경망 학습 시 각 레이어의 출력 분포를 정규화하여 학습 속도를 향상시키고, weight initialization에 대한 robustness를 증가시키고, 모델을 regularization한다. \n",
    "  - 딥러닝 모델이 매우 깊어질 때 발생하는 [Internal Covariate Shift](https://cvml.tistory.com/5)문제를 줄여준다. \n",
    "  - Covariant shift는 input data의 분포가 test와 train에서 각각 다르게 나타나는 현상을 의미한다. 그리고 이게 뉴럴 네트워크 내부에서 일어날 때 Internal covariant shift라고 부른다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f80c659-4416-4017-902f-4d0bea4ccfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:21] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:02:22] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset, test_dataset = get_data(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eca57c-2e0a-42e6-8c9c-38b2ea9cb19c",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization\n",
    "\n",
    "we wrap the function for training the model in a function that can be used by optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f25cf7-b977-48da-b569-8d231a6ac8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_optuna(trial, config):\n",
    "    # 1 \n",
    "    def pruning_callback(epoch, train_r):\n",
    "        trial.report(train_r, step = epoch)\n",
    "        if np.isnan(train_r):\n",
    "            raise optuna.TrialPruned()\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    # 2\n",
    "    config[\"model\"] = {\"embed_dim\": trial.suggest_int(\"embed_dim\", 64, 512),\n",
    "                    \"hidden_dim\": trial.suggest_int(\"hidden_dim\", 64, 2048),\n",
    "                    \"n_layers\": trial.suggest_int(\"n_layers\", 1, 6),\n",
    "                    \"norm\": trial.suggest_categorical(\"norm\", [\"batchnorm\", \"layernorm\", None]),\n",
    "                    \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.5)}\n",
    "    config[\"optimizer\"] = { \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-1, log=True),\n",
    "                            \"clip_norm\": trial.suggest_int(\"clip_norm\", 0.1, 20),\n",
    "                            \"batch_size\": trial.suggest_int(\"batch_size\", 128, 512),\n",
    "                            \"stopping_patience\":10}\n",
    "    # 3\n",
    "    try:\n",
    "        R, model = scripts.train_model(config,\n",
    "                                       train_dataset,\n",
    "                                       validation_dataset,\n",
    "                                       use_momentum=True,\n",
    "                                       callback_epoch = pruning_callback)\n",
    "        return R\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de1dab0",
   "metadata": {},
   "source": [
    "- Optuna 라이브러리를 사용하여 모델의 하이퍼파라미터를 자동으로 탐색하기위한 `train_model_optuna`함수를 정의\n",
    "- Optuna는 베이지안 최적화 기반의 하이퍼파라미터 최적화 라이브러리이다. `trial`객체를 통해 하이퍼파라미터를 샘플링하고 모델을 학습시킨다. \n",
    "- 위에서 이미 설정한 기본 `config`의 일부 항목을 수정하여 테스트한다.  \n",
    "\n",
    "1. \n",
    "    - `pruning_callback`함수를 정의한다. 이 함수는 학습 도중 성능이 저하되거나, 개선이 없을때 early stopping을 수행하는 콜백 함수(특정 이벤트 발생 시 자동 호출되는 함수)이다. pruning은 가지치기를 의미. 성능이 좋지않은 하이퍼파라미터 조합을 조기에 종료하여 속도 향상\n",
    "    - `trial.report(train_r, step=epoch)`는 현재 epoch에서 학습성능(`train_r`)을 보고한다. 이를 통해 하이퍼파라미터의 성능을 평가한다.\n",
    "    - `if np.isnan(train_r)`는 보고가 `NaN`인지 확인한다. `NaN`인 경우, 학습에 문제가 발생했음을 의미한다. 이때 `optuna.TrialPruned()` 예외를 발생시켜 학습을 조기에 종료한다. \n",
    "    - `if trial.should_prune()`는 Optuna가 특정 trial을 중단해야할 조건을 충족하면 예외를 발생시켜 조기종료한다. \n",
    "2. \n",
    "    - `trial.suggest_*` 메서드를 사용해 다양한 hyperparameter의 조합을 탐색. `trial` 객체는 무작위 샘플링 객체이다. \n",
    "    - 예를 들어 `(\"embed_dim\", 64, 512)`경우에는, 64와 512사이의 값을 샘플링한다. \n",
    "    - `(\"norm\", [\"batchnorm\", \"layernorm\", None])`에서는 셋 중 하나를 샘플링하여 레이어의 정규화 방식을 선택한다. \n",
    "3. \n",
    "    - `try`-`except`를 사용하여 오류를 처리한다. \n",
    "    - `scripts.train_model`을 호출하여 모델을 지정된 suggest된 하이퍼파라미터로 학습시키고, 최적화한 성능 지표 `R`과 학습된 모델을 반환한다. \n",
    "    - 오류가 발생할 경우 0을 반환하고 오류 내용을 출력한다. 이로 인해 모델이 특정 하이퍼파라미터 조합에서 학습되지 않으면, 해당 조합은 무시되고 최적화가 계속 진행된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4fad963-1f99-4aa3-8f1e-f5c614ef3bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "if config[\"env\"][\"search_hyperparameters\"]:\n",
    "    study_name = f\"baseline_model\"\n",
    "    storage_name = \"sqlite:///studies/{}.db\".format(study_name)\n",
    "    # 2\n",
    "    study = optuna.create_study(study_name=study_name,\n",
    "                                storage=storage_name,\n",
    "                                direction='maximize',\n",
    "                                load_if_exists=True,\n",
    "                                pruner=optuna.pruners.MedianPruner(n_startup_trials=30,\n",
    "                                                               n_warmup_steps=5,\n",
    "                                                               interval_steps=5))\n",
    "    # 3\n",
    "    objective = lambda x: train_model_optuna(x, config)\n",
    "    study.optimize(objective, n_trials=40)\n",
    "    best_config = study.best_params\n",
    "    print(best_config)\n",
    "    config[\"model\"][\"embed_dim\"] = best_config[\"embed_dim\"]\n",
    "    config[\"model\"][\"hidden_dim\"] = best_config[\"hidden_dim\"]\n",
    "    config[\"model\"][\"n_layers\"] = best_config[\"n_layers\"]\n",
    "    config[\"model\"][\"norm\"] = best_config[\"norm\"]\n",
    "    config[\"model\"][\"dropout\"] = best_config[\"dropout\"]\n",
    "    config[\"optimizer\"][\"learning_rate\"] = best_config[\"learning_rate\"]\n",
    "    config[\"optimizer\"][\"clip_norm\"] = best_config[\"clip_norm\"]\n",
    "    config[\"optimizer\"][\"batch_size\"] = best_config[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541846a7",
   "metadata": {},
   "source": [
    "1. \n",
    "    - \"search_hyperparameters\" 모드가 켜져있을때만 수행\n",
    "2. \n",
    "    - Optuna에서 하이퍼파라미터 최적화를 수행할 때, `optuna.create_study()`와 `study.optimize()`가 실행된다.\n",
    "    - 여기서 study name은 그저 baseline model이다. 하이퍼파라미터 탐색 결과는 SQLite 데이터베이스의 `studies/baseline_model.db`에 저장된다. \n",
    "    - `load_if_exists=True`는 동일한 `study_name`이 이미 존재하면 기존의 결과를 불러온다. 기존 결과를 이어서 탐색할 수 있게 해준다. \n",
    "    - `pruner=optuna.pruners.MedianPruner(...)`는 Pruning 전략으로 `MedianPruner`를 사용한다. 이는 일정 단계 후 현재 trial의 성능이 이전 trial 성능의 median보다 작을 경우 해당 trial을 중단시킨다. \n",
    "    - 여기서는 최소 30번의 trial, 첫 5번의 epoch(=step)를 Pruning 없이 실행하여 초기 성능을 확인한다. 이후 5 스텝마다 성능을 확인하고 Pruning 여부를 결정한다. Trial은 step을 포함한다. \n",
    "3. \n",
    "    - 미리 정의해둔 `train_model_optuna` 함수를 objective function으로 사용하고, 이걸 maximize한다. \n",
    "    - `study.optimize()`함수를 `n_trials`만큼 실행한다. \n",
    "    - best parameters를 받아 config을 업데이트한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620070ab-de87-486d-8a14-ca3791ff2803",
   "metadata": {},
   "source": [
    "# Model training and evaluation\n",
    "\n",
    "After we have a set of optimal hyperparameters we train our model. The train model function could be changed, but:\n",
    "- test_dataset cannot be used until we call the final evaluation step\n",
    "- the evaluation step cannot be modified, it must take the model produced by your pipeline, a dataloader that provides the correct data for your model, and the final metrics have to be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8e603f6-c81a-4cb1-8c1b-d1d834a94169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim14/.local/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: FutureWarning: The default value for `maximize` will be changed from `True` to `None` in v1.7.0 of TorchMetrics,will automatically infer the value based on the `higher_is_better` attribute of the metric (if such attribute exists) or raise an error if it does not. If you are explicitly setting the `maximize` argument to either `True` or `False` already, you can ignore this warning.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0: train loss: 1.9204309752527273 Smoothed R interaction (validation) None\n",
      "epoch : 1: train loss: 1.5204692332249767 Smoothed R interaction (validation) None\n",
      "epoch : 2: train loss: 1.4354846109196824 Smoothed R interaction (validation) None\n",
      "epoch : 3: train loss: 1.3596970630141925 Smoothed R interaction (validation) None\n",
      "epoch : 4: train loss: 1.3097563190842574 Smoothed R interaction (validation) None\n",
      "epoch : 5: train loss: 1.2619699643467956 Smoothed R interaction (validation) None\n",
      "epoch : 6: train loss: 1.2169348020036266 Smoothed R interaction (validation) None\n",
      "epoch : 7: train loss: 1.1833453347098153 Smoothed R interaction (validation) None\n",
      "epoch : 8: train loss: 1.1471325890073236 Smoothed R interaction (validation) None\n",
      "epoch : 9: train loss: 1.124045192633035 Smoothed R interaction (validation) None\n",
      "epoch : 10: train loss: 1.096354653925266 Smoothed R interaction (validation) None\n",
      "epoch : 11: train loss: 1.0771197381446946 Smoothed R interaction (validation) None\n",
      "epoch : 12: train loss: 1.0550278808148401 Smoothed R interaction (validation) None\n",
      "epoch : 13: train loss: 1.0330487133197064 Smoothed R interaction (validation) None\n",
      "epoch : 14: train loss: 1.0205097886188974 Smoothed R interaction (validation) None\n",
      "epoch : 15: train loss: 1.0029285714873728 Smoothed R interaction (validation) None\n",
      "epoch : 16: train loss: 0.987026398305623 Smoothed R interaction (validation) None\n",
      "epoch : 17: train loss: 0.9747966437407259 Smoothed R interaction (validation) None\n",
      "epoch : 18: train loss: 0.9625831714216269 Smoothed R interaction (validation) None\n",
      "epoch : 19: train loss: 0.9460799100264063 Smoothed R interaction (validation) None\n",
      "epoch : 20: train loss: 0.9350439232475353 Smoothed R interaction (validation) None\n",
      "epoch : 21: train loss: 0.9218795119591479 Smoothed R interaction (validation) None\n",
      "epoch : 22: train loss: 0.9046546102132438 Smoothed R interaction (validation) None\n",
      "epoch : 23: train loss: 0.8958546971375088 Smoothed R interaction (validation) None\n",
      "epoch : 24: train loss: 0.8850187811086763 Smoothed R interaction (validation) None\n",
      "epoch : 25: train loss: 0.876628924709446 Smoothed R interaction (validation) None\n",
      "epoch : 26: train loss: 0.863970180390016 Smoothed R interaction (validation) None\n",
      "epoch : 27: train loss: 0.8550210925768007 Smoothed R interaction (validation) None\n",
      "epoch : 28: train loss: 0.8481161461686189 Smoothed R interaction (validation) None\n",
      "epoch : 29: train loss: 0.8385179238499335 Smoothed R interaction (validation) None\n",
      "epoch : 30: train loss: 0.8305508755288034 Smoothed R interaction (validation) None\n",
      "epoch : 31: train loss: 0.819664266143205 Smoothed R interaction (validation) None\n",
      "epoch : 32: train loss: 0.8129620001001178 Smoothed R interaction (validation) None\n",
      "epoch : 33: train loss: 0.8029183112225442 Smoothed R interaction (validation) None\n",
      "epoch : 34: train loss: 0.7958826938890061 Smoothed R interaction (validation) None\n",
      "epoch : 35: train loss: 0.791195999282711 Smoothed R interaction (validation) None\n",
      "epoch : 36: train loss: 0.7812018948062411 Smoothed R interaction (validation) None\n",
      "epoch : 37: train loss: 0.7748218679765485 Smoothed R interaction (validation) None\n",
      "epoch : 38: train loss: 0.768511607708796 Smoothed R interaction (validation) None\n",
      "epoch : 39: train loss: 0.7615418082700586 Smoothed R interaction (validation) None\n",
      "epoch : 40: train loss: 0.756880610815759 Smoothed R interaction (validation) None\n",
      "epoch : 41: train loss: 0.7472144049855898 Smoothed R interaction (validation) None\n",
      "epoch : 42: train loss: 0.7431539901866103 Smoothed R interaction (validation) None\n",
      "epoch : 43: train loss: 0.7349553333981982 Smoothed R interaction (validation) None\n",
      "epoch : 44: train loss: 0.7289412844293522 Smoothed R interaction (validation) None\n",
      "epoch : 45: train loss: 0.7256789030050331 Smoothed R interaction (validation) None\n",
      "epoch : 46: train loss: 0.7177975154147958 Smoothed R interaction (validation) None\n",
      "epoch : 47: train loss: 0.7138052138798642 Smoothed R interaction (validation) None\n",
      "epoch : 48: train loss: 0.7096970498561859 Smoothed R interaction (validation) None\n",
      "epoch : 49: train loss: 0.7006373090845234 Smoothed R interaction (validation) None\n",
      "epoch : 50: train loss: 0.69858005153683 Smoothed R interaction (validation) None\n",
      "epoch : 51: train loss: 0.6928261124019353 Smoothed R interaction (validation) None\n",
      "epoch : 52: train loss: 0.6868090442047929 Smoothed R interaction (validation) None\n",
      "epoch : 53: train loss: 0.6820030028246483 Smoothed R interaction (validation) None\n",
      "epoch : 54: train loss: 0.6797819171592875 Smoothed R interaction (validation) None\n",
      "epoch : 55: train loss: 0.6754708703396455 Smoothed R interaction (validation) None\n",
      "epoch : 56: train loss: 0.6707292545516536 Smoothed R interaction (validation) None\n",
      "epoch : 57: train loss: 0.6641340605211707 Smoothed R interaction (validation) None\n",
      "epoch : 58: train loss: 0.660838746237305 Smoothed R interaction (validation) None\n",
      "epoch : 59: train loss: 0.6571584023394674 Smoothed R interaction (validation) None\n",
      "epoch : 60: train loss: 0.6520758184903073 Smoothed R interaction (validation) None\n",
      "epoch : 61: train loss: 0.6509471821616281 Smoothed R interaction (validation) None\n",
      "epoch : 62: train loss: 0.6443123239953563 Smoothed R interaction (validation) None\n",
      "epoch : 63: train loss: 0.6384215368133671 Smoothed R interaction (validation) None\n",
      "epoch : 64: train loss: 0.6324535532098896 Smoothed R interaction (validation) None\n",
      "epoch : 65: train loss: 0.6327436882369923 Smoothed R interaction (validation) None\n",
      "epoch : 66: train loss: 0.6305056950393713 Smoothed R interaction (validation) None\n",
      "epoch : 67: train loss: 0.6256516177980405 Smoothed R interaction (validation) None\n",
      "epoch : 68: train loss: 0.6224419011822286 Smoothed R interaction (validation) None\n",
      "epoch : 69: train loss: 0.6179035626773565 Smoothed R interaction (validation) None\n",
      "epoch : 70: train loss: 0.6151991666487928 Smoothed R interaction (validation) None\n",
      "epoch : 71: train loss: 0.6100441348721396 Smoothed R interaction (validation) None\n",
      "epoch : 72: train loss: 0.6049758183787454 Smoothed R interaction (validation) None\n",
      "epoch : 73: train loss: 0.601164203291794 Smoothed R interaction (validation) None\n",
      "epoch : 74: train loss: 0.6030351635138943 Smoothed R interaction (validation) None\n",
      "epoch : 75: train loss: 0.5955379759365658 Smoothed R interaction (validation) None\n",
      "epoch : 76: train loss: 0.5922352136306043 Smoothed R interaction (validation) None\n",
      "epoch : 77: train loss: 0.5917127256966986 Smoothed R interaction (validation) None\n",
      "epoch : 78: train loss: 0.5859223976450146 Smoothed R interaction (validation) None\n",
      "epoch : 79: train loss: 0.5825199287736191 Smoothed R interaction (validation) None\n",
      "epoch : 80: train loss: 0.5819435592811063 Smoothed R interaction (validation) None\n",
      "epoch : 81: train loss: 0.5764698803987143 Smoothed R interaction (validation) None\n",
      "epoch : 82: train loss: 0.5752179860787572 Smoothed R interaction (validation) None\n",
      "epoch : 83: train loss: 0.5708596923722411 Smoothed R interaction (validation) None\n",
      "epoch : 84: train loss: 0.5661358297995801 Smoothed R interaction (validation) None\n",
      "epoch : 85: train loss: 0.5637293020509324 Smoothed R interaction (validation) None\n",
      "epoch : 86: train loss: 0.5630563136260465 Smoothed R interaction (validation) None\n",
      "epoch : 87: train loss: 0.5597157664456458 Smoothed R interaction (validation) None\n",
      "epoch : 88: train loss: 0.5547084822407309 Smoothed R interaction (validation) None\n",
      "epoch : 89: train loss: 0.5520910513007415 Smoothed R interaction (validation) None\n",
      "epoch : 90: train loss: 0.5506884074717198 Smoothed R interaction (validation) None\n",
      "epoch : 91: train loss: 0.5468740542261106 Smoothed R interaction (validation) None\n",
      "epoch : 92: train loss: 0.545331985601839 Smoothed R interaction (validation) None\n",
      "epoch : 93: train loss: 0.5430568548587134 Smoothed R interaction (validation) None\n",
      "epoch : 94: train loss: 0.5390726203626057 Smoothed R interaction (validation) None\n",
      "epoch : 95: train loss: 0.5340631919087104 Smoothed R interaction (validation) None\n",
      "epoch : 96: train loss: 0.531101616373602 Smoothed R interaction (validation) None\n",
      "epoch : 97: train loss: 0.5294473680685151 Smoothed R interaction (validation) None\n",
      "epoch : 98: train loss: 0.5325495201742874 Smoothed R interaction (validation) None\n",
      "epoch : 99: train loss: 0.525575285334632 Smoothed R interaction (validation) None\n"
     ]
    }
   ],
   "source": [
    "_, model = scripts.train_model(config, torch.utils.data.ConcatDataset([train_dataset, validation_dataset]), None, use_momentum=False)\n",
    "device = torch.device(config[\"env\"][\"device\"])\n",
    "metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(\n",
    "    {\"R_cellwise_residuals\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"drugs\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=True),\n",
    "    \"R_cellwise\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"cell_lines\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=False),\n",
    "    \"MSE\":torchmetrics.MeanSquaredError()}))\n",
    "metrics.to(device)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                       batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                       drop_last=False,\n",
    "                                      shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8076a6",
   "metadata": {},
   "source": [
    "- best parameters를 사용하여 모델을 트레이닝한다. 이 때, `train_dataset`과 `validation_dataset`을 `ConcatDataset`을 통해 하나로 병합하여 사용한다. \n",
    "- `use_momentom = False`를 통해 모멘텀을 사용하지 않는다. \n",
    "  - 모멘텀은 딥러닝 학습 시 최적화 알고리즘이 사용하는 기법 중 하나로, 이전 단계의 그라디언트를 일정 비율로 현재 단계에 반영하여 학습을 안정화하고 가속하는 역할을 한다. \n",
    "  - 이로 인해 좁은 골짜기 형태의 loss 곡면에서는 진동을 줄여주고, 완만한 경사면에서는 더 빠르게 이동하여 최적화 속도를 높인다. 그런데 여기서는 안쓴다고 했으니, 여기서는 이전 단계의 그라디언트 방향을 고려하지 않는다. \n",
    "- `env`의 `device` 항목(cuda:3)을 `torch.device`로 설정한다. \n",
    "- `MetricTracker`를 통해서 메트릭을 설정한다. `MetricCollection`을 통해 여러개의 메트릭을 정의한 후, 트래커로 묶어 관리한다. \n",
    "  - `R_cellwise_residual`에서 피어슨 상관계수를 기반으로, `drugs` 단위로 계산한다.  \n",
    "  - 메트릭 계산 시 residual을 사용하여 성능을 평가하고, `average=\"macro\"`는 그룹별 성능을 균등하게 반영하여 전체 평균을 계산한다.\n",
    "  - `R_cellwise`는 `grouping=\"cell_lines\"`로 세포주 단위로 계산한다. 얘는 residual 없이 메트릭을 계산한다.\n",
    "- `metrics.to(device)`는 정의된 메트릭을 `device`에 전송하여 GPU에서 사용할 수 있게한다. \n",
    "- `torch.utils.data.DataLoader`를 통해, 테스트 데이터셋을 기반으로 데이터 로더를 생성하여, 모델의 성능을 테스트할 준비를 한다.\n",
    "  - `drop_last=False`를 사용하여 마지막 배치의 샘플 수가 배치 크기보다 작을 경우에도 이를 버리지 않고 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d785c4fb-4bd8-4bd2-b90b-81cb6e701cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/kim14/project_work/scripts/models.py:66: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3683.)\n",
      "  return torch.linalg.solve(A, Xy).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 1.9241538047790527, 'R_cellwise': 0.8880401253700256, 'R_cellwise_residuals': 0.31074970960617065}\n"
     ]
    }
   ],
   "source": [
    "final_metrics = scripts.evaluate_step(model, test_dataloader, metrics, device)\n",
    "print(final_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb80db5-68ec-4c58-9059-ab2f1b8b2f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
