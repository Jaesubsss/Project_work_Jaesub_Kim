!nvidia-smi


import pandas as pd
import numpy as np
import torch
import scripts
from functools import lru_cache
import torchmetrics
from torch import nn
import optuna
from torch.utils.data import Dataset
from torch import Tensor
import torch.nn.functional as F





config = {"features" : {"fp_radius":2}, # chemical의 fingerprint 생성 radius를 2로 설정
          "optimizer": {"batch_size": 220, # 한번에 학습시킬 데이터의 양
                        "clip_norm":19, # 그라디언트 클리핑에 사용할 최대 norm 값
                        "learning_rate": 0.0004592646200179472, # 학습률
                        "stopping_patience":15}, # 개선되지 않는 epoch가 15번 이상 나오면 학습을 중단
          "model":{"embed_dim":485, # input을 embedding할 때 사용할 차원
                 "hidden_dim":696, # hidden layer의 차원
                 "dropout":0.48541242824674574, # 40퍼센트의 노드를 랜덤하게 드랍아웃 
                 "n_layers": 4, # 3개의 hidden layer를 사용
                 "norm": "batchnorm"}, # batch normalization을 사용하여 모델이 학습 중 출력 분포를 정규화하여 학습을 안정화
         "env": {"fold": 0, # 0번째 fold를 사용하여 학습. 이는 음 n_fold에 들어갈 값을 의미하는 듯 하다. 
                "device":"cuda:0", # GPU자원을 사용할 장치를 지정한다. 
                 "max_epochs": 100, # 최대 epoch 수 
                 "search_hyperparameters":False}} # hyper parameter 이미 있으니 안쓴다.











# Define Model

class ResSubNet(nn.Module):
    def __init__(self, embed_dim=256, hidden_dim=1024, dropout=0.1, n_layers = 6, norm = "layernorm"):
        super().__init__()
        self.mlps = nn.ModuleList()
        if norm == "layernorm":
            norm = nn.LayerNorm
        elif norm == "batchnorm":
            norm = nn.BatchNorm1d
        else:
            norm = nn.Identity
        for l in range(n_layers):
            self.mlps.append(nn.Sequential(nn.Linear(embed_dim, hidden_dim),
                                           norm(hidden_dim),
                                     nn.ReLU(),
                                     nn.Dropout(dropout),
                                     nn.Linear(hidden_dim, embed_dim)))
        self.lin = nn.Linear(embed_dim, 1)
    def forward(self, x): 
        for l in self.mlps:
            x = (l(x) + x)/2
        return x # basically same, but feature vector

# sub-network model using ResNet

class SubNetworkModel(nn.Module):
    def __init__(self, embed_dim=256,
                 hidden_dim=1024,
                 dropout=0.1,
                 n_layers = 6,
                 norm = "layernorm"):
        super().__init__()
        self.ressubnet = ResSubNet(embed_dim, hidden_dim, dropout, n_layers, norm)
        self.embed_d = nn.Sequential(nn.LazyLinear(embed_dim), nn.ReLU())
        self.embed_c = nn.Sequential(nn.LazyLinear(embed_dim), nn.ReLU())
    def forward(self, c, d):
        return self.ressubnet(self.embed_d(d) + self.embed_c(c).squeeze(dim=1))
        # Lazylinear or Linear layer transform the inputdata from 2D to 3D tensor for c
        # therefore, value of self.embed_c(c) has different size with value of self.embed_d(d)
        # so using squeeze method, reduce dimension of self.embed_c(c)






class MultiOmicsDataset(Dataset):
    def __init__(self, rna_omic_dicts, pro_omic_dicts, mut_omic_dicts, met_omic_dicts, cnv_omic_dicts, drug_dict, data):
        self.rna_omic_dicts = rna_omic_dicts  
        self.pro_omic_dicts = pro_omic_dicts
        self.mut_omic_dicts = mut_omic_dicts
        self.met_omic_dicts = met_omic_dicts
        self.cnv_omic_dicts = cnv_omic_dicts
        self.drug_dict = drug_dict    # common drug_dict
        self.cell_mapped_ids = {key:i for i, key in enumerate(self.rna_omic_dicts.keys())} # those mapped ids are also common
        self.drug_mapped_ids = {key:i for i, key in enumerate(self.drug_dict.keys())}
        self.data = data              # common train_data, validation_data, test_data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # 현재 샘플 데이터 가져오기
        instance = self.data.iloc[idx]
        cell_id = instance["SANGER_MODEL_ID"]
        drug_id = instance["DRUG_ID"]
        target = instance["LN_IC50"]

        # 약물 데이터
        drug_features = torch.tensor(self.drug_dict[drug_id], dtype=torch.float32)

        # 타겟 값
        target = torch.tensor([target], dtype=torch.float32)

        return (self.rna_omic_dicts[cell_id].unsqueeze(0), # 
                self.pro_omic_dicts[cell_id].unsqueeze(0),
                self.mut_omic_dicts[cell_id].unsqueeze(0),
                self.met_omic_dicts[cell_id].unsqueeze(0),
                self.cnv_omic_dicts[cell_id].unsqueeze(0),
                self.drug_dict[drug_id],
                Tensor([target]),
                Tensor([self.cell_mapped_ids[cell_id]]),
                Tensor([self.drug_mapped_ids[drug_id]])) 





# Modified get data function
def get_data_subnetwork(n_fold = 0, fp_radius = 2):

    # drug
    smile_dict = pd.read_csv("data/smiles.csv", index_col=0)
    fp = scripts.FingerprintFeaturizer(R = fp_radius)
    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])

    # loading all datasets ---
    driver_genes = pd.read_csv("data/driver_genes.csv").loc[:, "symbol"].dropna()

    rnaseq = pd.read_csv("data/rnaseq_normcount.csv", index_col=0)
    driver_columns = rnaseq.columns.isin(driver_genes)
    filtered_rna = rnaseq.loc[:, driver_columns]

    proteomics = pd.read_csv("data/proteomics.csv", index_col=0)

    mutation = pd.read_csv("data/binary_mutations.csv")
    mutation.columns = mutation.iloc[0]
    mutation = mutation.iloc[2:,:].set_index("gene_symbol")
    driver_columns = mutation.columns.isin(driver_genes)
    filtered_mut = mutation.loc[:, driver_columns]
    filtered_mut = filtered_mut.astype(float)

    methylations = pd.read_csv("data/methylations.csv",index_col = 0).sort_index(ascending = True)

    cnvs = pd.read_csv("data/copy_number_variations.csv",index_col= 0)

    # concatenate all dataset  -----
    # inner join based on index: model_ids with NaN are automatically filtered out 
    data_concat = pd.concat([filtered_rna, proteomics, filtered_mut, methylations, cnvs], axis=1, join='inner')

    # choosing dataset to be used for training model -----
    data_rna = filtered_rna[filtered_rna.index.isin(data_concat.index)]
    tensor_rna = torch.Tensor(data_rna.to_numpy())
    rna_cell_dict = {cell: tensor_rna[i] for i, cell in enumerate(data_rna.index.to_numpy())}
    
    data_pro = proteomics[proteomics.index.isin(data_concat.index)]
    tensor_pro = torch.Tensor(data_pro.to_numpy())
    pro_cell_dict = {cell: tensor_pro[i] for i, cell in enumerate(data_pro.index.to_numpy())}
    
    data_mut = filtered_mut[filtered_mut.index.isin(data_concat.index)]
    tensor_mut = torch.Tensor(data_mut.to_numpy())
    mut_cell_dict = {cell: tensor_mut[i] for i, cell in enumerate(data_mut.index.to_numpy())}
    
    data_met = methylations[methylations.index.isin(data_concat.index)]
    tensor_met = torch.Tensor(data_met.to_numpy())
    met_cell_dict = {cell: tensor_met[i] for i, cell in enumerate(data_met.index.to_numpy())}
    
    data_cnv = cnvs[cnvs.index.isin(data_concat.index)]
    tensor_cnv = torch.Tensor(data_cnv.to_numpy())
    cnv_cell_dict = {cell: tensor_cnv[i] for i, cell in enumerate(data_cnv.index.to_numpy())}

    # GDSC
    data = pd.read_csv("data/GDSC1.csv", index_col=0)
    # default, remove data where lines or drugs are missing:
    data = data.query("SANGER_MODEL_ID in @rna_cell_dict.keys() & DRUG_ID in @drug_dict.keys()") # shared dict.key()
    unique_cell_lines = data.loc[:, "SANGER_MODEL_ID"].unique()

    np.random.seed(420) # for comparibility, don't change it!
    np.random.shuffle(unique_cell_lines)
    folds = np.array_split(unique_cell_lines, 10)
    test_lines = folds[0] 
    train_idxs = list(range(10))
    train_idxs.remove(n_fold)
    np.random.seed(420)
    validation_idx = np.random.choice(train_idxs)
    train_idxs.remove(validation_idx)
    train_lines = np.concatenate([folds[idx] for idx in train_idxs])
    validation_lines = folds[validation_idx]
    test_lines = folds[n_fold] 
    # 5
    train_data = data.query("SANGER_MODEL_ID in @train_lines")
    validation_data = data.query("SANGER_MODEL_ID in @validation_lines")
    test_data = data.query("SANGER_MODEL_ID in @test_lines")

    return(MultiOmicsDataset(rna_cell_dict, pro_cell_dict, mut_cell_dict, met_cell_dict, cnv_cell_dict, drug_dict, train_data),
    MultiOmicsDataset(rna_cell_dict, pro_cell_dict, mut_cell_dict, met_cell_dict, cnv_cell_dict, drug_dict, validation_data),
    MultiOmicsDataset(rna_cell_dict, pro_cell_dict, mut_cell_dict, met_cell_dict, cnv_cell_dict, drug_dict, test_data))





# modified & intergrated model train function

def train_model_multiomics(config, train_dataset, validation_dataset=None, use_momentum=True, callback_epoch = None):
    class FinalRegressor(nn.Module): # final model
        def __init__(self, embed_dim=256, hidden_dim=1024, dropout=0.1, n_layers=6, norm="layernorm"):
            super(FinalRegressor, self).__init__()
            self.FN = nn.Sequential(
                    nn.Linear(embed_dim * 5, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, 1)  # Regression output
                )
        def forward(self, x):
            return self.FN(x)
        
    train_loader = torch.utils.data.DataLoader(train_dataset,
                                           batch_size=config["optimizer"]["batch_size"],
                                           drop_last=True,
                                          shuffle=True)
    loader = train_loader
    if validation_dataset is not None:
        val_loader = torch.utils.data.DataLoader(validation_dataset,
                                               batch_size=config["optimizer"]["batch_size"],
                                               drop_last=False,
                                              shuffle=False)
        loader = val_loader

    # Model
    submodel_rna = SubNetworkModel(**config["model"])
    submodel_pro = SubNetworkModel(**config["model"])
    submodel_mut = SubNetworkModel(**config["model"])
    submodel_met = SubNetworkModel(**config["model"])
    submodel_cnv = SubNetworkModel(**config["model"])
    model_fn = FinalRegressor(**config["model"])
    # optimizer for each model
    optimizer_rna = torch.optim.Adam(submodel_rna.parameters(), config["optimizer"]["learning_rate"])
    optimizer_pro = torch.optim.Adam(submodel_pro.parameters(), config["optimizer"]["learning_rate"])
    optimizer_mut = torch.optim.Adam(submodel_mut.parameters(), config["optimizer"]["learning_rate"])
    optimizer_met = torch.optim.Adam(submodel_met.parameters(), config["optimizer"]["learning_rate"])
    optimizer_cnv = torch.optim.Adam(submodel_cnv.parameters(), config["optimizer"]["learning_rate"])
    optimizer_fn = torch.optim.Adam(model_fn.parameters(), config["optimizer"]["learning_rate"])

    device = torch.device(config["env"]["device"])

    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_fn, factor=0.5, patience=5)
    early_stop = scripts.EarlyStop(config["optimizer"]["stopping_patience"])

    submodel_rna.to(device)
    submodel_pro.to(device)
    submodel_mut.to(device)
    submodel_met.to(device)
    submodel_cnv.to(device)
    model_fn.to(device)

    metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(
    {"R_cellwise_residuals":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,
                          grouping="drugs",
                          average="macro",
                          residualize=True),
    "R_cellwise":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,
                          grouping="cell_lines",
                          average="macro",
                          residualize=False),
    "MSE":torchmetrics.MeanSquaredError()}))
    metrics.to(device)
    use_momentum = True  
    
    for epoch in range(config["env"]["max_epochs"]):
        ### Train step ---------------------------------------------------------
        #train_loss = train_step(model, optimizer, train_loader, config, device)
        loss = nn.MSELoss()
        ls = []

        submodel_rna.train()
        submodel_pro.train()
        submodel_mut.train()
        submodel_met.train()
        submodel_cnv.train()
        model_fn.train()
        
        for x in loader:
            # get feature vectors from each sub-model
            FV_rna = submodel_rna(x[0].to(device), x[5].to(device))   
            FV_pro = submodel_pro(x[1].to(device), x[5].to(device))
            FV_mut = submodel_mut(x[2].to(device), x[5].to(device))
            FV_met = submodel_met(x[3].to(device), x[5].to(device))
            FV_cnv = submodel_cnv(x[4].to(device), x[5].to(device))
            # simply concatenate all feature vector, it is used for input of final model
            FV = torch.cat((FV_rna, FV_pro, FV_mut, FV_met, FV_cnv), dim=1)
            FV = F.normalize(FV, p=2, dim=1) # L2 normalization, check the paper.
            # final output 
            out = model_fn(FV)

            optimizer_rna.zero_grad()
            optimizer_pro.zero_grad()
            optimizer_mut.zero_grad()
            optimizer_met.zero_grad()
            optimizer_cnv.zero_grad()
            optimizer_fn.zero_grad()

            l = loss(out.squeeze(), x[6].to(device).squeeze())
            l.backward()

            torch.nn.utils.clip_grad_norm_(submodel_rna.parameters(), config["optimizer"]["clip_norm"])
            torch.nn.utils.clip_grad_norm_(submodel_pro.parameters(), config["optimizer"]["clip_norm"])
            torch.nn.utils.clip_grad_norm_(submodel_mut.parameters(), config["optimizer"]["clip_norm"])
            torch.nn.utils.clip_grad_norm_(submodel_met.parameters(), config["optimizer"]["clip_norm"])
            torch.nn.utils.clip_grad_norm_(submodel_cnv.parameters(), config["optimizer"]["clip_norm"])
            torch.nn.utils.clip_grad_norm_(model_fn.parameters(), config["optimizer"]["clip_norm"])

            ls += [l.item()]
            
            optimizer_rna.step()
            optimizer_pro.step()
            optimizer_mut.step()
            optimizer_met.step()
            optimizer_cnv.step()
            optimizer_fn.step()

        train_loss = np.mean(ls)

        lr_scheduler.step(train_loss)
        
        if validation_dataset is not None:
            # validation_metrics = scripts.evaluate_step(model,val_loader, metrics, device)
            # hmmm.. is this validation step really conducted? I'm not sure. 
            ### Evaluation step ---------------------------------------------------------
            metrics.increment()
            submodel_rna.eval()
            submodel_pro.eval()
            submodel_mut.eval()
            submodel_met.eval()
            submodel_cnv.eval()
            model_fn.eval()
            
            submodel_rna.to(device)
            submodel_pro.to(device)
            submodel_mut.to(device)
            submodel_met.to(device)
            submodel_cnv.to(device)
            model_fn.to(device)
            
            for x in loader:
                with torch.no_grad():
                    FV_rna = submodel_rna(x[0].to(device), x[5].to(device))   
                    FV_pro = submodel_pro(x[1].to(device), x[5].to(device))
                    FV_mut = submodel_mut(x[2].to(device), x[5].to(device))
                    FV_met = submodel_met(x[3].to(device), x[5].to(device))
                    FV_cnv = submodel_cnv(x[4].to(device), x[5].to(device))
                    
                    FV = torch.cat((FV_rna, FV_pro, FV_mut, FV_met, FV_cnv), dim=1)
                    FV = F.normalize(FV, p=2, dim=1) 
                    out = model_fn(FV)
                    
                    metrics.update(out.squeeze(),
                                x[6].to(device).squeeze(),
                                cell_lines = x[7].to(device).squeeze().to(device),
                                drugs = x[8].to(device).squeeze().to(device))
            validation_metrics = {it[0]:it[1].item() for it in metrics.compute().items()}
            
            
            
            if epoch > 0 & use_momentum:
                val_target = 0.2*val_target + 0.8*validation_metrics['R_cellwise_residuals']
            else:
                val_target = validation_metrics['R_cellwise_residuals']
        else:
            val_target = None
        if callback_epoch is None:
            print(f"epoch : {epoch}: train loss: {train_loss} Smoothed R interaction (validation) {val_target}")
        else:
            callback_epoch(epoch, val_target)
        if early_stop(train_loss):
            break
    return val_target, {"submodels": {  # all trained subnetworks are returned as well                     
            "rna": submodel_rna,
            "pro": submodel_pro,
            "mut": submodel_mut,
            "met": submodel_met,
            "cnv": submodel_cnv,
        },
        "final_model": model_fn         # final model 
    }





# get data
train_dataset, validation_dataset, test_dataset = get_data_subnetwork(n_fold = config["env"]["fold"], fp_radius = config["features"]["fp_radius"])





# model training
_, subnetwork_models = train_model_multiomics(config, torch.utils.data.ConcatDataset([train_dataset, validation_dataset]), None, use_momentum=False)
device = torch.device(config["env"]["device"])
metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(
    {"R_cellwise_residuals":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,
                          grouping="drugs",
                          average="macro",
                          residualize=True),
    "R_cellwise":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,
                          grouping="cell_lines",
                          average="macro",
                          residualize=False),
    "MSE":torchmetrics.MeanSquaredError()}))

metrics.to(device)

test_dataloader = torch.utils.data.DataLoader(test_dataset,
                                       batch_size=config["optimizer"]["batch_size"],
                                       drop_last=False,
                                      shuffle=False,pin_memory=True)





### Test Evaluation ---

# get trained models
submodels = subnetwork_models["submodels"]
submodel_rna = submodels["rna"]
submodel_pro = submodels["pro"]
submodel_mut = submodels["mut"]
submodel_met = submodels["met"]
submodel_cnv = submodels["cnv"]
model_fn = subnetwork_models["final_model"]

metrics.increment() # update metrics

submodel_rna.to(device)
submodel_pro.to(device)
submodel_mut.to(device)
submodel_met.to(device)
submodel_cnv.to(device)
model_fn.to(device)

submodel_rna.eval()
submodel_pro.eval()
submodel_mut.eval()
submodel_met.eval()
submodel_cnv.eval()
model_fn.eval()

predictions = {"cell_line": [], "drug_id": [], "prediction": [], "target": []}

# evaluation step
for x in test_dataloader:
    with torch.no_grad():
        FV_rna = submodel_rna(x[0].to(device), x[5].to(device))   
        FV_pro = submodel_pro(x[1].to(device), x[5].to(device))
        FV_mut = submodel_mut(x[2].to(device), x[5].to(device))
        FV_met = submodel_met(x[3].to(device), x[5].to(device))
        FV_cnv = submodel_cnv(x[4].to(device), x[5].to(device))
        
        FV = torch.cat((FV_rna, FV_pro, FV_mut, FV_met, FV_cnv), dim=1)
        FV = F.normalize(FV, p=2, dim=1) # L2 normalization, 논문 참고
        out = model_fn(FV)
        
        metrics.update(out.squeeze(),
                    x[6].to(device).squeeze(),
                    cell_lines = x[7].to(device).squeeze().to(device),
                    drugs = x[8].to(device).squeeze().to(device))

        predictions["cell_line"].extend(x[7].squeeze().tolist())  
        predictions["drug_id"].extend(x[8].squeeze().cpu().tolist())    
        predictions["prediction"].extend(out.squeeze().tolist()) 
        predictions["target"].extend(x[6].squeeze().cpu() .tolist())
        
metrics_dict = {it[0]: it[1].item() for it in metrics.compute().items()}

# save result
df = pd.DataFrame(predictions)
filename = scripts.generate_filename("Encoder_subnetwork", "Multiomics", extension="csv")
df.to_csv("results/" + filename, index=False)
print(f"Predictions saved to: results/{filename}")

print(f"encoder sub-network model: {metrics_dict}")





from datetime import datetime

result = metrics_dict

model_name = "encoder_sub-network"
result["Model"] = model_name
time = datetime.now().strftime("%Y%m%d_%H:%M")
result["Time"] = time

result_df = pd.DataFrame([result])


ev_table = pd.read_csv("results/evalutation_table.csv")
ev_table = pd.concat([ev_table, result_df], ignore_index=True)
ev_table = ev_table.drop_duplicates(subset=["Model"])
display(ev_table)


ev_table.to_csv("results/evalutation_table.csv", index = False)



