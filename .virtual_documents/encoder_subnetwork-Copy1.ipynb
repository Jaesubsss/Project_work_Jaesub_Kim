!nvidia-smi


import pandas as pd
import numpy as np
import torch
import scripts
from functools import lru_cache
import torchmetrics
from torch import nn
import optuna
from torch.utils.data import Dataset
from torch import Tensor
import torch.nn.functional as F
from torchmetrics import Metric





config = {"features" : {"fp_radius":2}, # chemical의 fingerprint 생성 radius를 2로 설정
          "optimizer": {"batch_size": 220, # 한번에 학습시킬 데이터의 양
                        "clip_norm":19, # 그라디언트 클리핑에 사용할 최대 norm 값
                        "learning_rate": 0.0004592646200179472, # 학습률
                        "stopping_patience":15}, # 개선되지 않는 epoch가 15번 이상 나오면 학습을 중단
          "model":{"embed_dim":485, # input을 embedding할 때 사용할 차원
                 "hidden_dim":696, # hidden layer의 차원
                 "dropout":0.48541242824674574, # 40퍼센트의 노드를 랜덤하게 드랍아웃 
                 "n_layers": 4, # 3개의 hidden layer를 사용
                 "norm": "batchnorm"}, # batch normalization을 사용하여 모델이 학습 중 출력 분포를 정규화하여 학습을 안정화
         "env": {"fold": 0, # 0번째 fold를 사용하여 학습. 이는 음 n_fold에 들어갈 값을 의미하는 듯 하다. 
                "device":"cuda:1", # GPU자원을 사용할 장치를 지정한다. 
                 "max_epochs": 100, # 최대 epoch 수 
                 "search_hyperparameters":False}} # hyper parameter 이미 있으니 안쓴다.








# data loading seperately ---
# One of the traces of my various attempts
'''
rnaseq_train_dataset, rnaseq_validation_dataset, rnaseq_test_dataset = scripts.get_data(n_fold = config["env"]["fold"],
                                                           fp_radius = config["features"]["fp_radius"],typ = "rnaseq")

proteomics_train_dataset, proteomics_validation_dataset, proteomics_test_dataset = scripts.get_data(n_fold = config["env"]["fold"],
                                                           fp_radius = config["features"]["fp_radius"],typ = "proteomics")

mutations_train_dataset, mutations_validation_dataset, mutations_test_dataset = scripts.get_data(n_fold = config["env"]["fold"],
                                                           fp_radius = config["features"]["fp_radius"],typ = "mutations")

methylations_train_dataset, methylations_validation_dataset, methylations_test_dataset = scripts.get_data(n_fold = config["env"]["fold"],
                                                           fp_radius = config["features"]["fp_radius"],typ = "methylations")

cnvs_train_dataset, cnvs_validation_dataset, cnvs_test_dataset = scripts.get_data(n_fold = config["env"]["fold"],
                                                           fp_radius = config["features"]["fp_radius"],typ = "cnvs")

dim = {"rnaseq": list(rnaseq_train_dataset.omic_dict.values())[0].shape[0],
       "proteomics": list(proteomics_train_dataset.omic_dict.values())[0].shape[0],
       "mutations": list(mutations_train_dataset.omic_dict.values())[0].shape[0],
       "methylations": list(methylations_train_dataset.omic_dict.values())[0].shape[0],
       "cnvs": list(cnvs_train_dataset.omic_dict.values())[0].shape[0]}

# data integration, to make a shared data loader
train_data = rnaseq_train_dataset.data # they are shared, so we can use any of them
validation_data = rnaseq_validation_dataset.data
test_data = rnaseq_test_dataset.data
drug_dict = rnaseq_train_dataset.drug_dict

omic_dicts = {
    "rnaseq": rnaseq_train_dataset.omic_dict,
    "proteomics": proteomics_train_dataset.omic_dict,
    "mutations": mutations_train_dataset.omic_dict,
    "methylations": methylations_train_dataset.omic_dict,
    "cnvs": cnvs_train_dataset.omic_dict,
}
'''





# Define Model

class ResSubNet(nn.Module):
    def __init__(self, embed_dim=256, hidden_dim=1024, dropout=0.1, n_layers = 6, norm = "layernorm"):
        super().__init__()
        self.mlps = nn.ModuleList()
        if norm == "layernorm":
            norm = nn.LayerNorm
        elif norm == "batchnorm":
            norm = nn.BatchNorm1d
        else:
            norm = nn.Identity
        for l in range(n_layers):
            self.mlps.append(nn.Sequential(nn.Linear(embed_dim, hidden_dim),
                                           norm(hidden_dim),
                                     nn.ReLU(),
                                     nn.Dropout(dropout),
                                     nn.Linear(hidden_dim, embed_dim)))
        self.lin = nn.Linear(embed_dim, 1)
    def forward(self, x): 
        for l in self.mlps:
            x = (l(x) + x)/2
        return x # basically same, but feature vector

# sub-network model using ResNet

class SubNetworkModel(nn.Module):
    def __init__(self, embed_dim=256,
                 hidden_dim=1024,
                 dropout=0.1,
                 n_layers = 6,
                 norm = "layernorm"):
        super().__init__()
        self.ressubnet = ResSubNet(embed_dim, hidden_dim, dropout, n_layers, norm)
        self.embed_d = nn.Sequential(nn.LazyLinear(embed_dim), nn.ReLU())
        self.embed_c = nn.Sequential(nn.LazyLinear(embed_dim), nn.ReLU())
    def forward(self, c, d):
        return self.ressubnet(self.embed_d(d) + self.embed_c(c).squeeze(dim=1))
        # Lazylinear or Linear layer transform the inputdata from 2D to 3D tensor for c
        # therefore, value of self.embed_c(c) has different size with value of self.embed_d(d)
        # so using squeeze method, reduce dimension of self.embed_c(c)



class GroupwiseMetric(Metric): 
    # 모델 평가에서 특정 그룹 단위로 성능 지표를 계산하고, 필요 시 residual을 계산하는 기능. PyTorch의 metric 클래스를 상속한다.
    def __init__(self, metric,
                 grouping = "cell_lines",
                 average = "macro",
                 nan_ignore=False,
                 alpha=0.00001,
                 residualize = False,
                 **kwargs):
        super().__init__(**kwargs)
        self.grouping = grouping
        self.metric = metric
        self.average = average
        self.nan_ignore = nan_ignore
        self.residualize = residualize
        self.alpha = alpha
        # metric 클래스에서 사용하는 메서드로, 메트릭 계싼에 필요한 상태 변수를 정의하고 초기화. 
        # 새로운 state 변수 target, pred, drugs, cell_lines를 정의하고 torch.tensor([]) 형태로 초기화한다. 각 state 변수에 데이터를 축적할 수 있다. 
        self.add_state("target", default=torch.tensor([]))
        self.add_state("pred", default=torch.tensor([]))
        self.add_state("drugs", default=torch.tensor([]))
        self.add_state("cell_lines", default=torch.tensor([]))
        """
        Metric 클래스는 기본적으로 배치 단위로 계산된 값들을 계속해서 누적하여 최종 메트릭을 계산하는 기능을 제공한다. 예를들어 전체 데이터셋의 평균 손실 등을 개별 배치의 결과를 합산하거나 평균하여 얻어진다. 이러한 데이터 누적과 계산을 위해 Metric 클래스는 state 변수를 사용할 수 있다. state 변수는 Metric 클래스의 add_state 메서드를 통해 정의되며, 이를 통해 메트릭 계산에 필요한 상태 변수를 정의하고 초기화할 수 있다.
        
        정의된 state 변수들은 Metric 클래스의 메서드들(update, compute, reset)에 의해 자동으로 관리된다. update 메서드는 새로운 데이터를 받아 state 변수에 추가하고, compute 메서드는 state 변수를 이용하여 최종 메트릭을 계산한다. reset 메서드는 state 변수를 초기화한다.
        
        state를 사용함으로써 데이터의 누적과 관리를 자동으로 할 수 있으므로, 편리하다. 그러나 state 변수를 사용하면 메모리 사용량이 증가할 수 있으므로 주의해야 한다.
        """
        
    def get_residual(self, X, y):
        w = self.get_linear_weights(X, y)
        r = y-(X@w)
        return r
    def get_linear_weights(self, X, y):
        A = X.T@X
        Xy = X.T@y
        n_features = X.size(1)
        A.flatten()[:: n_features + 1] += self.alpha
        return torch.linalg.solve(A, Xy).T
    def get_residual_ind(self, y, drug_id, cell_id, alpha=0.001):
        X = torch.cat([y.new_ones(y.size(0), 1),
                       torch.nn.functional.one_hot(drug_id),
                       torch.nn.functional.one_hot(cell_id)], 1).float()
        return self.get_residual(X, y)

    def compute(self) -> Tensor:
        if self.grouping == "cell_lines":
            grouping = self.cell_lines
        elif self.grouping == "drugs":
            grouping = self.drugs
        metric = self.metric
        if not self.residualize:
            y_obs = self.target
            y_pred = self.pred
        else:
            y_obs = self.get_residual_ind(self.target, self.drugs, self.cell_lines)
            y_pred = self.get_residual_ind(self.pred, self.drugs, self.cell_lines)
        average = self.average
        nan_ignore = self.nan_ignore
        unique = grouping.unique()
        proportions = []
        metrics = []
        for g in unique:
            is_group = grouping == g
            metrics += [metric(y_obs[grouping == g], y_pred[grouping == g])]
            proportions += [is_group.sum()/len(is_group)]
        if average is None:
            return torch.stack(metrics)
        if (average == "macro") & (nan_ignore):
            return torch.nanmean(y_pred.new_tensor([metrics]))
        if (average == "macro") & (not nan_ignore):
            return torch.mean(y_pred.new_tensor([metrics]))
        if (average == "micro") & (not nan_ignore):
            return (y_pred.new_tensor([proportions])*y_pred.new_tensor([metrics])).sum()
        else:
            raise NotImplementedError
    
    def update(self, preds: Tensor, target: Tensor,  drugs: Tensor,  cell_lines: Tensor) -> None:
        print(f"self.target shape: {self.target.shape if hasattr(self, 'target') else 'Not initialized'}")
        print(f"target shape: {target.shape}")
        print(f"self.pred shape: {self.pred.shape if hasattr(self, 'pred') else 'Not initialized'}")
        print(f"preds shape: {preds.shape}")
        print(f"self.drugs shape: {self.drugs.shape if hasattr(self, 'drugs') else 'Not initialized'}")
        print(f"drugs shape: {drugs.shape}")
        print(f"self.cell_lines shape: {self.cell_lines.shape if hasattr(self, 'cell_lines') else 'Not initialized'}")
        print(f"cell_lines shape: {cell_lines.shape}")
        
        self.target = torch.cat([self.target, target])
        self.pred = torch.cat([self.pred, preds])
        self.drugs = torch.cat([self.drugs, drugs]).long()
        self.cell_lines = torch.cat([self.cell_lines, cell_lines]).long()





class MultiOmicsDataset(Dataset):
    def __init__(self, rna_omic_dicts, pro_omic_dicts, mut_omic_dicts, met_omic_dicts, cnv_omic_dicts, drug_dict, data):
        self.rna_omic_dicts = rna_omic_dicts  # 데이터 타입별 omic_dict
        self.pro_omic_dicts = pro_omic_dicts
        self.mut_omic_dicts = mut_omic_dicts
        self.met_omic_dicts = met_omic_dicts
        self.cnv_omic_dicts = cnv_omic_dicts
        self.drug_dict = drug_dict    # common drug_dict
        self.cell_mapped_ids = {key:i for i, key in enumerate(self.rna_omic_dicts.keys())} # those mapped ids are also common
        self.drug_mapped_ids = {key:i for i, key in enumerate(self.drug_dict.keys())}
        self.data = data              # common train_data, validation_data, test_data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # 현재 샘플 데이터 가져오기
        instance = self.data.iloc[idx]
        cell_id = instance["SANGER_MODEL_ID"]
        drug_id = instance["DRUG_ID"]
        target = instance["LN_IC50"]

        # 약물 데이터
        #drug_features = torch.tensor(self.drug_dict[drug_id], dtype=torch.float32)
        drug_features = self.drug_dict[drug_id].clone().detach().float()
        #print(f"Type of drug_dict[drug_id]: {type(self.drug_dict[drug_id])}") # dont use this! it will crash the server

        # 타겟 값
        target = torch.tensor([target], dtype=torch.float32)

        return (self.rna_omic_dicts[cell_id].unsqueeze(0), # 
                self.pro_omic_dicts[cell_id].unsqueeze(0),
                self.mut_omic_dicts[cell_id].unsqueeze(0),
                self.met_omic_dicts[cell_id].unsqueeze(0),
                self.cnv_omic_dicts[cell_id].unsqueeze(0),
                self.drug_dict[drug_id],
                Tensor([target]),
                Tensor([self.cell_mapped_ids[cell_id]]),
                Tensor([self.drug_mapped_ids[drug_id]])) 





# Modified get data function
def get_data_subnetwork(n_fold = 0, fp_radius = 2):

    # drug
    smile_dict = pd.read_csv("data/smiles.csv", index_col=0)
    fp = scripts.FingerprintFeaturizer(R = fp_radius)
    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])

    # loading all datasets
    driver_genes = pd.read_csv("data/driver_genes.csv").loc[:, "symbol"].dropna()

    rnaseq = pd.read_csv("data/rnaseq_normcount.csv", index_col=0)
    driver_columns = rnaseq.columns.isin(driver_genes)
    filtered_rna = rnaseq.loc[:, driver_columns]

    proteomics = pd.read_csv("data/proteomics.csv", index_col=0)

    mutation = pd.read_csv("data/binary_mutations.csv")
    mutation.columns = mutation.iloc[0]
    mutation = mutation.iloc[2:,:].set_index("gene_symbol")
    driver_columns = mutation.columns.isin(driver_genes)
    filtered_mut = mutation.loc[:, driver_columns]
    filtered_mut = filtered_mut.astype(float)

    methylations = pd.read_csv("data/methylations.csv",index_col = 0).sort_index(ascending = True)

    cnvs = pd.read_csv("data/copy_number_variations.csv",index_col= 0)

    # concatenate all dataset 
    # inner join based on index: model_ids with NaN are automatically filtered out 
    data_concat = pd.concat([filtered_rna, proteomics, filtered_mut, methylations, cnvs], axis=1, join='inner')

    # choosing dataset to be used for training model
    data_rna = filtered_rna[filtered_rna.index.isin(data_concat.index)]
    tensor_rna = torch.Tensor(data_rna.to_numpy())
    rna_cell_dict = {cell: tensor_rna[i] for i, cell in enumerate(data_rna.index.to_numpy())}
    
    data_pro = proteomics[proteomics.index.isin(data_concat.index)]
    tensor_pro = torch.Tensor(data_pro.to_numpy())
    pro_cell_dict = {cell: tensor_pro[i] for i, cell in enumerate(data_pro.index.to_numpy())}
    
    data_mut = filtered_mut[filtered_mut.index.isin(data_concat.index)]
    tensor_mut = torch.Tensor(data_mut.to_numpy())
    mut_cell_dict = {cell: tensor_mut[i] for i, cell in enumerate(data_mut.index.to_numpy())}
    
    data_met = methylations[methylations.index.isin(data_concat.index)]
    tensor_met = torch.Tensor(data_met.to_numpy())
    met_cell_dict = {cell: tensor_met[i] for i, cell in enumerate(data_met.index.to_numpy())}
    
    data_cnv = cnvs[cnvs.index.isin(data_concat.index)]
    tensor_cnv = torch.Tensor(data_cnv.to_numpy())
    cnv_cell_dict = {cell: tensor_cnv[i] for i, cell in enumerate(data_cnv.index.to_numpy())}

    dim = {"rnaseq": data_rna.shape[1], # 얘가 필요한가? 아닌거같은데
        "proteomics": data_pro.shape[1],
        "mutations": data_mut.shape[1],
        "methylations": data_met.shape[1],
        "cnvs": data_cnv.shape[1]}

    # GDSC
    data = pd.read_csv("data/GDSC1.csv", index_col=0)
    # default, remove data where lines or drugs are missing:
    data = data.query("SANGER_MODEL_ID in @rna_cell_dict.keys() & DRUG_ID in @drug_dict.keys()")
    unique_cell_lines = data.loc[:, "SANGER_MODEL_ID"].unique()

    np.random.seed(420) # for comparibility, don't change it!
    np.random.shuffle(unique_cell_lines)
    folds = np.array_split(unique_cell_lines, 10)
    test_lines = folds[0] # ?? 질문 필요. 0번째 fold를 test로 쓴다면, 이후에도 train_idx에서 0을 제거해야하는것 아닌가? 만약 train_idx에서 n_fold를 제거할 것이라면, trest_lines를 folds[n_fold]로 바꿔야하지 않는가? 
    train_idxs = list(range(10))
    train_idxs.remove(n_fold)
    np.random.seed(420)
    validation_idx = np.random.choice(train_idxs)
    train_idxs.remove(validation_idx)
    train_lines = np.concatenate([folds[idx] for idx in train_idxs])
    validation_lines = folds[validation_idx]
    test_lines = folds[n_fold] # 아 여기서 오버라이드 됐네? 그럼 위에 test_lines는 왜 있는거지?
    # 5
    train_data = data.query("SANGER_MODEL_ID in @train_lines")
    validation_data = data.query("SANGER_MODEL_ID in @validation_lines")
    test_data = data.query("SANGER_MODEL_ID in @test_lines")

    return(MultiOmicsDataset(rna_cell_dict, pro_cell_dict, mut_cell_dict, met_cell_dict, cnv_cell_dict, drug_dict, train_data),
    MultiOmicsDataset(rna_cell_dict, pro_cell_dict, mut_cell_dict, met_cell_dict, cnv_cell_dict, drug_dict, validation_data),
    MultiOmicsDataset(rna_cell_dict, pro_cell_dict, mut_cell_dict, met_cell_dict, cnv_cell_dict, drug_dict, test_data))





# modified & intergrated model train function

def train_model_multiomics(config, train_dataset, validation_dataset=None, use_momentum=True, callback_epoch = None):
    class FinalRegressor(nn.Module):
        def __init__(self, embed_dim=256, hidden_dim=1024, dropout=0.1, n_layers=6, norm="layernorm"):
            super(FinalRegressor, self).__init__()
            self.FN = nn.Sequential(
                    nn.Linear(embed_dim * 5, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, 1)  # Regression output
                )
        def forward(self, x):
            return self.FN(x)
        
    train_loader = torch.utils.data.DataLoader(train_dataset,
                                           batch_size=config["optimizer"]["batch_size"],
                                           drop_last=True,
                                          shuffle=True)
    loader = train_loader
    if validation_dataset is not None:
        val_loader = torch.utils.data.DataLoader(validation_dataset,
                                               batch_size=config["optimizer"]["batch_size"],
                                               drop_last=False,
                                              shuffle=False)
        loader = val_loader
        
    submodel_rna = SubNetworkModel(**config["model"])
    submodel_pro = SubNetworkModel(**config["model"])
    submodel_mut = SubNetworkModel(**config["model"])
    submodel_met = SubNetworkModel(**config["model"])
    submodel_cnv = SubNetworkModel(**config["model"])
    model_fn = FinalRegressor(**config["model"])
    
    optimizer_rna = torch.optim.Adam(submodel_rna.parameters(), config["optimizer"]["learning_rate"])
    optimizer_pro = torch.optim.Adam(submodel_pro.parameters(), config["optimizer"]["learning_rate"])
    optimizer_mut = torch.optim.Adam(submodel_mut.parameters(), config["optimizer"]["learning_rate"])
    optimizer_met = torch.optim.Adam(submodel_met.parameters(), config["optimizer"]["learning_rate"])
    optimizer_cnv = torch.optim.Adam(submodel_cnv.parameters(), config["optimizer"]["learning_rate"])
    optimizer_fn = torch.optim.Adam(model_fn.parameters(), config["optimizer"]["learning_rate"])

    device = torch.device(config["env"]["device"])

    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_fn, factor=0.5, patience=5)
    early_stop = scripts.EarlyStop(config["optimizer"]["stopping_patience"])

    submodel_rna.to(device)
    submodel_pro.to(device)
    submodel_mut.to(device)
    submodel_met.to(device)
    submodel_cnv.to(device)
    model_fn.to(device)

    metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(
    {"R_cellwise_residuals":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,
                          grouping="drugs",
                          average="macro",
                          residualize=True),
    "R_cellwise":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,
                          grouping="cell_lines",
                          average="macro",
                          residualize=False),
    "MSE":torchmetrics.MeanSquaredError()}))
    metrics.to(device)
    use_momentum = True    
    for epoch in range(config["env"]["max_epochs"]):
        ### Train step ---------------------------------------------------------
        #train_loss = train_step(model, optimizer, train_loader, config, device)
        if epoch > 3:
            return val_target, {"submodels": {                      #빠르게 해보자
                "rna": submodel_rna,
                "pro": submodel_pro,
                "mut": submodel_mut,
                "met": submodel_met,
                "cnv": submodel_cnv,
                },
            "final_model": model_fn             # 학습된 최종 Regressor
            }
        loss = nn.MSELoss()
        ls = []

        submodel_rna.train()
        submodel_pro.train()
        submodel_mut.train()
        submodel_met.train()
        submodel_cnv.train()
        model_fn.train()
        
        for x in loader:
            FV_rna = submodel_rna(x[0].to(device), x[5].to(device))   
            FV_pro = submodel_pro(x[1].to(device), x[5].to(device))
            FV_mut = submodel_mut(x[2].to(device), x[5].to(device))
            FV_met = submodel_met(x[3].to(device), x[5].to(device))
            FV_cnv = submodel_cnv(x[4].to(device), x[5].to(device))

            FV = torch.cat((FV_rna, FV_pro, FV_mut, FV_met, FV_cnv), dim=1)
            FV = F.normalize(FV, p=2, dim=1) # L2 normalization, 논문 참고
            out = model_fn(FV)

            optimizer_rna.zero_grad()
            optimizer_pro.zero_grad()
            optimizer_mut.zero_grad()
            optimizer_met.zero_grad()
            optimizer_cnv.zero_grad()
            optimizer_fn.zero_grad()

            l = loss(out.squeeze(), x[6].to(device).squeeze())
            l.backward()

            torch.nn.utils.clip_grad_norm_(submodel_rna.parameters(), config["optimizer"]["clip_norm"])
            torch.nn.utils.clip_grad_norm_(submodel_pro.parameters(), config["optimizer"]["clip_norm"])
            torch.nn.utils.clip_grad_norm_(submodel_mut.parameters(), config["optimizer"]["clip_norm"])
            torch.nn.utils.clip_grad_norm_(submodel_met.parameters(), config["optimizer"]["clip_norm"])
            torch.nn.utils.clip_grad_norm_(submodel_cnv.parameters(), config["optimizer"]["clip_norm"])
            torch.nn.utils.clip_grad_norm_(model_fn.parameters(), config["optimizer"]["clip_norm"])

            ls += [l.item()]
            
            optimizer_rna.step()
            optimizer_pro.step()
            optimizer_mut.step()
            optimizer_met.step()
            optimizer_cnv.step()
            optimizer_fn.step()

        train_loss = np.mean(ls)

        lr_scheduler.step(train_loss)
        
        if validation_dataset is not None:
            # validation_metrics = scripts.evaluate_step(model,val_loader, metrics, device)
            ### Evaluation step ---------------------------------------------------------
            metrics.increment()
            submodel_rna.eval()
            submodel_pro.eval()
            submodel_mut.eval()
            submodel_met.eval()
            submodel_cnv.eval()
            model_fn.eval()
            
            submodel_rna.to(device)
            submodel_pro.to(device)
            submodel_mut.to(device)
            submodel_met.to(device)
            submodel_cnv.to(device)
            model_fn.to(device)
            
            for x in loader:
                with torch.no_grad():                    
                    FV_rna = submodel_rna(x[0].to(device), x[5].to(device))   
                    FV_pro = submodel_pro(x[1].to(device), x[5].to(device))
                    FV_mut = submodel_mut(x[2].to(device), x[5].to(device))
                    FV_met = submodel_met(x[3].to(device), x[5].to(device))
                    FV_cnv = submodel_cnv(x[4].to(device), x[5].to(device))
                    
                    FV = torch.cat((FV_rna, FV_pro, FV_mut, FV_met, FV_cnv), dim=1)
                    FV = F.normalize(FV, p=2, dim=1) # L2 normalization, 논문 참고
                    
                    out = model_fn(FV)
                    
                    metrics.update(out.squeeze(),
                                x[6].to(device).squeeze(),
                                cell_lines = x[7].to(device).squeeze().to(device),
                                drugs = x[8].to(device).squeeze().to(device))
            validation_metrics = {it[0]:it[1].item() for it in metrics.compute().items()}
            
            
            
            if epoch > 0 & use_momentum:
                val_target = 0.2*val_target + 0.8*validation_metrics['R_cellwise_residuals']
            else:
                val_target = validation_metrics['R_cellwise_residuals']
        else:
            val_target = None
        if callback_epoch is None:
            print(f"epoch : {epoch}: train loss: {train_loss} Smoothed R interaction (validation) {val_target}")
        else:
            callback_epoch(epoch, val_target)
        if early_stop(train_loss):
            break
    return val_target, {"submodels": {                      # 학습된 서브네트워크
            "rna": submodel_rna,
            "pro": submodel_pro,
            "mut": submodel_mut,
            "met": submodel_met,
            "cnv": submodel_cnv,
        },
        "final_model": model_fn             # 학습된 최종 Regressor
    }





# get data
train_dataset, validation_dataset, test_dataset = get_data_subnetwork(n_fold = config["env"]["fold"], fp_radius = config["features"]["fp_radius"])





# model training
_, subnetwork_models = train_model_multiomics(config, torch.utils.data.ConcatDataset([train_dataset, validation_dataset]), None, use_momentum=False)
device = torch.device(config["env"]["device"])
metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(
    {"R_cellwise_residuals":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,
                          grouping="drugs",
                          average="macro",
                          residualize=True),
    "R_cellwise":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,
                          grouping="cell_lines",
                          average="macro",
                          residualize=False),
    "MSE":torchmetrics.MeanSquaredError()}))

metrics.to(device)

test_dataloader = torch.utils.data.DataLoader(test_dataset,
                                       batch_size=config["optimizer"]["batch_size"],
                                       drop_last=False,
                                      shuffle=False,pin_memory=True)





### Test Evaluation ---

# get trained models
submodels = subnetwork_models["submodels"]
submodel_rna = submodels["rna"]
submodel_pro = submodels["pro"]
submodel_mut = submodels["mut"]
submodel_met = submodels["met"]
submodel_cnv = submodels["cnv"]
model_fn = subnetwork_models["final_model"]

metrics.increment() # update metrics

submodel_rna.to(device)
submodel_pro.to(device)
submodel_mut.to(device)
submodel_met.to(device)
submodel_cnv.to(device)
model_fn.to(device)

submodel_rna.eval()
submodel_pro.eval()
submodel_mut.eval()
submodel_met.eval()
submodel_cnv.eval()
model_fn.eval()

predictions = {"cell_line": [], "drug_id": [], "prediction": [], "target": []}

# evaluation step
for x in test_dataloader:
    with torch.no_grad():
        # 데이터 점검
        #print(f"x[0]: {x[0].shape}, x[5]: {x[5].shape}")
        #print(f"x[6]: {x[6].shape}, x[7]: {x[7].shape}, x[8]: {x[8].shape}")
        FV_rna = submodel_rna(x[0].to(device), x[5].to(device))   
        FV_pro = submodel_pro(x[1].to(device), x[5].to(device))
        FV_mut = submodel_mut(x[2].to(device), x[5].to(device))
        FV_met = submodel_met(x[3].to(device), x[5].to(device))
        FV_cnv = submodel_cnv(x[4].to(device), x[5].to(device))
        #print(f"FV_rna: {FV_rna.shape}, FV_pro: {FV_pro.shape}, FV_mut: {FV_mut.shape}, FV_met: {FV_met.shape}, FV_cnv: {FV_cnv.shape}")
                    
        FV = torch.cat((FV_rna, FV_pro, FV_mut, FV_met, FV_cnv), dim=1)
        FV = F.normalize(FV, p=2, dim=1) # L2 normalization, 논문 참고
        
        #print(f"FV shape: {FV.shape}")
        out = model_fn(FV)
        #print(f"model_fn output shape: {out.shape}")
        
        metrics.update(out.squeeze(),
                    x[6].to(device).squeeze(),
                    cell_lines = x[7].to(device).squeeze().to(device),
                    drugs = x[8].to(device).squeeze().to(device))

        predictions["cell_line"].extend(x[7].squeeze().tolist())  
        predictions["drug_id"].extend(x[8].squeeze().cpu().tolist())    
        predictions["prediction"].extend(out.squeeze().tolist()) 
        predictions["target"].extend(x[6].squeeze().cpu() .tolist())
        
metrics_dict = {it[0]: it[1].item() for it in metrics.compute().items()}

# save result
df = pd.DataFrame(predictions)
filename = scripts.generate_filename("Encoder_subnetwork", "Multiomics", extension="csv")
df.to_csv("results/" + filename, index=False)
print(f"Predictions saved to: results/{filename}")

print(f"encoder sub-network model: {metrics_dict}")


from datetime import datetime

result = metrics_dict

model_name = "encoder_sub-network"
result["Model"] = model_name
time = datetime.now().strftime("%Y%m%d_%H:%M")
result["Time"] = time

result_df = pd.DataFrame([result])


new_column_order = ["Model", "MSE", "R_cellwise", "R_cellwise_residuals","Time"]
result_df = result_df[new_column_order]
result_df.head()


result_df.to_csv("results/evalutation_table.csv", index = False)



