{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a3912e-5f0b-4454-af45-415bb4c1249c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar  2 20:20:09 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              51W / 400W |      4MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          On  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              51W / 400W |      4MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-40GB          On  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              54W / 400W |      4MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-40GB          On  | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   26C    P0              51W / 400W |      4MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9067e-d08d-4136-8aed-730fcc049b05",
   "metadata": {},
   "source": [
    "# Package Loading & Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed4ebb8d-24c1-4ae0-86f8-28b17754179f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_105446/1495640561.py\", line 2, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/__init__.py\", line 26, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/compat/__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/compat/pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_105446/1495640561.py\", line 2, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/core/api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/core/dtypes/dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_105446/1495640561.py\", line 8, in <module>\n",
      "    import torchmetrics\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torchmetrics/__init__.py\", line 37, in <module>\n",
      "    from torchmetrics import functional  # noqa: E402\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torchmetrics/functional/__init__.py\", line 122, in <module>\n",
      "    from torchmetrics.functional.text._deprecated import _bleu_score as bleu_score\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torchmetrics/functional/text/__init__.py\", line 17, in <module>\n",
      "    from torchmetrics.functional.text.chrf import chrf_score\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torchmetrics/functional/text/chrf.py\", line 32, in <module>\n",
      "    _EPS_SMOOTHING = tensor(1e-16)\n",
      "/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torchmetrics/functional/text/chrf.py:32: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/torch/csrc/utils/tensor_numpy.cpp:77.)\n",
      "  _EPS_SMOOTHING = tensor(1e-16)\n"
     ]
    }
   ],
   "source": [
    "import MOICVAE.SNF as snf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "from torchmetrics import MeanSquaredError\n",
    "from sklearn.impute import KNNImputer\n",
    "import scripts\n",
    "from functools import lru_cache\n",
    "import optuna\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import warnings\n",
    "from torch.utils.data import Dataset\n",
    "# 경고 무시 설정\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de47a1d7-b286-494c-8332-9a9cf70630d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dataset for sim_matrix\n",
    "class OmicsDataset_dict(Dataset): \n",
    "    def __init__(self, omic_dict, drug_dict, data): \n",
    "        self.omic_dict = omic_dict\n",
    "        self.drug_dict = drug_dict\n",
    "        self.cell_mapped_ids = {key:i for i, key in enumerate(self.omic_dict.keys())}\n",
    "        # omic_dict의 키를 고유한 인덱스로 매핑\n",
    "        # enumerate는 키들을 순서대로 열거하여 (인덱스, 키) 형태의 튜플로 반환\n",
    "        # 딕셔너리 컴프레헨션: 각 키를 key로, 각 키의 인덱스를 i로 사용하여 {key:i}형태로 매핑된 딕셔너리 만듬.\n",
    "        self.drug_mapped_ids = {key:i for i, key in enumerate(self.drug_dict.keys())}\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx): # idx = train_data\n",
    "        instance = self.data.iloc[idx] \n",
    "        cell_id = instance.iloc[0]\n",
    "        drug_id = instance.iloc[1]\n",
    "        target = instance.iloc[2]\n",
    "        \n",
    "        #omics_data = { # usage of dictionary here causes a problem or crash with collate_fn function in Dataloader \n",
    "        #    cell_id : {\n",
    "        #        data_type: self.omic_dict[cell_id][data_type] for data_type in self.omic_dict[cell_id].keys()\n",
    "        #    }\n",
    "        #}\n",
    "        \n",
    "        return (torch.cat([self.omic_dict[cell_id][modality] for modality in self.omic_dict[cell_id].keys()]), \n",
    "                self.drug_dict[drug_id],\n",
    "                torch.Tensor([target]),\n",
    "                torch.Tensor([self.cell_mapped_ids[cell_id]]),\n",
    "                torch.Tensor([self.drug_mapped_ids[drug_id]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a70cb18-226e-46ee-bb31-67e48878158d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# MDA model\n",
    "class MultimodalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dims,  hidden_dim_encoders = 150, embed_dim = 75, fusion_dim = 150, dropout_encoders = 0.2):\n",
    "        # get input as a dictionary\n",
    "        super(MultimodalAutoencoder, self).__init__()\n",
    "        # EEEEEEEEEEncoder\n",
    "        self.input_dims = input_dims\n",
    "        print(f\"Expected input_dims: {len(input_dims)}\")\n",
    "        self.num_modalities = len(input_dims)\n",
    "        self.do = nn.Dropout(dropout_encoders)\n",
    "\n",
    "        self.omics_encoder = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(mod_input_dim, hidden_dim_encoders), # input \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim_encoders, embed_dim) # encoder hidden layer: 150, 75 as the value from the paper. so we start from this \n",
    "            )                                 # I dont get why they used 150, 75 for dimension, but we can tune it later\n",
    "            for mod_input_dim in self.input_dims\n",
    "        ])\n",
    "        # fused latent feature \n",
    "        self.fusion_layer = nn.Sequential( # I think we need a fusion layer here, to combine the data modalities\n",
    "            nn.Linear(embed_dim * self.num_modalities, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fusion_dim, embed_dim) # This concatenate latent features of all omics data, and fusion them and make its dim final latent dim\n",
    "        )                                     # This is the only way I can think of to fuse omics data\n",
    "        # decoder\n",
    "        self.omics_decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim_encoders),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_encoders, sum(input_dims))\n",
    "        )\n",
    "        # I actually dont understand this step in paper. they said that decoder has symmetric structure as encoder,\n",
    "        # but the data after MDA they provided, has weird dimension(363x90) which makes no sense. this is the point that i cant understand\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_features = [] # get dictionary as an input \n",
    "        start_idx = 0\n",
    "        for i, encoder in enumerate(self.omics_encoder):\n",
    "            mod_input_dim = self.input_dims[i]\n",
    "            x_modality = x[:, start_idx:start_idx + mod_input_dim]\n",
    "            latent_features.append(encoder(self.do(x_modality)))\n",
    "            start_idx += mod_input_dim\n",
    "            \n",
    "        latent_fused = torch.cat(latent_features, dim=1)\n",
    "        latent_final = self.fusion_layer(latent_fused)\n",
    "        decoded = self.omics_decoder(latent_final)\n",
    "        return decoded, latent_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d787ac75-9a90-4c05-85be-a2e6dda20ad6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Main resnet model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, embed_dim=256, hidden_dim=1024, dropout=0.1, n_layers=6, norm=\"layernorm\"):\n",
    "        super().__init__()\n",
    "        self.mlps = nn.ModuleList()\n",
    "        \n",
    "        # Determine normalization layer\n",
    "        if norm == \"layernorm\":\n",
    "            norm_layer = nn.LayerNorm\n",
    "        elif norm == \"batchnorm\":\n",
    "            norm_layer = nn.BatchNorm1d\n",
    "        else:\n",
    "            norm_layer = nn.Identity\n",
    "        \n",
    "        # Create MLP layers\n",
    "        for _ in range(n_layers):\n",
    "            self.mlps.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(embed_dim, hidden_dim),\n",
    "                    norm_layer(hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(hidden_dim, embed_dim)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.lin = nn.Linear(embed_dim, 1)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.mlps:\n",
    "            \n",
    "            x = (layer(x) + x) / 2  # Residual connection\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "class Main_model(nn.Module):\n",
    "    def __init__(self, embed_dim=256, hidden_dim=1024, dropout=0.1, n_layers=6, norm=\"layernorm\", \n",
    "                 dropout_omics=0.4, dropout_omics_finetuning=0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ResNet as backbone\n",
    "        self.resnet = ResNet(embed_dim, hidden_dim, dropout, n_layers, norm)\n",
    "        \n",
    "        # Modified embed_d: Two-layer MLP with dropout after ReLU\n",
    "        self.embed_d = nn.Sequential(\n",
    "            nn.LazyLinear(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Modified embed_c: Two-layer MLP with dropout after ReLU and before first Linear Layer\n",
    "        self.embed_c = nn.Sequential(\n",
    "            nn.Dropout(dropout_omics_finetuning),\n",
    "            nn.LazyLinear(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_omics),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, c, d):\n",
    "        # Combine embedded inputs and pass through ResNet\n",
    "        return self.resnet(self.embed_d(d) + self.embed_c(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9908faaa-375e-4e92-86a7-66cec1e1ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Data function\n",
    "@lru_cache(maxsize=None)\n",
    "def get_data_corr(n_fold = 0, fp_radius = 2, transform_into_corr = True, typ = [\"rnaseq\", \"mutations\", \"cnvs\"],\n",
    "                  #reconstructed = None\n",
    "                 ):\n",
    "    # drug\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    \n",
    "    # loading all datasets\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    \n",
    "    proteomics = pd.read_csv(\"data/proteomics.csv\", index_col=0)\n",
    "    \n",
    "    mutation = pd.read_csv(\"data/binary_mutations.csv\")\n",
    "    mutation.columns = mutation.iloc[0]\n",
    "    mutation = mutation.iloc[2:,:].set_index(\"gene_symbol\")\n",
    "    driver_columns = mutation.columns.isin(driver_genes)\n",
    "    filtered_mut = mutation.loc[:, driver_columns]\n",
    "    filtered_mut = filtered_mut.astype(float)\n",
    "\n",
    "    methylations = pd.read_csv(\"data/methylations.csv\",index_col = 0).sort_index(ascending = True)\n",
    "\n",
    "    cnvs = pd.read_csv(\"data/copy_number_variations.csv\",index_col= 0)\n",
    "\n",
    "    # concatenate all dataset \n",
    "    # inner join based on index: model_ids with NaN are automatically filtered out \n",
    "    data_concat = pd.concat([filtered_rna, proteomics, filtered_mut, methylations, cnvs], axis=1, join='inner')\n",
    "    \n",
    "    \n",
    "    # Filter data by common indices in all modalities\n",
    "    filtered_rna = filtered_rna[filtered_rna.index.isin(data_concat.index)]\n",
    "    proteomics = proteomics[proteomics.index.isin(data_concat.index)]\n",
    "    filtered_mut = filtered_mut[filtered_mut.index.isin(data_concat.index)]\n",
    "    methylations = methylations[methylations.index.isin(data_concat.index)]\n",
    "    cnvs = cnvs[cnvs.index.isin(data_concat.index)]\n",
    "    \n",
    "    # Initialize cell_dict\n",
    "    cell_dict = {}\n",
    "\n",
    "    if not transform_into_corr : #and reconstructed is None:\n",
    "\n",
    "        dims = []\n",
    "        if \"rnaseq\" in typ:\n",
    "            dims.append(filtered_rna.shape[1])\n",
    "        if \"proteomics\" in typ:\n",
    "            dims.append(proteomics.shape[1])\n",
    "        if \"mutations\" in typ:\n",
    "            dims.append(filtered_mut.shape[1])\n",
    "        if \"methylations\" in typ:\n",
    "            dims.append(methylations.shape[1])\n",
    "        if \"cnvs\" in typ:\n",
    "            dims.append(cnvs.shape[1])\n",
    "        \n",
    "        for cell in data_concat.index:\n",
    "            # Initialize a sub-dictionary for each cell\n",
    "            concatenated_data = []\n",
    "            \n",
    "            # Add data for each type specified in typ\n",
    "            if \"rnaseq\" in typ:\n",
    "                concatenated_data.append(filtered_rna.loc[cell].to_numpy())\n",
    "            if \"proteomics\" in typ:\n",
    "                concatenated_data.append(proteomics.loc[cell].to_numpy())\n",
    "            if \"mutations\" in typ:\n",
    "                concatenated_data.append(filtered_mut.loc[cell].to_numpy())\n",
    "            if \"methylations\" in typ:\n",
    "                concatenated_data.append(methylations.loc[cell].to_numpy())\n",
    "            if \"cnvs\" in typ:\n",
    "                concatenated_data.append(cnvs.loc[cell].to_numpy())\n",
    "\n",
    "            cell_dict[cell] = torch.Tensor(np.concatenate(concatenated_data))\n",
    "            \n",
    "#    if reconstructed is not None:\n",
    "#        for cell_idx, cell in enumerate(data_concat.index):\n",
    "#            # cell_dict에 reconstructed 텐서의 각 행(cell 데이터) 저장\n",
    "#            cell_dict[cell] = reconstructed[cell_idx]\n",
    "        \n",
    "\n",
    "    # GDSC\n",
    "    GDSC1 = pd.read_csv(\"data/GDSC1.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = GDSC1.query(\"SANGER_MODEL_ID in @data_concat.index & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "\n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    test_lines = folds[0] \n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold] \n",
    "\n",
    "        # no change needed, query works fine with some missing\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "    \n",
    "    if transform_into_corr: #and reconstructed is None:\n",
    "        # train, val, test among filtered data\n",
    "        # these are valid train_, val_ and test_data index\n",
    "        \n",
    "        \n",
    "        n_train = len(train_lines)  \n",
    "        n_val = len(validation_lines)      \n",
    "        n_test = len(test_lines)\n",
    "        \n",
    "        # Precompute similarity matrices for each data type\n",
    "        similarity_matrices = {}\n",
    "        dims = []\n",
    "        \n",
    "        if \"rnaseq\" in typ:\n",
    "            exp_com = np.corrcoef(np.vstack([filtered_rna.loc[train_lines], \n",
    "                                             filtered_rna.loc[validation_lines], \n",
    "                                             filtered_rna.loc[test_lines]]), rowvar=True)\n",
    "            train = exp_com[:n_train, :n_train]\n",
    "            val = exp_com[n_train:n_train+n_val, :n_train]\n",
    "            test = exp_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"rnaseq\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"rnaseq\"][0]))\n",
    "        \n",
    "        if \"proteomics\" in typ:\n",
    "            prot_com = np.corrcoef(np.vstack([proteomics.loc[train_lines], \n",
    "                                              proteomics.loc[validation_lines], \n",
    "                                              proteomics.loc[test_lines]]), rowvar=True)\n",
    "            train = prot_com[:n_train, :n_train]\n",
    "            val = prot_com[n_train:n_train+n_val, :n_train]\n",
    "            test = prot_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"proteomics\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"proteomics\"][0]))\n",
    "        \n",
    "        if \"mutations\" in typ:\n",
    "            train_snp = filtered_mut.loc[train_lines].astype(bool)\n",
    "            val_snp = filtered_mut.loc[validation_lines].astype(bool)\n",
    "            test_snp = filtered_mut.loc[test_lines].astype(bool)\n",
    "            \n",
    "            train = 1 - pairwise_distances(train_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            val = 1 - pairwise_distances(val_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            test = 1 - pairwise_distances(test_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "    \n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"mutations\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"mutations\"][0]))\n",
    "        \n",
    "        if \"methylations\" in typ:\n",
    "            methyl_com = np.nan_to_num(np.corrcoef(np.vstack([methylations.loc[train_lines], \n",
    "                                                methylations.loc[validation_lines], \n",
    "                                                methylations.loc[test_lines]]), rowvar=True))\n",
    "            train = methyl_com[:n_train, :n_train]\n",
    "            val = methyl_com[n_train:n_train+n_val, :n_train]\n",
    "            test = methyl_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"methylations\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"methylations\"][0]))\n",
    "        \n",
    "        if \"cnvs\" in typ:\n",
    "            cnv_com = np.nan_to_num(np.corrcoef(np.vstack([cnvs.loc[train_lines], # nan-generation problem fixed \n",
    "                                             cnvs.loc[validation_lines], \n",
    "                                             cnvs.loc[test_lines]]), rowvar=True))\n",
    "            train= cnv_com[:n_train, :n_train]\n",
    "            val= cnv_com[n_train:n_train+n_val, :n_train]\n",
    "            test= cnv_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"cnvs\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"cnvs\"][0]))\n",
    "            \n",
    "        cell_dict = {}\n",
    "\n",
    "        # \n",
    "        for cell in unique_cell_lines:\n",
    "            cell_dict[cell] = {}\n",
    "            for data_type in typ:\n",
    "                sim_matrices = similarity_matrices[data_type]\n",
    "                sim_tensor = torch.Tensor(sim_matrices)\n",
    "                cell_idx = np.where(unique_cell_lines == cell)[0][0]\n",
    "                cell_dict[cell][data_type] = sim_tensor[cell_idx]\n",
    "    \n",
    "        return (OmicsDataset_dict(cell_dict, drug_dict, train_data),\n",
    "        OmicsDataset_dict(cell_dict, drug_dict, validation_data),\n",
    "        OmicsDataset_dict(cell_dict, drug_dict, test_data),\n",
    "        dims)\n",
    "\n",
    "    return (scripts.OmicsDataset(cell_dict, drug_dict, train_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, validation_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, test_data),\n",
    "    dims)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99a313f5-7380-4163-aca1-cea2ce314e45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# model training function\n",
    "def train_step(model, optimizer, loader, config, device):\n",
    "    loss = nn.MSELoss()\n",
    "    ls = []\n",
    "    model.train()\n",
    "    for x in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x[0].to(device), x[1].to(device))\n",
    "        l = loss(out.squeeze(), x[2].to(device).squeeze())\n",
    "        l.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"optimizer\"][\"clip_norm\"])\n",
    "        ls += [l.item()]\n",
    "        optimizer.step()\n",
    "    return np.mean(ls)\n",
    "\n",
    "# finetune old version: uses pre-trained weights, which is non-sense.\n",
    "\"\"\"\n",
    "def finetune_model_train(config, train_dataset, pre_trained_weight_path, validation_dataset=None, use_momentum=True, callback_epoch = None):\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                           drop_last=True,\n",
    "                                          shuffle=True)\n",
    "    if validation_dataset is not None:\n",
    "        val_loader = torch.utils.data.DataLoader(validation_dataset,\n",
    "                                               batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                               drop_last=False,\n",
    "                                              shuffle=False)\n",
    "        \n",
    "    model = Main_model(embed_dim=config[\"model\"][\"embed_dim\"],\n",
    "                       hidden_dim=config[\"model\"][\"hidden_dim\"], \n",
    "                       dropout=config[\"model\"][\"dropout\"], \n",
    "                       n_layers=config[\"model\"][\"n_layers\"],  \n",
    "                       dropout_omics=config[\"model\"][\"dropout_omics\"], \n",
    "                       dropout_omics_finetuning=config[\"model\"][\"dropout_omics_finetuning\"],\n",
    "                       norm=config[\"model\"][\"norm\"])\n",
    "    model.embed_c.load_state_dict(torch.load(pre_trained_weight_path), strict=False) # pre-trained weights\n",
    "    optimizer = torch.optim.Adam(model.parameters(), config[\"optimizer\"][\"learning_rate\"])\n",
    "    device = torch.device(config[\"env\"][\"device\"])\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)\n",
    "    early_stop = scripts.EarlyStop(config[\"optimizer\"][\"stopping_patience\"])\n",
    "    model.to(device)\n",
    "    metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(\n",
    "    {\"R_cellwise_residuals\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"drugs\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=True),\n",
    "    \"R_cellwise\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"cell_lines\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=False),\n",
    "    \"MSE\":torchmetrics.MeanSquaredError()}))\n",
    "    metrics.to(device)\n",
    "    for epoch in range(config[\"env\"][\"max_epochs\"]):\n",
    "        train_loss = train_step(model, optimizer, train_loader, config, device)\n",
    "        if epoch == 0:  # first epoch\n",
    "            # if main model is called with input data, then LazyLinear is initialized with input\n",
    "            # our pretrained weights has already [hidden_dim, embed_dim] dimension, so it cannot be directly loaded  into model\n",
    "            # but embed_c will eventually be [hidden_dim, embed_dim] dimension after last linear layer, so we override the weights after first epoch\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=config[\"optimizer\"][\"learning_rate\"]) # reinitialize to prevent any problem\n",
    "            \n",
    "        lr_scheduler.step(train_loss)\n",
    "        \n",
    "        if validation_dataset is not None:\n",
    "            validation_metrics = scripts.evaluate_step(model,val_loader, metrics, device)\n",
    "            if epoch > 0 & use_momentum:\n",
    "                val_target = 0.2*val_target + 0.8*validation_metrics['R_cellwise_residuals']\n",
    "            else:\n",
    "                val_target = validation_metrics['R_cellwise_residuals']\n",
    "        else:\n",
    "            val_target = None\n",
    "        if callback_epoch is None:\n",
    "            print(f\"epoch : {epoch}: train loss: {train_loss} Smoothed R interaction (validation) {val_target}\")\n",
    "        else:\n",
    "            callback_epoch(epoch, val_target)\n",
    "        if early_stop(train_loss):\n",
    "            break\n",
    "    return val_target, model\n",
    "\"\"\"\n",
    "\n",
    "def finetune_model_train(config, train_dataset, pre_trained_model, validation_dataset=None, use_momentum=True, callback_epoch=None):\n",
    "    device = torch.device(config[\"env\"][\"device\"])\n",
    "    pre_raw_model.eval()\n",
    "    latent_dict = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cell_id, omics_tensor in fine_raw_train_dataset.omic_dict.items():\n",
    "            omics_tensor = omics_tensor.to(device)  # GPU로 이동\n",
    "            \n",
    "            #  Autoencoder의 forward() 호출하여 바로 latent_final 획득\n",
    "            _, latent_final = pre_raw_model(omics_tensor.unsqueeze(0))  # decoded는 필요 없으므로 _ 처리\n",
    "    \n",
    "            latent_dict[cell_id] = latent_final.squeeze(0).cpu()  # Latent Representation 저장\n",
    "    \n",
    "    train_dataset.omic_dict = latent_dict\n",
    "    print(f\"the omics data of {len(latent_dict)} samples are replaced to pre-trained latent features.\")\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "        drop_last=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    if validation_dataset is not None:\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            validation_dataset,\n",
    "            batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "            drop_last=False,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    ### Fine-tuning 모델 생성 ###\n",
    "    model = Main_model(\n",
    "        embed_dim=config[\"model\"][\"embed_dim\"],\n",
    "        hidden_dim=config[\"model\"][\"hidden_dim\"], \n",
    "        dropout=config[\"model\"][\"dropout\"], \n",
    "        n_layers=config[\"model\"][\"n_layers\"],  \n",
    "        dropout_omics=config[\"model\"][\"dropout_omics\"], \n",
    "        dropout_omics_finetuning=config[\"model\"][\"dropout_omics_finetuning\"],\n",
    "        norm=config[\"model\"][\"norm\"]\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), config[\"optimizer\"][\"learning_rate\"])\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)\n",
    "    early_stop = scripts.EarlyStop(config[\"optimizer\"][\"stopping_patience\"])\n",
    "    model.to(device)\n",
    "\n",
    "    metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection({\n",
    "        \"R_cellwise_residuals\": scripts.GroupwiseMetric(\n",
    "            metric=torchmetrics.functional.pearson_corrcoef,\n",
    "            grouping=\"drugs\",\n",
    "            average=\"macro\",\n",
    "            residualize=True\n",
    "        ),\n",
    "        \"R_cellwise\": scripts.GroupwiseMetric(\n",
    "            metric=torchmetrics.functional.pearson_corrcoef,\n",
    "            grouping=\"cell_lines\",\n",
    "            average=\"macro\",\n",
    "            residualize=False\n",
    "        ),\n",
    "        \"MSE\": torchmetrics.MeanSquaredError()\n",
    "    }))\n",
    "    metrics.to(device)\n",
    "\n",
    "    ### Fine-tuning Loop ###\n",
    "    for epoch in range(config[\"env\"][\"max_epochs\"]):\n",
    "        train_loss = train_step(model, optimizer, train_loader, config, device)\n",
    "\n",
    "        if epoch == 0:  \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=config[\"optimizer\"][\"learning_rate\"])\n",
    "        \n",
    "        lr_scheduler.step(train_loss)\n",
    "        \n",
    "        if validation_dataset is not None:\n",
    "            validation_metrics = scripts.evaluate_step(model, val_loader, metrics, device)\n",
    "            if epoch > 0 and use_momentum:\n",
    "                val_target = 0.2 * val_target + 0.8 * validation_metrics['R_cellwise_residuals']\n",
    "            else:\n",
    "                val_target = validation_metrics['R_cellwise_residuals']\n",
    "        else:\n",
    "            val_target = None\n",
    "\n",
    "        if callback_epoch is None:\n",
    "            print(f\"epoch : {epoch}: train loss: {train_loss} Smoothed R interaction (validation) {val_target}\")\n",
    "        else:\n",
    "            callback_epoch(epoch, val_target)\n",
    "\n",
    "        if early_stop(train_loss):\n",
    "            break\n",
    "\n",
    "    return val_target, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edbc17a1-a593-4f45-bfee-0508a200f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-training 과정 이후, weights를 반환하는 것이 아닌 latnet feature를 반환하는 함수 \n",
    "\n",
    "def pre_train_step(config, pre_dataset, input_dims):\n",
    "    \"\"\"\n",
    "    Pre-training function for Multimodal Autoencoder.\n",
    "    This function can handle both raw and similarity-based datasets.\n",
    "    \n",
    "    Parameters:\n",
    "        - config: Training configuration dictionary\n",
    "        - pre_dataset: Dataset to train the autoencoder (can be raw or similarity-based)\n",
    "        - input_dims: Input dimensions for the autoencoder\n",
    "\n",
    "    Returns:\n",
    "        - pre_trained_model: Trained autoencoder model\n",
    "        - total_losses: List of training losses per epoch\n",
    "    \"\"\"\n",
    "    # 1️⃣ 데이터 로더 설정\n",
    "    pre_train_loader = torch.utils.data.DataLoader(\n",
    "        pre_dataset,\n",
    "        batch_size=config[\"optimizer\"][\"pre_batch_size\"],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    # 2️⃣ 모델 생성\n",
    "    pre_model = MultimodalAutoencoder(\n",
    "        input_dims=input_dims,\n",
    "        hidden_dim_encoders=config[\"model\"][\"hidden_dim_encoders\"],\n",
    "        embed_dim=config[\"model\"][\"embed_dim\"],\n",
    "        fusion_dim=config[\"model\"][\"fusion_dim\"],\n",
    "        dropout_encoders=config[\"model\"][\"dropout_encoders\"]\n",
    "    )\n",
    "\n",
    "    # 3️⃣ 옵티마이저, 손실 함수, 스케줄러 설정\n",
    "    optimizer = torch.optim.Adam(pre_model.parameters(), lr=config[\"optimizer\"][\"lr_pretraining\"])\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "    device = torch.device(config[\"env\"][\"device\"])\n",
    "\n",
    "    pre_model.to(device)\n",
    "    pre_model.train()\n",
    "\n",
    "    pre_training_epochs = config[\"model\"][\"pre_training_epochs\"]\n",
    "    total_losses = []\n",
    "\n",
    "    # 4️⃣ 모델 학습\n",
    "    for epoch in range(pre_training_epochs):\n",
    "        total_loss = []\n",
    "        \n",
    "        for batch in pre_train_loader:\n",
    "            inputs = batch[0].to(device)  # 데이터는 `batch[0]`에 저장됨\n",
    "\n",
    "            optimizer.zero_grad()  # 기울기 초기화\n",
    "            reconstructed, latent_features = pre_model(inputs)  # Latent features 추출\n",
    "\n",
    "            # 손실 계산 (입력 데이터와 복원된 데이터 비교)\n",
    "            loss = criterion(reconstructed, inputs)\n",
    "            \n",
    "            loss.backward()  # 기울기 계산\n",
    "            optimizer.step()  # 옵티마이저 스텝 진행\n",
    "\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        # 평균 손실값을 계산하고 스케줄러 적용\n",
    "        train_loss = np.mean(total_loss)\n",
    "        total_losses.append(train_loss)\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "        # 학습 상태 출력\n",
    "        print(f'Epoch {epoch + 1}/{pre_training_epochs}, Loss: {train_loss:.5f}')\n",
    "\n",
    "    print(\"Pre-training complete!\")\n",
    "\n",
    "    return pre_model, total_losses\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8596e9e-cdf5-43e1-87c5-4053a6f3364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "config = {\"features\" : {\"fp_radius\":2,\n",
    "                        \"use_correlation_representation\": True,\n",
    "                        \"num_modalities\": 4},\n",
    "          \"optimizer\": {\"batch_size\": 220,\n",
    "                        \"clip_norm\":19,\n",
    "                        \"learning_rate\": 0.0004592646200179472,\n",
    "                        \"stopping_patience\":15,\n",
    "                        \"pre_batch_size\": 200,\n",
    "                        \"lr_pretraining\": 0.0004592646200179472},\n",
    "          \"model\":{\"embed_dim\":485, # shared\n",
    "                 \"hidden_dim\":696, \n",
    "                 \"dropout\":0.48541242824674574, \n",
    "                 \"n_layers\": 4, \n",
    "                 \"norm\": \"batchnorm\", \n",
    "                 \"hidden_dim_encoders\": 256, # ENCODER\n",
    "                 \"fusion_dim\": 700, # ENCODER\n",
    "                 \"dropout_encoders\": 0.2,\n",
    "                 \"dropout_omics\": 0.4, # second\n",
    "                 \"dropout_omics_finetuning\": 0.4, # first\n",
    "                 \"pre_training_epochs\": 100}, \n",
    "         \"env\": {\"fold\": 0,  \n",
    "                \"device\":\"cuda:0\", \n",
    "                 \"max_epochs\": 100, \n",
    "                 \"search_hyperparameters\":False}} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c33f7-e4f1-4b95-a40a-c56e7f3ed775",
   "metadata": {},
   "source": [
    "# Using Similarity Matrix \n",
    "\n",
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "626095e5-8277-4a15-814f-a3557eec4c4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:12:40] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "# data loading with corr\n",
    "pre_sim_train_dataset, pre_sim_val_dataset, pre_sim_test_dataset, pre_sim_dims = get_data_corr(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"], \n",
    "                                                           transform_into_corr = config[\"features\"][\"use_correlation_representation\"],\n",
    "                                                           typ = (\"rnaseq\", \"proteomics\", \"mutations\", \"methylations\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62796ca2-b096-4083-92a5-c66178b13b1a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected input_dims: 4\n",
      "Epoch 1/100, Loss: 0.00718\n",
      "Epoch 2/100, Loss: 0.00167\n",
      "Epoch 3/100, Loss: 0.00127\n",
      "Epoch 4/100, Loss: 0.00104\n",
      "Epoch 5/100, Loss: 0.00091\n",
      "Epoch 6/100, Loss: 0.00082\n",
      "Epoch 7/100, Loss: 0.00076\n",
      "Epoch 8/100, Loss: 0.00070\n",
      "Epoch 9/100, Loss: 0.00065\n",
      "Epoch 10/100, Loss: 0.00060\n",
      "Epoch 11/100, Loss: 0.00057\n",
      "Epoch 12/100, Loss: 0.00053\n",
      "Epoch 13/100, Loss: 0.00050\n",
      "Epoch 14/100, Loss: 0.00048\n",
      "Epoch 15/100, Loss: 0.00045\n",
      "Epoch 16/100, Loss: 0.00043\n",
      "Epoch 17/100, Loss: 0.00041\n",
      "Epoch 18/100, Loss: 0.00040\n",
      "Epoch 19/100, Loss: 0.00038\n",
      "Epoch 20/100, Loss: 0.00037\n",
      "Epoch 21/100, Loss: 0.00036\n",
      "Epoch 22/100, Loss: 0.00036\n",
      "Epoch 23/100, Loss: 0.00035\n",
      "Epoch 24/100, Loss: 0.00034\n",
      "Epoch 25/100, Loss: 0.00034\n",
      "Epoch 26/100, Loss: 0.00033\n",
      "Epoch 27/100, Loss: 0.00033\n",
      "Epoch 28/100, Loss: 0.00032\n",
      "Epoch 29/100, Loss: 0.00032\n",
      "Epoch 30/100, Loss: 0.00032\n",
      "Epoch 31/100, Loss: 0.00031\n",
      "Epoch 32/100, Loss: 0.00031\n",
      "Epoch 33/100, Loss: 0.00031\n",
      "Epoch 34/100, Loss: 0.00031\n",
      "Epoch 35/100, Loss: 0.00031\n",
      "Epoch 36/100, Loss: 0.00030\n",
      "Epoch 37/100, Loss: 0.00030\n",
      "Epoch 38/100, Loss: 0.00030\n",
      "Epoch 39/100, Loss: 0.00030\n",
      "Epoch 40/100, Loss: 0.00030\n",
      "Epoch 41/100, Loss: 0.00030\n",
      "Epoch 42/100, Loss: 0.00030\n",
      "Epoch 43/100, Loss: 0.00030\n",
      "Epoch 44/100, Loss: 0.00030\n",
      "Epoch 45/100, Loss: 0.00030\n",
      "Epoch 46/100, Loss: 0.00030\n",
      "Epoch 47/100, Loss: 0.00029\n",
      "Epoch 48/100, Loss: 0.00029\n",
      "Epoch 49/100, Loss: 0.00029\n",
      "Epoch 50/100, Loss: 0.00029\n",
      "Epoch 51/100, Loss: 0.00029\n",
      "Epoch 52/100, Loss: 0.00029\n",
      "Epoch 53/100, Loss: 0.00029\n",
      "Epoch 54/100, Loss: 0.00029\n",
      "Epoch 55/100, Loss: 0.00029\n",
      "Epoch 56/100, Loss: 0.00029\n",
      "Epoch 57/100, Loss: 0.00028\n",
      "Epoch 58/100, Loss: 0.00028\n",
      "Epoch 59/100, Loss: 0.00028\n",
      "Epoch 60/100, Loss: 0.00028\n",
      "Epoch 61/100, Loss: 0.00028\n",
      "Epoch 62/100, Loss: 0.00028\n",
      "Epoch 63/100, Loss: 0.00028\n",
      "Epoch 64/100, Loss: 0.00028\n",
      "Epoch 65/100, Loss: 0.00028\n",
      "Epoch 66/100, Loss: 0.00028\n",
      "Epoch 67/100, Loss: 0.00027\n",
      "Epoch 68/100, Loss: 0.00027\n",
      "Epoch 69/100, Loss: 0.00027\n",
      "Epoch 70/100, Loss: 0.00027\n",
      "Epoch 71/100, Loss: 0.00027\n",
      "Epoch 72/100, Loss: 0.00027\n",
      "Epoch 73/100, Loss: 0.00027\n",
      "Epoch 74/100, Loss: 0.00027\n",
      "Epoch 75/100, Loss: 0.00027\n",
      "Epoch 76/100, Loss: 0.00027\n",
      "Epoch 77/100, Loss: 0.00027\n",
      "Epoch 78/100, Loss: 0.00026\n",
      "Epoch 79/100, Loss: 0.00026\n",
      "Epoch 80/100, Loss: 0.00026\n",
      "Epoch 81/100, Loss: 0.00026\n",
      "Epoch 82/100, Loss: 0.00026\n",
      "Epoch 83/100, Loss: 0.00026\n",
      "Epoch 84/100, Loss: 0.00026\n",
      "Epoch 85/100, Loss: 0.00026\n",
      "Epoch 86/100, Loss: 0.00026\n",
      "Epoch 87/100, Loss: 0.00026\n",
      "Epoch 88/100, Loss: 0.00025\n",
      "Epoch 89/100, Loss: 0.00025\n",
      "Epoch 90/100, Loss: 0.00025\n",
      "Epoch 91/100, Loss: 0.00025\n",
      "Epoch 92/100, Loss: 0.00025\n",
      "Epoch 93/100, Loss: 0.00025\n",
      "Epoch 94/100, Loss: 0.00025\n",
      "Epoch 95/100, Loss: 0.00025\n",
      "Epoch 96/100, Loss: 0.00025\n",
      "Epoch 97/100, Loss: 0.00025\n",
      "Epoch 98/100, Loss: 0.00024\n",
      "Epoch 99/100, Loss: 0.00024\n",
      "Epoch 100/100, Loss: 0.00024\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# pre training with sim_matrix\n",
    "pre_sim_train_loader = torch.utils.data.DataLoader(\n",
    "        pre_sim_train_dataset,\n",
    "        batch_size = config[\"optimizer\"][\"pre_batch_size\"],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "#if validation_dataset is not None:\n",
    "#    val_loader = torch.utils.data.DataLoader(val_dataset,batch_size = batch_size,shuffle=True,drop_last=True)\n",
    "    \n",
    "sample_batch = next(iter(pre_sim_train_loader))\n",
    "sample_omics_data = sample_batch[0]  # 첫 번째 요소는 omics_data (dict)\n",
    "mod_len = sample_omics_data.shape[1] // config[\"features\"][\"num_modalities\"]\n",
    "#input_dict = {f\"modality_{i+1}\": mod_len for i in range(num_modality)}\n",
    "\n",
    "pre_sim_model = MultimodalAutoencoder(input_dims = pre_sim_dims, \n",
    "                              hidden_dim_encoders = config[\"model\"][\"hidden_dim_encoders\"],\n",
    "                              embed_dim = config[\"model\"][\"embed_dim\"],\n",
    "                              fusion_dim = config[\"model\"][\"fusion_dim\"],\n",
    "                              #num_modalities = config[\"features\"][\"num_modalities\"], \n",
    "                              dropout_encoders = config[\"model\"][\"dropout_encoders\"])\n",
    "\n",
    "optimizer = torch.optim.Adam(pre_sim_model.parameters(), lr=config[\"optimizer\"][\"lr_pretraining\"])\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pre_sim_model.to(device)\n",
    "\n",
    "# model training\n",
    "for epoch in range(config[\"model\"][\"pre_training_epochs\"]):\n",
    "    total_loss = []\n",
    "    pre_sim_model.train()\n",
    "\n",
    "\n",
    "#    if validation_dataset is not None:\n",
    "#        validation_metrics = evaluate_step(model,val_loader, metrics, device)\n",
    "#        if epoch > 0 & use_momentum:\n",
    "#            val_target = 0.2*val_target + 0.8*validation_metrics['R_cellwise_residuals']\n",
    "#        else:\n",
    "#            val_target = validation_metrics['R_cellwise_residuals']\n",
    "\n",
    "    \n",
    "    # train step\n",
    "    for batch in pre_sim_train_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "\n",
    "        optimizer.zero_grad() # gradient initialization\n",
    "        reconstructed, latent = pre_sim_model(inputs)\n",
    "\n",
    "        #target_data = torch.cat([inputs[modality] for modality in inputs], dim=1)\n",
    "        # I think, here inappropriate loss is used. \n",
    "        # From what I know, Autoencoder reconstructs input feature, and calculate loss through comparing input feature and reconstructed feature.\n",
    "        # So, calculating MSE comparing with target_data, which is drug sensitivity, is nonsense. \n",
    "        #loss = criterion(reconstructed, target_data)\n",
    "        \n",
    "        # here is a new loss function, but it still outputs nan value as a loss...\n",
    "        loss = criterion(reconstructed, inputs)\n",
    "        \n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config[\"optimizer\"][\"clip_norm\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "    train_loss = np.mean(total_loss)\n",
    "    scheduler.step(train_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{config[\"model\"][\"pre_training_epochs\"]}, Loss: {train_loss:.5f}')\n",
    "\n",
    "torch.save(pre_sim_model.fusion_layer.state_dict(), \"trained_models/pretrained_omics.pth\") # save pre-trained weights from fusion layer\n",
    "#torch.save(reconstructed,\"reconstructedMDA_data.pth\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c659d9-cfa1-4a27-86ca-ff712213153a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected input_dims: 4\n",
      "Epoch 1/100, Loss: 0.00649\n",
      "Epoch 2/100, Loss: 0.00156\n",
      "Epoch 3/100, Loss: 0.00117\n",
      "Epoch 4/100, Loss: 0.00099\n",
      "Epoch 5/100, Loss: 0.00088\n",
      "Epoch 6/100, Loss: 0.00080\n",
      "Epoch 7/100, Loss: 0.00074\n",
      "Epoch 8/100, Loss: 0.00068\n",
      "Epoch 9/100, Loss: 0.00064\n",
      "Epoch 10/100, Loss: 0.00059\n",
      "Epoch 11/100, Loss: 0.00056\n",
      "Epoch 12/100, Loss: 0.00053\n",
      "Epoch 13/100, Loss: 0.00050\n",
      "Epoch 14/100, Loss: 0.00047\n",
      "Epoch 15/100, Loss: 0.00045\n",
      "Epoch 16/100, Loss: 0.00043\n",
      "Epoch 17/100, Loss: 0.00042\n",
      "Epoch 18/100, Loss: 0.00040\n",
      "Epoch 19/100, Loss: 0.00039\n",
      "Epoch 20/100, Loss: 0.00038\n",
      "Epoch 21/100, Loss: 0.00037\n",
      "Epoch 22/100, Loss: 0.00036\n",
      "Epoch 23/100, Loss: 0.00035\n",
      "Epoch 24/100, Loss: 0.00035\n",
      "Epoch 25/100, Loss: 0.00034\n",
      "Epoch 26/100, Loss: 0.00034\n",
      "Epoch 27/100, Loss: 0.00033\n",
      "Epoch 28/100, Loss: 0.00033\n",
      "Epoch 29/100, Loss: 0.00033\n",
      "Epoch 30/100, Loss: 0.00032\n",
      "Epoch 31/100, Loss: 0.00032\n",
      "Epoch 32/100, Loss: 0.00032\n",
      "Epoch 33/100, Loss: 0.00032\n",
      "Epoch 34/100, Loss: 0.00032\n",
      "Epoch 35/100, Loss: 0.00032\n",
      "Epoch 36/100, Loss: 0.00031\n",
      "Epoch 37/100, Loss: 0.00031\n",
      "Epoch 38/100, Loss: 0.00031\n",
      "Epoch 39/100, Loss: 0.00031\n",
      "Epoch 40/100, Loss: 0.00031\n",
      "Epoch 41/100, Loss: 0.00031\n",
      "Epoch 42/100, Loss: 0.00031\n",
      "Epoch 43/100, Loss: 0.00030\n",
      "Epoch 44/100, Loss: 0.00030\n",
      "Epoch 45/100, Loss: 0.00030\n",
      "Epoch 46/100, Loss: 0.00030\n",
      "Epoch 47/100, Loss: 0.00030\n",
      "Epoch 48/100, Loss: 0.00030\n",
      "Epoch 49/100, Loss: 0.00030\n",
      "Epoch 50/100, Loss: 0.00030\n",
      "Epoch 51/100, Loss: 0.00029\n",
      "Epoch 52/100, Loss: 0.00029\n",
      "Epoch 53/100, Loss: 0.00029\n",
      "Epoch 54/100, Loss: 0.00029\n",
      "Epoch 55/100, Loss: 0.00029\n",
      "Epoch 56/100, Loss: 0.00029\n",
      "Epoch 57/100, Loss: 0.00029\n",
      "Epoch 58/100, Loss: 0.00029\n"
     ]
    }
   ],
   "source": [
    "pretrained_sim_model, pretraining_sim_losses = pre_train_step(config, torch.utils.data.ConcatDataset([pre_sim_train_dataset, pre_sim_val_dataset]), input_dims = pre_sim_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e976d-fba3-4450-a6fb-70288a921d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation MDA\n",
    "\n",
    "pre_sim_eval_loader = torch.utils.data.DataLoader(\n",
    "        pre_sim_test_dataset,\n",
    "        batch_size = config[\"optimizer\"][\"pre_batch_size\"],\n",
    "        shuffle=False, # we dont need this in evaluation step => repeatable result, for comparision\n",
    "        drop_last=False # to use all data\n",
    "    )\n",
    "\n",
    "pre_sim_model.to(device)\n",
    "pre_sim_model.eval()\n",
    "\n",
    "eval_losses = []\n",
    "\n",
    "for x in pre_sim_eval_loader: \n",
    "    with torch.no_grad(): # no autograd => in evaluation step we dont need to update weights\n",
    "        inputs = batch[0].to(device)\n",
    "        reconstructed, _ = pre_sim_model(inputs)\n",
    "        evel_loss = criterion(reconstructed,inputs)\n",
    "        eval_losses.append(evel_loss.item())\n",
    "\n",
    "final_eval_loss = np.mean(eval_losses)\n",
    "print(f\"Final MSE of Model: {final_eval_loss:.5f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a95f1-d6ee-4791-9a99-ef186f8852e0",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fa84fe-34b7-4802-a511-45a23b4c89de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#recon_sim = torch.load(\"reconstructedMDA_data.pth\")\n",
    "fine_sim_train_dataset, fine_val_sim_dataset, fine_test_sim_dataset, dim = get_data_corr(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"], \n",
    "                                                           transform_into_corr = config[\"features\"][\"use_correlation_representation\"],\n",
    "                                                           typ = (\"rnaseq\", \"proteomics\", \"mutations\", \"methylations\"),\n",
    "                                                           #reconstructed = recon_sim\n",
    "                                                                                   )\n",
    "# use sim_matrix as an input data as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18a07a55-ed81-4f02-843f-8d8a6c037c6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: FutureWarning: The default value for `maximize` will be changed from `True` to `None` in v1.7.0 of TorchMetrics,will automatically infer the value based on the `higher_is_better` attribute of the metric (if such attribute exists) or raise an error if it does not. If you are explicitly setting the `maximize` argument to either `True` or `False` already, you can ignore this warning.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained weights applied to the final Linear layer!\n",
      "epoch : 0: train loss: 2.3652852571791567 Smoothed R interaction (validation) None\n",
      "epoch : 1: train loss: 2.0498048929113097 Smoothed R interaction (validation) None\n",
      "epoch : 2: train loss: 1.9744003243440456 Smoothed R interaction (validation) None\n",
      "epoch : 3: train loss: 1.923729983336846 Smoothed R interaction (validation) None\n",
      "epoch : 4: train loss: 1.8867335819048698 Smoothed R interaction (validation) None\n",
      "epoch : 5: train loss: 1.8523730247365529 Smoothed R interaction (validation) None\n",
      "epoch : 6: train loss: 1.8281392441102395 Smoothed R interaction (validation) None\n",
      "epoch : 7: train loss: 1.8046435273916672 Smoothed R interaction (validation) None\n",
      "epoch : 8: train loss: 1.7864722365649286 Smoothed R interaction (validation) None\n",
      "epoch : 9: train loss: 1.7648730460733948 Smoothed R interaction (validation) None\n",
      "epoch : 10: train loss: 1.7477140310226176 Smoothed R interaction (validation) None\n",
      "epoch : 11: train loss: 1.734586451492734 Smoothed R interaction (validation) None\n",
      "epoch : 12: train loss: 1.7252156339558151 Smoothed R interaction (validation) None\n",
      "epoch : 13: train loss: 1.7123984129378764 Smoothed R interaction (validation) None\n",
      "epoch : 14: train loss: 1.7017543660695533 Smoothed R interaction (validation) None\n",
      "epoch : 15: train loss: 1.6974155675937572 Smoothed R interaction (validation) None\n",
      "epoch : 16: train loss: 1.6880687115661 Smoothed R interaction (validation) None\n",
      "epoch : 17: train loss: 1.677633520404547 Smoothed R interaction (validation) None\n",
      "epoch : 18: train loss: 1.6806607367229107 Smoothed R interaction (validation) None\n",
      "epoch : 19: train loss: 1.6685722280933772 Smoothed R interaction (validation) None\n",
      "epoch : 20: train loss: 1.6666296664510285 Smoothed R interaction (validation) None\n",
      "epoch : 21: train loss: 1.660982274007149 Smoothed R interaction (validation) None\n",
      "epoch : 22: train loss: 1.6605740006096725 Smoothed R interaction (validation) None\n",
      "epoch : 23: train loss: 1.655056782499674 Smoothed R interaction (validation) None\n",
      "epoch : 24: train loss: 1.650952633585417 Smoothed R interaction (validation) None\n",
      "epoch : 25: train loss: 1.6470033515369051 Smoothed R interaction (validation) None\n",
      "epoch : 26: train loss: 1.648431762555621 Smoothed R interaction (validation) None\n",
      "epoch : 27: train loss: 1.6369242545848135 Smoothed R interaction (validation) None\n",
      "epoch : 28: train loss: 1.633359744905394 Smoothed R interaction (validation) None\n",
      "epoch : 29: train loss: 1.6364625130949269 Smoothed R interaction (validation) None\n",
      "epoch : 30: train loss: 1.6307981145396664 Smoothed R interaction (validation) None\n",
      "epoch : 31: train loss: 1.630090071627472 Smoothed R interaction (validation) None\n",
      "epoch : 32: train loss: 1.6256307720106524 Smoothed R interaction (validation) None\n",
      "epoch : 33: train loss: 1.6237733899736582 Smoothed R interaction (validation) None\n",
      "epoch : 34: train loss: 1.6253929719494653 Smoothed R interaction (validation) None\n",
      "epoch : 35: train loss: 1.620162169924478 Smoothed R interaction (validation) None\n",
      "epoch : 36: train loss: 1.6183996744297344 Smoothed R interaction (validation) None\n",
      "epoch : 37: train loss: 1.611631056434292 Smoothed R interaction (validation) None\n",
      "epoch : 38: train loss: 1.6083923232717479 Smoothed R interaction (validation) None\n",
      "epoch : 39: train loss: 1.6114636313340573 Smoothed R interaction (validation) None\n",
      "epoch : 40: train loss: 1.6114261994697845 Smoothed R interaction (validation) None\n",
      "epoch : 41: train loss: 1.6047889225562513 Smoothed R interaction (validation) None\n",
      "epoch : 42: train loss: 1.6030649147900131 Smoothed R interaction (validation) None\n",
      "epoch : 43: train loss: 1.6037655367397408 Smoothed R interaction (validation) None\n",
      "epoch : 44: train loss: 1.6025611145976741 Smoothed R interaction (validation) None\n",
      "epoch : 45: train loss: 1.5994956281924866 Smoothed R interaction (validation) None\n",
      "epoch : 46: train loss: 1.6031844866437877 Smoothed R interaction (validation) None\n",
      "epoch : 47: train loss: 1.5959685634181584 Smoothed R interaction (validation) None\n",
      "epoch : 48: train loss: 1.599826144788999 Smoothed R interaction (validation) None\n",
      "epoch : 49: train loss: 1.5934055203707758 Smoothed R interaction (validation) None\n",
      "epoch : 50: train loss: 1.5957046913717527 Smoothed R interaction (validation) None\n",
      "epoch : 51: train loss: 1.5900478728474734 Smoothed R interaction (validation) None\n",
      "epoch : 52: train loss: 1.5940436911671358 Smoothed R interaction (validation) None\n",
      "epoch : 53: train loss: 1.5894257832221843 Smoothed R interaction (validation) None\n",
      "epoch : 54: train loss: 1.5883339015750861 Smoothed R interaction (validation) None\n",
      "epoch : 55: train loss: 1.587448218402226 Smoothed R interaction (validation) None\n",
      "epoch : 56: train loss: 1.5859040749824533 Smoothed R interaction (validation) None\n",
      "epoch : 57: train loss: 1.57959319840845 Smoothed R interaction (validation) None\n",
      "epoch : 58: train loss: 1.5810122124491575 Smoothed R interaction (validation) None\n",
      "epoch : 59: train loss: 1.5861980337442367 Smoothed R interaction (validation) None\n",
      "epoch : 60: train loss: 1.576909714340427 Smoothed R interaction (validation) None\n",
      "epoch : 61: train loss: 1.5765793926194218 Smoothed R interaction (validation) None\n",
      "epoch : 62: train loss: 1.5743487909786191 Smoothed R interaction (validation) None\n",
      "epoch : 63: train loss: 1.573325668188786 Smoothed R interaction (validation) None\n",
      "epoch : 64: train loss: 1.5752632696047997 Smoothed R interaction (validation) None\n",
      "epoch : 65: train loss: 1.5716991616120592 Smoothed R interaction (validation) None\n",
      "epoch : 66: train loss: 1.5696950112049306 Smoothed R interaction (validation) None\n",
      "epoch : 67: train loss: 1.5676545992180502 Smoothed R interaction (validation) None\n",
      "epoch : 68: train loss: 1.5677053045725498 Smoothed R interaction (validation) None\n",
      "epoch : 69: train loss: 1.5647490412107357 Smoothed R interaction (validation) None\n",
      "epoch : 70: train loss: 1.561872315126827 Smoothed R interaction (validation) None\n",
      "epoch : 71: train loss: 1.5631063520834678 Smoothed R interaction (validation) None\n",
      "epoch : 72: train loss: 1.5651357608613625 Smoothed R interaction (validation) None\n",
      "epoch : 73: train loss: 1.560535520204655 Smoothed R interaction (validation) None\n",
      "epoch : 74: train loss: 1.5626495952806603 Smoothed R interaction (validation) None\n",
      "epoch : 75: train loss: 1.5598043142938791 Smoothed R interaction (validation) None\n",
      "epoch : 76: train loss: 1.5542916685748012 Smoothed R interaction (validation) None\n",
      "epoch : 77: train loss: 1.558285989705358 Smoothed R interaction (validation) None\n",
      "epoch : 78: train loss: 1.554499785449213 Smoothed R interaction (validation) None\n",
      "epoch : 79: train loss: 1.555252912607417 Smoothed R interaction (validation) None\n",
      "epoch : 80: train loss: 1.558195477657766 Smoothed R interaction (validation) None\n",
      "epoch : 81: train loss: 1.5505274991022495 Smoothed R interaction (validation) None\n",
      "epoch : 82: train loss: 1.5494676267702883 Smoothed R interaction (validation) None\n",
      "epoch : 83: train loss: 1.5477328661493082 Smoothed R interaction (validation) None\n",
      "epoch : 84: train loss: 1.546286173155635 Smoothed R interaction (validation) None\n",
      "epoch : 85: train loss: 1.5471300515904562 Smoothed R interaction (validation) None\n",
      "epoch : 86: train loss: 1.5437906105232475 Smoothed R interaction (validation) None\n",
      "epoch : 87: train loss: 1.545643306485802 Smoothed R interaction (validation) None\n",
      "epoch : 88: train loss: 1.5465824117766618 Smoothed R interaction (validation) None\n",
      "epoch : 89: train loss: 1.5450391330294615 Smoothed R interaction (validation) None\n",
      "epoch : 90: train loss: 1.5412160392450018 Smoothed R interaction (validation) None\n",
      "epoch : 91: train loss: 1.538206883060328 Smoothed R interaction (validation) None\n",
      "epoch : 92: train loss: 1.5379576652100118 Smoothed R interaction (validation) None\n",
      "epoch : 93: train loss: 1.5385399254497107 Smoothed R interaction (validation) None\n",
      "epoch : 94: train loss: 1.5393956215920936 Smoothed R interaction (validation) None\n",
      "epoch : 95: train loss: 1.5387429315757988 Smoothed R interaction (validation) None\n",
      "epoch : 96: train loss: 1.5371092350874902 Smoothed R interaction (validation) None\n",
      "epoch : 97: train loss: 1.534158660985336 Smoothed R interaction (validation) None\n",
      "epoch : 98: train loss: 1.5332538103143718 Smoothed R interaction (validation) None\n",
      "epoch : 99: train loss: 1.5334059461232905 Smoothed R interaction (validation) None\n"
     ]
    }
   ],
   "source": [
    "#weight_path = \"trained_models/pretrained_omics.pth\"\n",
    "\n",
    "_, main_sim_model = finetune_model_train(config,\n",
    "                                         torch.utils.data.ConcatDataset([fine_sim_train_dataset, fine_val_sim_dataset]), \n",
    "                                         pre_trained_weight_path = weight_path,\n",
    "                                         validation_dataset = None, use_momentum=False)\n",
    "metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(\n",
    "    {\"R_cellwise_residuals\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"drugs\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=True),\n",
    "    \"R_cellwise\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"cell_lines\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=False),\n",
    "    \"MSE\":torchmetrics.MeanSquaredError()}))\n",
    "metrics.to(device)\n",
    "fine_sim_test_dataloader = torch.utils.data.DataLoader(fine_test_sim_dataset,\n",
    "                                       batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                       drop_last=False,\n",
    "                                      shuffle=False,\n",
    "                                      pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc37a1cf-70ad-422b-972a-631ee8b7c070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/kim14/project_work/scripts/models.py:69: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/TensorShape.cpp:3277.)\n",
      "  return torch.linalg.solve(A, Xy).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to: results/pred_pretrained_NoOptHyper_sim_matrix_20250225_13:59:59_0d96501d-5d73-4701-9051-f09e9c5398b6.csv\n",
      "main model final metrics: {'MSE': 2.1859209537506104, 'R_cellwise': 0.8763778209686279, 'R_cellwise_residuals': 0.0007535024778917432}\n"
     ]
    }
   ],
   "source": [
    "main_sim_final_metrics_nohyper = scripts.evaluate_step(main_sim_model, fine_sim_test_dataloader, metrics, device, save_predictions = True, model_name = \"pretrained_NoOptHyper\", dataset_name = \"sim_matrix\")\n",
    "print(f\"main model final metrics: {main_sim_final_metrics_nohyper}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb4fd3-c30c-4ca8-969d-f746485edd84",
   "metadata": {},
   "source": [
    "# Using raw data\n",
    "## Pre-training\n",
    "\n",
    "할일:\n",
    "\n",
    "1. 페드로와 토의. 기존에 하던 sim-sim, raw-raw 방식이 논리적으로 말이 안된다는것을 건의해보기. \n",
    "\n",
    "\n",
    "### 문제!\n",
    "\n",
    "Autoencoder의 역할은 단순히 입력을 압축한 뒤, 다시 복원하는 것이다. 이때 생성된 weights를 이후 Resnet의 fine-tuning에 사용하는것은 그렇게 논리적으로 신빙성이 있어보이지 않는다.\n",
    "또한, Autoencoder와 Resnet의 목적이 다르기 때문에, 예측 성능을 높이는데 도움이 되지 않을 가능성이 크다. \n",
    "\n",
    "그러나, Autoencoder는 입력 데이터에서 중요한 정보를 추출해내서 압축하는 과정을 거치기 때문에, 이렇게 압축된 feature들은 중요한 정보만을 가지고 있을 가능성이 크다. 이를 아예 finetune 과정에서 입력으로 제공하면, 더욱 논리적으로 타당하다. \n",
    "\n",
    "pre-trained weight로 결과가 나아지지 않는다는 것을 이미 파악했으니, 이번에는 pre-training과정에서 생성된 latent feature를 fine-tune의 1차 입력 데이터로 사용하는 것으로 하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31e23dc8-8eed-4837-95ee-b8b636c1e375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data loading with raw data\n",
    "pre_raw_train_dataset, pre_raw_val_dataset, pre_raw_test_dataset, pre_raw_dims= get_data_corr(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"], \n",
    "                                                           transform_into_corr = False,\n",
    "                                                           typ = (\"rnaseq\", \"proteomics\", \"mutations\", \"methylations\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf5d821f-d6a2-4bf1-8af6-4bfeed444efc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected input_dims: 4\n",
      "Epoch 1/100, Loss: 0.50720\n",
      "Epoch 2/100, Loss: 0.22842\n",
      "Epoch 3/100, Loss: 0.19452\n",
      "Epoch 4/100, Loss: 0.17042\n",
      "Epoch 5/100, Loss: 0.15249\n",
      "Epoch 6/100, Loss: 0.13720\n",
      "Epoch 7/100, Loss: 0.12511\n",
      "Epoch 8/100, Loss: 0.11496\n",
      "Epoch 9/100, Loss: 0.10688\n",
      "Epoch 10/100, Loss: 0.09995\n",
      "Epoch 11/100, Loss: 0.09482\n",
      "Epoch 12/100, Loss: 0.09108\n",
      "Epoch 13/100, Loss: 0.08740\n",
      "Epoch 14/100, Loss: 0.10341\n",
      "Epoch 15/100, Loss: 0.08243\n",
      "Epoch 16/100, Loss: 0.08148\n",
      "Epoch 17/100, Loss: 0.08112\n",
      "Epoch 18/100, Loss: 0.08070\n",
      "Epoch 19/100, Loss: 0.08023\n",
      "Epoch 20/100, Loss: 0.07970\n",
      "Epoch 21/100, Loss: 0.07967\n",
      "Epoch 22/100, Loss: 0.07910\n",
      "Epoch 23/100, Loss: 0.07876\n",
      "Epoch 24/100, Loss: 0.07855\n",
      "Epoch 25/100, Loss: 0.07827\n",
      "Epoch 26/100, Loss: 0.11180\n",
      "Epoch 27/100, Loss: 0.07711\n",
      "Epoch 28/100, Loss: 0.07656\n",
      "Epoch 29/100, Loss: 0.07639\n",
      "Epoch 30/100, Loss: 0.07634\n",
      "Epoch 31/100, Loss: 0.07629\n",
      "Epoch 32/100, Loss: 0.07620\n",
      "Epoch 33/100, Loss: 0.07613\n",
      "Epoch 34/100, Loss: 0.07609\n",
      "Epoch 35/100, Loss: 0.07597\n",
      "Epoch 36/100, Loss: 0.07588\n",
      "Epoch 37/100, Loss: 0.07578\n",
      "Epoch 38/100, Loss: 0.07573\n",
      "Epoch 39/100, Loss: 0.07534\n",
      "Epoch 40/100, Loss: 0.07527\n",
      "Epoch 41/100, Loss: 0.07510\n",
      "Epoch 42/100, Loss: 0.07503\n",
      "Epoch 43/100, Loss: 0.07498\n",
      "Epoch 44/100, Loss: 0.07477\n",
      "Epoch 45/100, Loss: 0.07483\n",
      "Epoch 46/100, Loss: 0.07463\n",
      "Epoch 47/100, Loss: 0.07466\n",
      "Epoch 48/100, Loss: 0.07433\n",
      "Epoch 49/100, Loss: 0.07421\n",
      "Epoch 50/100, Loss: 0.07418\n",
      "Epoch 51/100, Loss: 0.07403\n",
      "Epoch 52/100, Loss: 0.07407\n",
      "Epoch 53/100, Loss: 0.07391\n",
      "Epoch 54/100, Loss: 0.07388\n",
      "Epoch 55/100, Loss: 0.07382\n",
      "Epoch 56/100, Loss: 0.07373\n",
      "Epoch 57/100, Loss: 0.07372\n",
      "Epoch 58/100, Loss: 0.07364\n",
      "Epoch 59/100, Loss: 0.07360\n",
      "Epoch 60/100, Loss: 0.07353\n",
      "Epoch 61/100, Loss: 0.07349\n",
      "Epoch 62/100, Loss: 0.07344\n",
      "Epoch 63/100, Loss: 0.07342\n",
      "Epoch 64/100, Loss: 0.07336\n",
      "Epoch 65/100, Loss: 0.07334\n",
      "Epoch 66/100, Loss: 0.07327\n",
      "Epoch 67/100, Loss: 0.07323\n",
      "Epoch 68/100, Loss: 0.07323\n",
      "Epoch 69/100, Loss: 0.07302\n",
      "Epoch 70/100, Loss: 0.07291\n",
      "Epoch 71/100, Loss: 0.07282\n",
      "Epoch 72/100, Loss: 0.07289\n",
      "Epoch 73/100, Loss: 0.07275\n",
      "Epoch 74/100, Loss: 0.07265\n",
      "Epoch 75/100, Loss: 0.07240\n",
      "Epoch 76/100, Loss: 0.07209\n",
      "Epoch 77/100, Loss: 0.07212\n",
      "Epoch 78/100, Loss: 0.07208\n",
      "Epoch 79/100, Loss: 0.07210\n",
      "Epoch 80/100, Loss: 0.07188\n",
      "Epoch 81/100, Loss: 0.07181\n",
      "Epoch 82/100, Loss: 0.07174\n",
      "Epoch 83/100, Loss: 0.07175\n",
      "Epoch 84/100, Loss: 0.07167\n",
      "Epoch 85/100, Loss: 0.07160\n",
      "Epoch 86/100, Loss: 0.07516\n",
      "Epoch 87/100, Loss: 0.07052\n",
      "Epoch 88/100, Loss: 0.07031\n",
      "Epoch 89/100, Loss: 0.07033\n",
      "Epoch 90/100, Loss: 0.07043\n",
      "Epoch 91/100, Loss: 0.07050\n",
      "Epoch 92/100, Loss: 0.07055\n",
      "Epoch 93/100, Loss: 0.07052\n",
      "Epoch 94/100, Loss: 0.07049\n",
      "Epoch 95/100, Loss: 0.07037\n",
      "Epoch 96/100, Loss: 0.07040\n",
      "Epoch 97/100, Loss: 0.07038\n",
      "Epoch 98/100, Loss: 0.07031\n",
      "Epoch 99/100, Loss: 0.07026\n",
      "Epoch 100/100, Loss: 0.07024\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# pre training with raw data \n",
    "# 가중치 저장 버전\n",
    "\"\"\"\n",
    "pre_raw_train_loader = torch.utils.data.DataLoader(\n",
    "        pre_raw_train_dataset,\n",
    "        batch_size = config[\"optimizer\"][\"pre_batch_size\"],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "#if validation_dataset is not None:\n",
    "#    val_loader = torch.utils.data.DataLoader(val_dataset,batch_size = batch_size,shuffle=True,drop_last=True)\n",
    "    \n",
    "sample_batch = next(iter(pre_raw_train_loader))\n",
    "sample_omics_data = sample_batch[0]  # 첫 번째 요소는 omics_data (dict)\n",
    "mod_len = sample_omics_data.shape[1] // config[\"features\"][\"num_modalities\"]\n",
    "#input_dict = {f\"modality_{i+1}\": mod_len for i in range(num_modality)}\n",
    "\n",
    "pre_raw_model = MultimodalAutoencoder(input_dims = pre_raw_dims, \n",
    "                              hidden_dim_encoders = config[\"model\"][\"hidden_dim_encoders\"],\n",
    "                              embed_dim = config[\"model\"][\"embed_dim\"],\n",
    "                              fusion_dim = config[\"model\"][\"fusion_dim\"],\n",
    "                              #num_modalities = config[\"features\"][\"num_modalities\"], \n",
    "                              dropout_encoders = config[\"model\"][\"dropout_encoders\"])\n",
    "\n",
    "optimizer = torch.optim.Adam(pre_raw_model.parameters(), lr=config[\"optimizer\"][\"lr_pretraining\"])\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pre_raw_model.to(device)\n",
    "use_momentum = True\n",
    "# model training\n",
    "for epoch in range(config[\"model\"][\"pre_training_epochs\"]):\n",
    "    total_loss = []\n",
    "    pre_raw_model.train()\n",
    " \n",
    "    # train step\n",
    "    for batch in pre_raw_train_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "        #print(f\"Actual input data shape: {inputs.shape}\")\n",
    "\n",
    "        optimizer.zero_grad() # gradient initialization\n",
    "        reconstructed, latent = pre_raw_model(inputs)\n",
    "\n",
    "        #target_data = torch.cat([inputs[modality] for modality in inputs], dim=1)\n",
    "        # I think, here inappropriate loss is used. \n",
    "        # From what I know, Autoencoder reconstructs input feature, and calculate loss through comparing input feature and reconstructed feature.\n",
    "        # So, calculating MSE comparing with target_data, which is drug sensitivity, is nonsense. \n",
    "        #loss = criterion(reconstructed, target_data)\n",
    "        \n",
    "        # here is a new loss function, but it still outputs nan value as a loss...\n",
    "        loss = criterion(reconstructed, inputs)\n",
    "        \n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config[\"optimizer\"][\"clip_norm\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "    train_loss = np.mean(total_loss)\n",
    "    scheduler.step(train_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{config[\"model\"][\"pre_training_epochs\"]}, Train Loss: {train_loss:.5f}')\n",
    "\n",
    "torch.save(pre_raw_model.fusion_layer.state_dict(), \"trained_models/pretrained_raw_omics.pth\") # save pre-trained weights from fusion layer\n",
    "#torch.save(reconstructed,\"reconstructedMDA_data.pth\")\n",
    "print(\"Training complete!\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "495f9ead-f69f-4400-b37c-76e188955ffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected input_dims: 4\n",
      "Epoch 1/100, Train Loss: 0.47233\n",
      "Epoch 2/100, Train Loss: 0.22022\n",
      "Epoch 3/100, Train Loss: 0.18782\n",
      "Epoch 4/100, Train Loss: 0.16429\n",
      "Epoch 5/100, Train Loss: 0.14611\n",
      "Epoch 6/100, Train Loss: 0.13125\n",
      "Epoch 7/100, Train Loss: 0.11980\n",
      "Epoch 8/100, Train Loss: 0.11032\n",
      "Epoch 9/100, Train Loss: 0.10305\n",
      "Epoch 10/100, Train Loss: 0.09690\n",
      "Epoch 11/100, Train Loss: 0.09961\n",
      "Epoch 12/100, Train Loss: 0.08784\n",
      "Epoch 13/100, Train Loss: 0.08587\n",
      "Epoch 14/100, Train Loss: 0.08438\n",
      "Epoch 15/100, Train Loss: 0.08301\n",
      "Epoch 16/100, Train Loss: 0.08191\n",
      "Epoch 17/100, Train Loss: 0.08125\n",
      "Epoch 18/100, Train Loss: 0.08143\n",
      "Epoch 19/100, Train Loss: 0.07934\n",
      "Epoch 20/100, Train Loss: 0.07947\n",
      "Epoch 21/100, Train Loss: 0.07930\n",
      "Epoch 22/100, Train Loss: 0.10375\n",
      "Epoch 23/100, Train Loss: 0.09157\n",
      "Epoch 24/100, Train Loss: 0.07551\n",
      "Epoch 25/100, Train Loss: 0.07503\n",
      "Epoch 26/100, Train Loss: 0.07470\n",
      "Epoch 27/100, Train Loss: 0.07455\n",
      "Epoch 28/100, Train Loss: 0.07431\n",
      "Epoch 29/100, Train Loss: 0.07403\n",
      "Epoch 30/100, Train Loss: 0.07376\n",
      "Epoch 31/100, Train Loss: 0.07350\n",
      "Epoch 32/100, Train Loss: 0.07332\n",
      "Epoch 33/100, Train Loss: 0.07293\n",
      "Epoch 34/100, Train Loss: 0.07281\n",
      "Epoch 35/100, Train Loss: 0.07222\n",
      "Epoch 36/100, Train Loss: 0.07200\n",
      "Epoch 37/100, Train Loss: 0.07194\n",
      "Epoch 38/100, Train Loss: 0.07165\n",
      "Epoch 39/100, Train Loss: 0.07157\n",
      "Epoch 40/100, Train Loss: 0.07129\n",
      "Epoch 41/100, Train Loss: 0.07127\n",
      "Epoch 42/100, Train Loss: 0.07112\n",
      "Epoch 43/100, Train Loss: 0.07099\n",
      "Epoch 44/100, Train Loss: 0.07084\n",
      "Epoch 45/100, Train Loss: 0.07075\n",
      "Epoch 46/100, Train Loss: 0.07069\n",
      "Epoch 47/100, Train Loss: 0.07052\n",
      "Epoch 48/100, Train Loss: 0.07045\n",
      "Epoch 49/100, Train Loss: 0.07043\n",
      "Epoch 50/100, Train Loss: 0.07021\n",
      "Epoch 51/100, Train Loss: 0.07017\n",
      "Epoch 52/100, Train Loss: 0.07010\n",
      "Epoch 53/100, Train Loss: 0.07008\n",
      "Epoch 54/100, Train Loss: 0.06986\n",
      "Epoch 55/100, Train Loss: 0.06974\n",
      "Epoch 56/100, Train Loss: 0.06965\n",
      "Epoch 57/100, Train Loss: 0.06957\n",
      "Epoch 58/100, Train Loss: 0.06948\n",
      "Epoch 59/100, Train Loss: 0.06941\n",
      "Epoch 60/100, Train Loss: 0.06938\n",
      "Epoch 61/100, Train Loss: 0.06929\n",
      "Epoch 62/100, Train Loss: 0.06925\n",
      "Epoch 63/100, Train Loss: 0.06905\n",
      "Epoch 64/100, Train Loss: 0.06891\n",
      "Epoch 65/100, Train Loss: 0.06885\n",
      "Epoch 66/100, Train Loss: 0.06872\n",
      "Epoch 67/100, Train Loss: 0.06853\n",
      "Epoch 68/100, Train Loss: 0.06847\n",
      "Epoch 69/100, Train Loss: 0.06841\n",
      "Epoch 70/100, Train Loss: 0.06833\n",
      "Epoch 71/100, Train Loss: 0.06833\n",
      "Epoch 72/100, Train Loss: 0.06816\n",
      "Epoch 73/100, Train Loss: 0.06818\n",
      "Epoch 74/100, Train Loss: 0.06811\n",
      "Epoch 75/100, Train Loss: 0.06804\n",
      "Epoch 76/100, Train Loss: 0.06802\n",
      "Epoch 77/100, Train Loss: 0.06796\n",
      "Epoch 78/100, Train Loss: 0.06797\n",
      "Epoch 79/100, Train Loss: 0.06788\n",
      "Epoch 80/100, Train Loss: 0.06783\n",
      "Epoch 81/100, Train Loss: 0.06779\n",
      "Epoch 82/100, Train Loss: 0.06779\n",
      "Epoch 83/100, Train Loss: 0.06771\n",
      "Epoch 84/100, Train Loss: 0.06769\n",
      "Epoch 85/100, Train Loss: 0.06765\n",
      "Epoch 86/100, Train Loss: 0.06751\n",
      "Epoch 87/100, Train Loss: 0.06742\n",
      "Epoch 88/100, Train Loss: 0.06735\n",
      "Epoch 89/100, Train Loss: 0.06731\n",
      "Epoch 90/100, Train Loss: 0.06731\n",
      "Epoch 91/100, Train Loss: 0.06725\n",
      "Epoch 92/100, Train Loss: 0.06723\n",
      "Epoch 93/100, Train Loss: 0.06707\n",
      "Epoch 94/100, Train Loss: 0.06699\n",
      "Epoch 95/100, Train Loss: 0.06693\n",
      "Epoch 96/100, Train Loss: 0.06689\n",
      "Epoch 97/100, Train Loss: 0.06684\n",
      "Epoch 98/100, Train Loss: 0.06678\n",
      "Epoch 99/100, Train Loss: 0.06664\n",
      "Epoch 100/100, Train Loss: 0.06637\n",
      "Pre-training complete!\n"
     ]
    }
   ],
   "source": [
    "# pre training with raw data! final version. \n",
    "pretrained_model, pretraining_losses = pre_train_step(config, torch.utils.data.ConcatDataset([pre_raw_train_dataset, pre_raw_val_dataset]), input_dims = pre_raw_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce6d79-9c5b-4de4-8a43-c2c3df143e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만들긴 했는데, 굳이 안써도 될것 같다. \n",
    "def pre_eval(model, pre_test_dataset, device):\n",
    "    pre_raw_eval_loader = torch.utils.data.DataLoader(\n",
    "            pre_raw_test_dataset,\n",
    "            batch_size = config[\"optimizer\"][\"pre_batch_size\"],\n",
    "            shuffle=False, # we dont need this in evaluation step => repeatable result, for comparision\n",
    "            drop_last=False # to use all data\n",
    "        )\n",
    "    \n",
    "    pre_raw_model.to(device)\n",
    "    pre_raw_model.eval()\n",
    "    \n",
    "    eval_losses = []\n",
    "    \n",
    "    for x in pre_raw_eval_loader: \n",
    "        with torch.no_grad(): # no autograd => in evaluation step we dont need to update weights\n",
    "            inputs = batch[0].to(device)\n",
    "            reconstructed, _ = pre_raw_model(inputs)\n",
    "            evel_loss = criterion(reconstructed,inputs)\n",
    "            eval_losses.append(evel_loss.item())\n",
    "    return np.mean(eval_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1e8a650-7b3c-4cf8-97f5-c80069a42056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MSE of Model: 0.07000\n"
     ]
    }
   ],
   "source": [
    "# evaluation MDA\n",
    "\n",
    "pre_raw_eval_loader = torch.utils.data.DataLoader(\n",
    "        pre_raw_test_dataset,\n",
    "        batch_size = config[\"optimizer\"][\"pre_batch_size\"],\n",
    "        shuffle=False, # we dont need this in evaluation step => repeatable result, for comparision\n",
    "        drop_last=False # to use all data\n",
    "    )\n",
    "\n",
    "pre_raw_model.to(device)\n",
    "pre_raw_model.eval()\n",
    "\n",
    "eval_losses = []\n",
    "\n",
    "for x in pre_raw_eval_loader: \n",
    "    with torch.no_grad(): # no autograd => in evaluation step we dont need to update weights\n",
    "        inputs = batch[0].to(device)\n",
    "        reconstructed, _ = pre_raw_model(inputs)\n",
    "        evel_loss = criterion(reconstructed,inputs)\n",
    "        eval_losses.append(evel_loss.item())\n",
    "\n",
    "final_eval_loss = np.mean(eval_losses)\n",
    "print(f\"Final MSE of Model: {final_eval_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aabc1ea-0b33-4348-a066-9bc7bb6ff7fe",
   "metadata": {},
   "source": [
    "## Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b7a2bfc-e55a-4fe0-b3ad-7cedeea62f19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:20:39] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "# we dont use train dataset here! we use latent features from pre-training.\n",
    "# actually, this step is unnecessary. pre-fine both uses same data.\n",
    "fine_raw_train_dataset, fine_raw_val_dataset, fine_raw_test_dataset, fine_raw_dims = get_data_corr(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"], \n",
    "                                                           transform_into_corr = False,\n",
    "                                                           typ = (\"rnaseq\", \"proteomics\", \"mutations\", \"methylations\"),\n",
    "                                                           #reconstructed = recon_sim\n",
    "                                                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f671f282-e5e9-4c70-b383-99d94b49523e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the omics data of 737 samples are replaced to pre-trained latent features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: FutureWarning: The default value for `maximize` will be changed from `True` to `None` in v1.7.0 of TorchMetrics,will automatically infer the value based on the `higher_is_better` attribute of the metric (if such attribute exists) or raise an error if it does not. If you are explicitly setting the `maximize` argument to either `True` or `False` already, you can ignore this warning.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0: train loss: 2.22680638850251 Smoothed R interaction (validation) None\n",
      "epoch : 1: train loss: 1.8836918908674432 Smoothed R interaction (validation) None\n",
      "epoch : 2: train loss: 1.819210430010877 Smoothed R interaction (validation) None\n",
      "epoch : 3: train loss: 1.7625145434743836 Smoothed R interaction (validation) None\n",
      "epoch : 4: train loss: 1.7220535630496088 Smoothed R interaction (validation) None\n",
      "epoch : 5: train loss: 1.7004863274259532 Smoothed R interaction (validation) None\n",
      "epoch : 6: train loss: 1.6739621645735279 Smoothed R interaction (validation) None\n",
      "epoch : 7: train loss: 1.665813243212305 Smoothed R interaction (validation) None\n",
      "epoch : 8: train loss: 1.6423299567513767 Smoothed R interaction (validation) None\n",
      "epoch : 9: train loss: 1.633694402514341 Smoothed R interaction (validation) None\n",
      "epoch : 10: train loss: 1.6285162163605944 Smoothed R interaction (validation) None\n",
      "epoch : 11: train loss: 1.610349529163799 Smoothed R interaction (validation) None\n",
      "epoch : 12: train loss: 1.605704639249738 Smoothed R interaction (validation) None\n",
      "epoch : 13: train loss: 1.5998404783430442 Smoothed R interaction (validation) None\n",
      "epoch : 14: train loss: 1.5933175141496034 Smoothed R interaction (validation) None\n",
      "epoch : 15: train loss: 1.589187088796618 Smoothed R interaction (validation) None\n",
      "epoch : 16: train loss: 1.5830344035392638 Smoothed R interaction (validation) None\n",
      "epoch : 17: train loss: 1.5814366654500973 Smoothed R interaction (validation) None\n",
      "epoch : 18: train loss: 1.5743868118163535 Smoothed R interaction (validation) None\n",
      "epoch : 19: train loss: 1.5770521954494 Smoothed R interaction (validation) None\n",
      "epoch : 20: train loss: 1.5665669305657575 Smoothed R interaction (validation) None\n",
      "epoch : 21: train loss: 1.562096311886909 Smoothed R interaction (validation) None\n",
      "epoch : 22: train loss: 1.5597907250242857 Smoothed R interaction (validation) None\n",
      "epoch : 23: train loss: 1.5563753356302918 Smoothed R interaction (validation) None\n",
      "epoch : 24: train loss: 1.5552482630030628 Smoothed R interaction (validation) None\n",
      "epoch : 25: train loss: 1.5551516706186113 Smoothed R interaction (validation) None\n",
      "epoch : 26: train loss: 1.5558870307299941 Smoothed R interaction (validation) None\n",
      "epoch : 27: train loss: 1.547661818444213 Smoothed R interaction (validation) None\n",
      "epoch : 28: train loss: 1.5505278476678543 Smoothed R interaction (validation) None\n",
      "epoch : 29: train loss: 1.5441979637694152 Smoothed R interaction (validation) None\n",
      "epoch : 30: train loss: 1.5478729540102267 Smoothed R interaction (validation) None\n",
      "epoch : 31: train loss: 1.5455236499772231 Smoothed R interaction (validation) None\n",
      "epoch : 32: train loss: 1.542115229316636 Smoothed R interaction (validation) None\n",
      "epoch : 33: train loss: 1.5372219995603573 Smoothed R interaction (validation) None\n",
      "epoch : 34: train loss: 1.5390956549915602 Smoothed R interaction (validation) None\n",
      "epoch : 35: train loss: 1.536885488313267 Smoothed R interaction (validation) None\n",
      "epoch : 36: train loss: 1.5329248398284534 Smoothed R interaction (validation) None\n",
      "epoch : 37: train loss: 1.5324904612174581 Smoothed R interaction (validation) None\n",
      "epoch : 38: train loss: 1.5288914580575026 Smoothed R interaction (validation) None\n",
      "epoch : 39: train loss: 1.5287994419688495 Smoothed R interaction (validation) None\n",
      "epoch : 40: train loss: 1.526645224822319 Smoothed R interaction (validation) None\n",
      "epoch : 41: train loss: 1.5269158495076656 Smoothed R interaction (validation) None\n",
      "epoch : 42: train loss: 1.5287802791123926 Smoothed R interaction (validation) None\n",
      "epoch : 43: train loss: 1.5245240477313218 Smoothed R interaction (validation) None\n",
      "epoch : 44: train loss: 1.5230083552514961 Smoothed R interaction (validation) None\n",
      "epoch : 45: train loss: 1.5239369248872339 Smoothed R interaction (validation) None\n",
      "epoch : 46: train loss: 1.5233502783645658 Smoothed R interaction (validation) None\n",
      "epoch : 47: train loss: 1.5213142281999106 Smoothed R interaction (validation) None\n",
      "epoch : 48: train loss: 1.52225357213038 Smoothed R interaction (validation) None\n",
      "epoch : 49: train loss: 1.5187533692317485 Smoothed R interaction (validation) None\n",
      "epoch : 50: train loss: 1.5221887184456635 Smoothed R interaction (validation) None\n",
      "epoch : 51: train loss: 1.5157227729837444 Smoothed R interaction (validation) None\n",
      "epoch : 52: train loss: 1.5095826828730268 Smoothed R interaction (validation) None\n",
      "epoch : 53: train loss: 1.5148080998504412 Smoothed R interaction (validation) None\n",
      "epoch : 54: train loss: 1.513199198083913 Smoothed R interaction (validation) None\n",
      "epoch : 55: train loss: 1.5101641143945004 Smoothed R interaction (validation) None\n",
      "epoch : 56: train loss: 1.4990969690021094 Smoothed R interaction (validation) None\n",
      "epoch : 57: train loss: 1.5073370176133767 Smoothed R interaction (validation) None\n",
      "epoch : 58: train loss: 1.506629563453613 Smoothed R interaction (validation) None\n",
      "epoch : 59: train loss: 1.5092466625353904 Smoothed R interaction (validation) None\n",
      "epoch : 60: train loss: 1.5047525979385092 Smoothed R interaction (validation) None\n",
      "epoch : 61: train loss: 1.5010675127780335 Smoothed R interaction (validation) None\n",
      "epoch : 62: train loss: 1.5052363171123604 Smoothed R interaction (validation) None\n",
      "epoch : 63: train loss: 1.5001085789595605 Smoothed R interaction (validation) None\n",
      "epoch : 64: train loss: 1.5002642337265062 Smoothed R interaction (validation) None\n",
      "epoch : 65: train loss: 1.494447050415836 Smoothed R interaction (validation) None\n",
      "epoch : 66: train loss: 1.4965697478304982 Smoothed R interaction (validation) None\n",
      "epoch : 67: train loss: 1.4984358482072027 Smoothed R interaction (validation) None\n",
      "epoch : 68: train loss: 1.4995717114512204 Smoothed R interaction (validation) None\n",
      "epoch : 69: train loss: 1.4974200636406323 Smoothed R interaction (validation) None\n",
      "epoch : 70: train loss: 1.4946325391862536 Smoothed R interaction (validation) None\n",
      "epoch : 71: train loss: 1.5008766998908722 Smoothed R interaction (validation) None\n",
      "epoch : 72: train loss: 1.4959683720349382 Smoothed R interaction (validation) None\n",
      "epoch : 73: train loss: 1.4875611407205702 Smoothed R interaction (validation) None\n",
      "epoch : 74: train loss: 1.4963727732671352 Smoothed R interaction (validation) None\n",
      "epoch : 75: train loss: 1.49239153310896 Smoothed R interaction (validation) None\n",
      "epoch : 76: train loss: 1.4927299438949273 Smoothed R interaction (validation) None\n",
      "epoch : 77: train loss: 1.4909164126046066 Smoothed R interaction (validation) None\n",
      "epoch : 78: train loss: 1.492729518042212 Smoothed R interaction (validation) None\n",
      "epoch : 79: train loss: 1.4911057626361164 Smoothed R interaction (validation) None\n",
      "epoch : 80: train loss: 1.4897255830475957 Smoothed R interaction (validation) None\n",
      "epoch : 81: train loss: 1.486800983824453 Smoothed R interaction (validation) None\n",
      "epoch : 82: train loss: 1.4882134680252876 Smoothed R interaction (validation) None\n",
      "epoch : 83: train loss: 1.4854163762518149 Smoothed R interaction (validation) None\n",
      "epoch : 84: train loss: 1.4827831245323342 Smoothed R interaction (validation) None\n",
      "epoch : 85: train loss: 1.4889758665423163 Smoothed R interaction (validation) None\n",
      "epoch : 86: train loss: 1.4849433360347346 Smoothed R interaction (validation) None\n",
      "epoch : 87: train loss: 1.4873010620051172 Smoothed R interaction (validation) None\n",
      "epoch : 88: train loss: 1.4862551222036147 Smoothed R interaction (validation) None\n",
      "epoch : 89: train loss: 1.485703930350847 Smoothed R interaction (validation) None\n",
      "epoch : 90: train loss: 1.4872836225259731 Smoothed R interaction (validation) None\n",
      "epoch : 91: train loss: 1.4866406681069042 Smoothed R interaction (validation) None\n",
      "epoch : 92: train loss: 1.4849700046558167 Smoothed R interaction (validation) None\n",
      "epoch : 93: train loss: 1.4882986148444035 Smoothed R interaction (validation) None\n",
      "epoch : 94: train loss: 1.4820922439266342 Smoothed R interaction (validation) None\n",
      "epoch : 95: train loss: 1.476246892521791 Smoothed R interaction (validation) None\n",
      "epoch : 96: train loss: 1.4861643398353284 Smoothed R interaction (validation) None\n",
      "epoch : 97: train loss: 1.4831848299252826 Smoothed R interaction (validation) None\n",
      "epoch : 98: train loss: 1.4756371019355152 Smoothed R interaction (validation) None\n",
      "epoch : 99: train loss: 1.4791958987049767 Smoothed R interaction (validation) None\n"
     ]
    }
   ],
   "source": [
    "#weight_path_raw = \"trained_models/pretrained_raw_omics.pth\"\n",
    "_, main_raw_model = finetune_model_train(config,\n",
    "                                         torch.utils.data.ConcatDataset([fine_raw_train_dataset, fine_raw_val_dataset]), \n",
    "                                         pre_trained_model = pretrained_model,\n",
    "                                         use_momentum=False)\n",
    "metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(\n",
    "    {\"R_cellwise_residuals\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"drugs\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=True),\n",
    "    \"R_cellwise\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"cell_lines\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=False),\n",
    "    \"MSE\":torchmetrics.MeanSquaredError()}))\n",
    "metrics.to(device)\n",
    "fine_raw_test_dataloader = torch.utils.data.DataLoader(fine_raw_test_dataset,\n",
    "                                       batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                       drop_last=False,\n",
    "                                      shuffle=False,\n",
    "                                      pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0b19125b-2cc0-44c3-a161-f1ed462ec128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to: results/pred_pretrained_NoOptHyper_raw_data_20250303_01:07:22_280646e8-3475-47d3-8114-c4166b3a2430.csv\n",
      "main model final metrics: {'MSE': 1.7892388105392456, 'R_cellwise': 0.8867660164833069, 'R_cellwise_residuals': 0.21353977918624878}\n"
     ]
    }
   ],
   "source": [
    "main_raw_final_metrics_nohyper = scripts.evaluate_step(main_raw_model, fine_raw_test_dataloader, metrics, device, save_predictions = True, model_name = \"pretrained_NoOptHyper\", dataset_name = \"raw_data\")\n",
    "print(f\"main model final metrics: {main_raw_final_metrics_nohyper}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81623a07-4201-4a10-a9c3-3e44b49aefa6",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "24555ffa-bb76-4721-86a4-2449c66c7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_optuna(trial, config):\n",
    "    \"\"\"\n",
    "    Optuna를 활용한 하이퍼파라미터 최적화 함수\n",
    "    - Pre-training (Autoencoder)\n",
    "    - Fine-tuning (ResNet)\n",
    "    \"\"\"\n",
    "\n",
    "    ### 1️⃣ Pre-training 하이퍼파라미터 샘플링 ###\n",
    "    config[\"model\"][\"embed_dim\"] = trial.suggest_int(\"embed_dim\", 64, 512)\n",
    "    config[\"model\"][\"hidden_dim_encoders\"] = trial.suggest_int(\"hidden_dim_encoders\", 64, 2048)\n",
    "    config[\"model\"][\"fusion_dim\"] = trial.suggest_int(\"fusion_dim\", 64, 2048)\n",
    "    config[\"model\"][\"dropout_encoders\"] = trial.suggest_float(\"dropout_encoders\", 0.0, 0.5)\n",
    "    config[\"model\"][\"pre_training_epochs\"] = trial.suggest_int(\"pre_training_epochs\", 1, 500)\n",
    "    config[\"optimizer\"][\"pre_batch_size\"] = trial.suggest_int(\"pre_batch_size\", 128, 512)\n",
    "    config[\"optimizer\"][\"lr_pretraining\"] = trial.suggest_float(\"lr_pretraining\", 1e-6, 1e-1, log=True)\n",
    "\n",
    "    ### 2️⃣ Pre-training 실행 ###\n",
    "    try:\n",
    "        #pre_raw_dataset = torch.utils.data.ConcatDataset([pre_raw_train_dataset, pre_raw_val_dataset])\n",
    "        pre_trained_model, pre_training_losses = pre_train_step(config, pre_raw_train_dataset, input_dims=pre_raw_dims)\n",
    "\n",
    "        # Pre-training 동안의 최소 loss 반환 (낮을수록 좋음)\n",
    "        pre_training_score = min(pre_training_losses)\n",
    "        print(f\"Trial {trial.number}: Pre-training Loss = {pre_training_score:.5f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Pre-training Error: {e}\")\n",
    "        return float(\"inf\")  # Pre-training 실패 시, 최대 loss 반환\n",
    "\n",
    "    ### 3️⃣ Fine-tuning 하이퍼파라미터 샘플링 ###\n",
    "    config[\"model\"][\"dropout_omics\"] = trial.suggest_float(\"dropout_omics\", 0.0, 0.9)\n",
    "    config[\"model\"][\"dropout_omics_finetuning\"] = trial.suggest_float(\"dropout_omics_finetuning\", 0.0, 0.9)\n",
    "\n",
    "    ### 4️⃣ Fine-tuning 실행 (Latent Representation 생성 포함) ###\n",
    "    try:\n",
    "        fine_tuned_model, val_target = finetune_model_train(\n",
    "            config, fine_raw_train_dataset, pre_trained_model, fine_raw_val_dataset\n",
    "        )\n",
    "\n",
    "        print(f\"Trial {trial.number}: Fine-tuning R_cellwise_residuals = {val_target:.5f}\")\n",
    "        return val_target  # Fine-tuning 성능 반환 (높을수록 좋음)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fine-tuning Error: {e}\")\n",
    "        return -float(\"inf\")  # Fine-tuning 실패 시, 최소 성능 반환\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8a54afb6-f81e-4e29-b7e9-905a45c9fa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected input_dims: 4\n",
      "Epoch 1/100, Train Loss: 0.47340\n",
      "Epoch 2/100, Train Loss: 0.22392\n",
      "Epoch 3/100, Train Loss: 0.18894\n",
      "Epoch 4/100, Train Loss: 0.16475\n",
      "Epoch 5/100, Train Loss: 0.14655\n",
      "Epoch 6/100, Train Loss: 0.13096\n",
      "Epoch 7/100, Train Loss: 0.11915\n",
      "Epoch 8/100, Train Loss: 0.11128\n",
      "Epoch 9/100, Train Loss: 0.10199\n",
      "Epoch 10/100, Train Loss: 0.09705\n",
      "Epoch 11/100, Train Loss: 0.09227\n",
      "Epoch 12/100, Train Loss: 0.14269\n",
      "Epoch 13/100, Train Loss: 0.09182\n",
      "Epoch 14/100, Train Loss: 0.08695\n",
      "Epoch 15/100, Train Loss: 0.08497\n",
      "Epoch 16/100, Train Loss: 0.08385\n",
      "Epoch 17/100, Train Loss: 0.08271\n",
      "Epoch 18/100, Train Loss: 0.08183\n",
      "Epoch 19/100, Train Loss: 0.08117\n",
      "Epoch 20/100, Train Loss: 0.08038\n",
      "Epoch 21/100, Train Loss: 0.08002\n",
      "Epoch 22/100, Train Loss: 0.07956\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m pre_raw_dataset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mConcatDataset([pre_raw_train_dataset, pre_raw_val_dataset])\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Pre-training 실행\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m pre_trained_model, pre_training_losses \u001b[38;5;241m=\u001b[39m \u001b[43mpre_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_raw_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_raw_dims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Fine-tuning 실행 (이제 Train + Validation을 합쳐서 사용)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m fine_tuned_model, final_target \u001b[38;5;241m=\u001b[39m finetune_model_train(\n\u001b[1;32m     38\u001b[0m     config, pre_raw_dataset, pre_trained_model, validation_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# ✅ Validation 없이 학습\u001b[39;00m\n\u001b[1;32m     39\u001b[0m )\n",
      "Cell \u001b[0;32mIn[77], line 43\u001b[0m, in \u001b[0;36mpre_train_step\u001b[0;34m(config, pre_raw_dataset, input_dims)\u001b[0m\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(reconstructed, inputs)\n\u001b[1;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 43\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Latent Features 저장\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#all_latent_features.append(latent_features.cpu().detach().numpy())\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/torch/optim/adam.py:364\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    363\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 364\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    367\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1️⃣ Optuna를 사용한 Hyperparameter Optimization (Validation 데이터 필요)\n",
    "if config[\"env\"][\"search_hyperparameters\"]:\n",
    "    study_name = \"raw_pretrained\"\n",
    "    storage_name = f\"sqlite:///studies/{study_name}.db\"\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=storage_name,\n",
    "        direction='maximize',  # Fine-tuning 성능을 최적화 (R_cellwise_residuals 최대화)\n",
    "        load_if_exists=True,\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=30, n_warmup_steps=5, interval_steps=5)\n",
    "    )\n",
    "\n",
    "    study.optimize(lambda trial: train_model_optuna(trial, config), n_trials=40)\n",
    "\n",
    "    best_config = study.best_params\n",
    "    print(\"Best Hyperparameters:\", best_config)\n",
    "\n",
    "    # 최적의 하이퍼파라미터를 config에 반영\n",
    "    config[\"model\"][\"embed_dim\"] = best_config[\"embed_dim\"]\n",
    "    config[\"model\"][\"hidden_dim_encoders\"] = best_config[\"hidden_dim_encoders\"]\n",
    "    config[\"model\"][\"fusion_dim\"] = best_config[\"fusion_dim\"]\n",
    "    config[\"model\"][\"dropout_encoders\"] = best_config[\"dropout_encoders\"]\n",
    "    config[\"model\"][\"pre_training_epochs\"] = best_config[\"pre_training_epochs\"]\n",
    "    config[\"optimizer\"][\"pre_batch_size\"] = best_config[\"pre_batch_size\"]\n",
    "    config[\"optimizer\"][\"lr_pretraining\"] = best_config[\"lr_pretraining\"]\n",
    "    config[\"model\"][\"dropout_omics\"] = best_config[\"dropout_omics\"]\n",
    "    config[\"model\"][\"dropout_omics_finetuning\"] = best_config[\"dropout_omics_finetuning\"]\n",
    "\n",
    "# 2️⃣ 최적 하이퍼파라미터를 적용한 후, Training (Train + Validation 합쳐서 사용)\n",
    "pre_raw_dataset = torch.utils.data.ConcatDataset([pre_raw_train_dataset, pre_raw_val_dataset])\n",
    "\n",
    "# Pre-training 실행\n",
    "pre_trained_model, pre_training_losses = pre_train_step(config, pre_raw_dataset, input_dims=pre_raw_dims)\n",
    "\n",
    "# Fine-tuning 실행 (이제 Train + Validation을 합쳐서 사용)\n",
    "fine_tuned_model, final_target = finetune_model_train(\n",
    "    config, pre_raw_dataset, pre_trained_model, validation_dataset=None  # ✅ Validation 없이 학습\n",
    ")\n",
    "\n",
    "print(f\"Final Training Complete! Final R_cellwise_residuals: {final_target:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9341b18-b33d-4a32-9b0a-32e6c5837d43",
   "metadata": {},
   "source": [
    "# Mülleimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81276835-348b-420d-aee3-dc30de9deaba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def get_data_corr_with_filtering(n_fold = 0, fp_radius = 2, transform_into_corr = True, typ = [\"rnaseq\", \"mutations\", \"cnvs\"]):\n",
    "    # drug\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    \n",
    "    # loading all datasets\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    \n",
    "    proteomics = pd.read_csv(\"data/proteomics.csv\", index_col=0)\n",
    "    \n",
    "    mutation = pd.read_csv(\"data/binary_mutations.csv\")\n",
    "    mutation.columns = mutation.iloc[0]\n",
    "    mutation = mutation.iloc[2:,:].set_index(\"gene_symbol\")\n",
    "    driver_columns = mutation.columns.isin(driver_genes)\n",
    "    filtered_mut = mutation.loc[:, driver_columns]\n",
    "    filtered_mut = filtered_mut.astype(float)\n",
    "\n",
    "    methylations = pd.read_csv(\"data/methylations.csv\",index_col = 0).sort_index(ascending = True)\n",
    "\n",
    "    cnvs = pd.read_csv(\"data/copy_number_variations.csv\",index_col= 0)\n",
    "\n",
    "    # concatenate all dataset \n",
    "    # inner join based on index: model_ids with NaN are automatically filtered out \n",
    "    data_concat = pd.concat([filtered_rna, proteomics, filtered_mut, methylations, cnvs], axis=1, join='inner')\n",
    "    \n",
    "    \n",
    "    # Filter data by common indices in all modalities\n",
    "    filtered_rna = filtered_rna[filtered_rna.index.isin(data_concat.index)]\n",
    "    proteomics = proteomics[proteomics.index.isin(data_concat.index)]\n",
    "    filtered_mut = filtered_mut[filtered_mut.index.isin(data_concat.index)]\n",
    "    methylations = methylations[methylations.index.isin(data_concat.index)]\n",
    "    cnvs = cnvs[cnvs.index.isin(data_concat.index)]\n",
    "    \n",
    "    # Initialize cell_dict\n",
    "    cell_dict = {}\n",
    "\n",
    "    if not transform_into_corr:\n",
    "        for cell in data_concat.index:\n",
    "            # Initialize a sub-dictionary for each cell\n",
    "            cell_dict[cell] = {}\n",
    "            \n",
    "            # Add data for each type specified in typ\n",
    "            if \"rnaseq\" in typ:\n",
    "                cell_dict[cell][\"rnaseq\"] = torch.Tensor(filtered_rna.loc[cell].to_numpy())\n",
    "            if \"proteomics\" in typ:\n",
    "                cell_dict[cell][\"proteomics\"] = torch.Tensor(proteomics.loc[cell].to_numpy())\n",
    "            if \"mutations\" in typ:\n",
    "                cell_dict[cell][\"mutations\"] = torch.Tensor(filtered_mut.loc[cell].to_numpy())\n",
    "            if \"methylations\" in typ:\n",
    "                cell_dict[cell][\"methylations\"] = torch.Tensor(methylations.loc[cell].to_numpy())\n",
    "            if \"cnvs\" in typ:\n",
    "                cell_dict[cell][\"cnvs\"] = torch.Tensor(cnvs.loc[cell].to_numpy())\n",
    "\n",
    "    # GDSC\n",
    "    GDSC1 = pd.read_csv(\"data/GDSC1.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = GDSC1.query(\"SANGER_MODEL_ID in @data_concat.index & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "\n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    test_lines = folds[0] \n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold] \n",
    "    \n",
    "    if transform_into_corr:\n",
    "        # ic50 filtering\n",
    "        ic50_mat = data.pivot(index = 'SANGER_MODEL_ID', columns = 'DRUG_ID', values = 'LN_IC50')\n",
    "        drug_nan_ratio = ic50_mat.isna().mean(axis=0) \n",
    "        cellline_nan_ratio = ic50_mat.isna().mean(axis=1)\n",
    "        filtered_ic50 = ic50_mat.loc[cellline_nan_ratio < 0.3, drug_nan_ratio < 0.3]\n",
    "        imputer = KNNImputer(n_neighbors=5)  # k-NN에서 k=5\n",
    "        imputed_ic50 = pd.DataFrame(\n",
    "            imputer.fit_transform(filtered_ic50),\n",
    "            index=filtered_ic50.index,\n",
    "            columns=filtered_ic50.columns)      \n",
    "        t = imputed_ic50.median()\n",
    "        binarized_ic50 = imputed_ic50.apply(lambda x: x.apply(lambda v: 1 if v <= t[x.name] else 0), axis=0)\n",
    "        \n",
    "        # index filtering, here only exp, mutation, cnv data are used\n",
    "        cell_line_index = binarized_ic50.index.intersection(data_concat.index)\n",
    "        ic50 = binarized_ic50.loc[cell_line_index]\n",
    "        \n",
    "        # train, val, test among filtered data\n",
    "        # these are valid train_, val_ and test_data index\n",
    "        train_lines = np.intersect1d(train_lines, ic50.index)\n",
    "        valid_validation_lines = np.intersect1d(validation_lines, ic50.index)\n",
    "        valid_test_lines = np.intersect1d(test_lines, ic50.index)\n",
    "        \n",
    "        n_train = len(train_lines)  \n",
    "        n_val = len(valid_validation_lines)      \n",
    "        n_test = len(valid_test_lines)\n",
    "        \n",
    "        # Precompute similarity matrices for each data type\n",
    "        similarity_matrices = {}\n",
    "        \n",
    "        if \"rnaseq\" in typ:\n",
    "            exp_com = np.corrcoef(np.vstack([filtered_rna.loc[train_lines], \n",
    "                                             filtered_rna.loc[valid_validation_lines], \n",
    "                                             filtered_rna.loc[valid_test_lines]]), rowvar=True)\n",
    "            train = exp_com[:n_train, :n_train]\n",
    "            val = exp_com[n_train:n_train+n_val, :n_train]\n",
    "            test = exp_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"rnaseq\"] = sim_combined\n",
    "        \n",
    "        if \"proteomics\" in typ:\n",
    "            prot_com = np.corrcoef(np.vstack([proteomics.loc[train_lines], \n",
    "                                              proteomics.loc[valid_validation_lines], \n",
    "                                              proteomics.loc[valid_test_lines]]), rowvar=True)\n",
    "            train = prot_com[:n_train, :n_train]\n",
    "            val = prot_com[n_train:n_train+n_val, :n_train]\n",
    "            test = prot_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"proteomics\"] = sim_combined\n",
    "        \n",
    "        if \"mutations\" in typ:\n",
    "            train_snp = filtered_mut.loc[train_lines].astype(bool)\n",
    "            val_snp = filtered_mut.loc[valid_validation_lines].astype(bool)\n",
    "            test_snp = filtered_mut.loc[valid_test_lines].astype(bool)\n",
    "            \n",
    "            train = 1 - pairwise_distances(train_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            val = 1 - pairwise_distances(val_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            test = 1 - pairwise_distances(test_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "    \n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"mutations\"] = sim_combined\n",
    "        \n",
    "        if \"methylations\" in typ:\n",
    "            methyl_com = np.corrcoef(np.vstack([methylations.loc[train_lines], \n",
    "                                                methylations.loc[valid_validation_lines], \n",
    "                                                methylations.loc[valid_test_lines]]), rowvar=True)\n",
    "            train = methyl_com[:n_train, :n_train]\n",
    "            val = methyl_com[n_train:n_train+n_val, :n_train]\n",
    "            test = methyl_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"mathylations\"] = sim_combined\n",
    "        \n",
    "        if \"cnvs\" in typ:\n",
    "            cnv_com = np.corrcoef(np.vstack([cnvs.loc[train_lines], \n",
    "                                             cnvs.loc[valid_validation_lines], \n",
    "                                             cnvs.loc[valid_test_lines]]), rowvar=True)\n",
    "            train= cnv_com[:n_train, :n_train]\n",
    "            val= cnv_com[n_train:n_train+n_val, :n_train]\n",
    "            test= cnv_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"cnvs\"] = sim_combined\n",
    "            \n",
    "        cell_dict = {}\n",
    "\n",
    "        # \n",
    "        for cell in cell_line_index:\n",
    "            cell_dict[cell] = {}\n",
    "            for data_type in typ:\n",
    "                sim_matrices = similarity_matrices[data_type]\n",
    "                sim_tensor = torch.Tensor(sim_matrices)\n",
    "                cell_idx = cell_line_index.get_loc(cell)\n",
    "                cell_dict[cell][data_type] = sim_tensor[cell_idx]\n",
    "                \n",
    "        train_lines = train_lines\n",
    "        validation_lines = valid_validation_lines\n",
    "        test_lines = valid_test_lines\n",
    "\n",
    "    # no change needed, query works fine with some missing\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "    \n",
    "    return (OmicsDataset_dict(cell_dict, drug_dict, train_data),\n",
    "    OmicsDataset_dict(cell_dict, drug_dict, validation_data),\n",
    "    OmicsDataset_dict(cell_dict, drug_dict, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "034d85fc-39ea-43da-9a59-369f824c3746",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:39:48] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "n_fold = 0\n",
    "fp_radius = 2\n",
    "transform_into_corr = True\n",
    "typ = [\"rnaseq\", \"proteomics\", \"mutations\", \"methylations\"]\n",
    "\n",
    "# drug\n",
    "smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "\n",
    "# loading all datasets\n",
    "driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "\n",
    "rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "\n",
    "proteomics = pd.read_csv(\"data/proteomics.csv\", index_col=0)\n",
    "\n",
    "mutation = pd.read_csv(\"data/binary_mutations.csv\")\n",
    "mutation.columns = mutation.iloc[0]\n",
    "mutation = mutation.iloc[2:,:].set_index(\"gene_symbol\")\n",
    "driver_columns = mutation.columns.isin(driver_genes)\n",
    "filtered_mut = mutation.loc[:, driver_columns]\n",
    "filtered_mut = filtered_mut.astype(float)\n",
    "\n",
    "methylations = pd.read_csv(\"data/methylations.csv\",index_col = 0).sort_index(ascending = True)\n",
    "\n",
    "cnvs = pd.read_csv(\"data/copy_number_variations.csv\",index_col= 0)\n",
    "\n",
    "# concatenate all dataset \n",
    "# inner join based on index: model_ids with NaN are automatically filtered out \n",
    "data_concat = pd.concat([filtered_rna, proteomics, filtered_mut, methylations, cnvs], axis=1, join='inner')\n",
    "\n",
    "\n",
    "# Filter data by common indices in all modalities\n",
    "filtered_rna = filtered_rna[filtered_rna.index.isin(data_concat.index)]\n",
    "proteomics = proteomics[proteomics.index.isin(data_concat.index)]\n",
    "filtered_mut = filtered_mut[filtered_mut.index.isin(data_concat.index)]\n",
    "methylations = methylations[methylations.index.isin(data_concat.index)]\n",
    "cnvs = cnvs[cnvs.index.isin(data_concat.index)]\n",
    "\n",
    "# Initialize cell_dict\n",
    "cell_dict = {}\n",
    "\n",
    "if not transform_into_corr:\n",
    "    for cell in data_concat.index:\n",
    "        # Initialize a sub-dictionary for each cell\n",
    "        concatenated_data = []\n",
    "        \n",
    "        # Add data for each type specified in typ\n",
    "        if \"rnaseq\" in typ:\n",
    "            concatenated_data.append(filtered_rna.loc[cell].to_numpy())\n",
    "        if \"proteomics\" in typ:\n",
    "            concatenated_data.append(proteomics.loc[cell].to_numpy())\n",
    "        if \"mutations\" in typ:\n",
    "            concatenated_data.append(filtered_mut.loc[cell].to_numpy())\n",
    "        if \"methylations\" in typ:\n",
    "            concatenated_data.append(methylations.loc[cell].to_numpy())\n",
    "        if \"cnvs\" in typ:\n",
    "            concatenated_data.append(cnvs.loc[cell].to_numpy())\n",
    "\n",
    "        cell_dict[cell] = torch.Tensor(np.concatenate(concatenated_data))\n",
    "\n",
    "# GDSC\n",
    "GDSC1 = pd.read_csv(\"data/GDSC1.csv\", index_col=0)\n",
    "# default, remove data where lines or drugs are missing:\n",
    "data = GDSC1.query(\"SANGER_MODEL_ID in @data_concat.index & DRUG_ID in @drug_dict.keys()\")\n",
    "unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "\n",
    "np.random.seed(420) # for comparibility, don't change it!\n",
    "np.random.shuffle(unique_cell_lines)\n",
    "folds = np.array_split(unique_cell_lines, 10)\n",
    "test_lines = folds[0] \n",
    "train_idxs = list(range(10))\n",
    "train_idxs.remove(n_fold)\n",
    "np.random.seed(420)\n",
    "validation_idx = np.random.choice(train_idxs)\n",
    "train_idxs.remove(validation_idx)\n",
    "train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "validation_lines = folds[validation_idx]\n",
    "test_lines = folds[n_fold] \n",
    "\n",
    "    # no change needed, query works fine with some missing\n",
    "train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "\n",
    "if transform_into_corr:\n",
    "    # train, val, test among filtered data\n",
    "    # these are valid train_, val_ and test_data index\n",
    "    \n",
    "    \n",
    "    n_train = len(train_lines)  \n",
    "    n_val = len(validation_lines)      \n",
    "    n_test = len(test_lines)\n",
    "    \n",
    "    # Precompute similarity matrices for each data type\n",
    "    similarity_matrices = {}\n",
    "    \n",
    "    if \"rnaseq\" in typ:\n",
    "        exp_com = np.corrcoef(np.vstack([filtered_rna.loc[train_lines], \n",
    "                                         filtered_rna.loc[validation_lines], \n",
    "                                         filtered_rna.loc[test_lines]]), rowvar=True)\n",
    "        train = exp_com[:n_train, :n_train]\n",
    "        val = exp_com[n_train:n_train+n_val, :n_train]\n",
    "        test = exp_com[n_train+n_val:, :n_train]\n",
    "        sim_combined = np.vstack([train, val, test])\n",
    "        similarity_matrices[\"rnaseq\"] = sim_combined\n",
    "    \n",
    "    if \"proteomics\" in typ:\n",
    "        prot_com = np.corrcoef(np.vstack([proteomics.loc[train_lines], \n",
    "                                          proteomics.loc[validation_lines], \n",
    "                                          proteomics.loc[test_lines]]), rowvar=True)\n",
    "        train = prot_com[:n_train, :n_train]\n",
    "        val = prot_com[n_train:n_train+n_val, :n_train]\n",
    "        test = prot_com[n_train+n_val:, :n_train]\n",
    "        sim_combined = np.vstack([train, val, test])\n",
    "        similarity_matrices[\"proteomics\"] = sim_combined\n",
    "    \n",
    "    if \"mutations\" in typ:\n",
    "        train_snp = filtered_mut.loc[train_lines].astype(bool)\n",
    "        val_snp = filtered_mut.loc[validation_lines].astype(bool)\n",
    "        test_snp = filtered_mut.loc[test_lines].astype(bool)\n",
    "        \n",
    "        train = 1 - pairwise_distances(train_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "        val = 1 - pairwise_distances(val_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "        test = 1 - pairwise_distances(test_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "\n",
    "        sim_combined = np.vstack([train, val, test])\n",
    "        similarity_matrices[\"mutations\"] = sim_combined\n",
    "    \n",
    "    if \"methylations\" in typ:\n",
    "        methyl_com = np.nan_to_num(np.corrcoef(np.vstack([methylations.loc[train_lines], \n",
    "                                            methylations.loc[validation_lines], \n",
    "                                            methylations.loc[test_lines]]), rowvar=True))\n",
    "        train = methyl_com[:n_train, :n_train]\n",
    "        val = methyl_com[n_train:n_train+n_val, :n_train]\n",
    "        test = methyl_com[n_train+n_val:, :n_train]\n",
    "        sim_combined = np.vstack([train, val, test])\n",
    "        similarity_matrices[\"methylations\"] = sim_combined\n",
    "    \n",
    "    if \"cnvs\" in typ:\n",
    "        cnv_com = np.nan_to_num(np.corrcoef(np.vstack([cnvs.loc[train_lines], # nan-generation problem fixed \n",
    "                                         cnvs.loc[validation_lines], \n",
    "                                         cnvs.loc[test_lines]]), rowvar=True))\n",
    "        train= cnv_com[:n_train, :n_train]\n",
    "        val= cnv_com[n_train:n_train+n_val, :n_train]\n",
    "        test= cnv_com[n_train+n_val:, :n_train]\n",
    "        sim_combined = np.vstack([train, val, test])\n",
    "        similarity_matrices[\"cnvs\"] = sim_combined\n",
    "        \n",
    "    cell_dict = {}\n",
    "\n",
    "    # \n",
    "    for cell in unique_cell_lines:\n",
    "        cell_dict[cell] = {}\n",
    "        for data_type in typ:\n",
    "            sim_matrices = similarity_matrices[data_type]\n",
    "            sim_tensor = torch.Tensor(sim_matrices)\n",
    "            cell_idx = np.where(unique_cell_lines == cell)[0][0]\n",
    "            cell_dict[cell][data_type] = sim_tensor[cell_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58406d83-d4bc-47cd-b2a6-cd19c83ef59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "587"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similarity_matrices[\"methylations\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95a1cb45-d113-46c1-b2c6-06a7f121a547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(similarity_matrices[\"methylations\"]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db1df455-3151-4ee1-b50b-18954d72b09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "methylations.to_csv(\"met_nan_check.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41d75789-9496-43b0-aafe-633ff09c5e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6d7fe67-62d4-4d1b-8c75-58c0d17ad719",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(reconstructed,\"reconstructedMDA_data.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6f487be-09fd-4dc3-94e6-1c5870006c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(737, 3996)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "915aec61-af19-4b7d-8d7b-68d233dd6b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "587.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fine_sim_train_dataset[0][0])/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8ff52a1a-6a86-4a1f-8a16-4ab5084b165e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2940"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_combined.shape[0]*4 # 왜 2348이 나오지? 이 디멘션 문제를 해결해야한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d58f3051-6d9f-48b8-90a1-b99ec7645158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2348"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fine_sim_train_dataset[0][0]) # 정상적으로 4개의 sim mat이 concat되었다면 2940이 나와야한다. 이것부터 해결하고, 트레이닝 돌려보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b0e35b9-3098-45fd-84a8-1bb0d67f166d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d62282df-ddaf-4f55-9d9b-4b108c791ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(\"trained_models/pretrained_omics.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50714bb0-3882-458d-b7ac-f87fd69914ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight: torch.Size([700, 1940])\n",
      "0.bias: torch.Size([700])\n",
      "2.weight: torch.Size([485, 700])\n",
      "2.bias: torch.Size([485])\n"
     ]
    }
   ],
   "source": [
    "for key, value in weights.items():\n",
    "    print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e093ae1a-db5a-43f4-95be-c818e173906c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "777"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_rna.loc[cell].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7849ada7-a6c9-4a3e-a349-959954c0afa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1642"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(proteomics.loc[cell].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a374814c-6280-4413-9bd0-712359cbf502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_mut.loc[cell].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "522488e2-a1e3-46f8-bb1a-af6b37bdc5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(methylations.loc[cell].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f92615f7-aa3d-4757-9364-8061e1e77da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "587"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similarity_matrices[\"rnaseq\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bccf77-754a-4d19-befc-57ae2a8b0dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
