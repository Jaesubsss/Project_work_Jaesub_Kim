{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a3912e-5f0b-4454-af45-415bb4c1249c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 17 09:38:21 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-PCIE-16GB           On  | 00000000:21:00.0 Off |                    0 |\n",
      "| N/A   28C    P0              24W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE-16GB           On  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   28C    P0              23W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9067e-d08d-4136-8aed-730fcc049b05",
   "metadata": {},
   "source": [
    "# Package Loading & Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed4ebb8d-24c1-4ae0-86f8-28b17754179f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_388780/1495640561.py\", line 2, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/__init__.py\", line 26, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/compat/__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/compat/pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_388780/1495640561.py\", line 2, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/core/api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/core/dtypes/dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_388780/1495640561.py\", line 8, in <module>\n",
      "    import torchmetrics\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torchmetrics/__init__.py\", line 37, in <module>\n",
      "    from torchmetrics import functional  # noqa: E402\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torchmetrics/functional/__init__.py\", line 122, in <module>\n",
      "    from torchmetrics.functional.text._deprecated import _bleu_score as bleu_score\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torchmetrics/functional/text/__init__.py\", line 17, in <module>\n",
      "    from torchmetrics.functional.text.chrf import chrf_score\n",
      "  File \"/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torchmetrics/functional/text/chrf.py\", line 32, in <module>\n",
      "    _EPS_SMOOTHING = tensor(1e-16)\n",
      "/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torchmetrics/functional/text/chrf.py:32: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/torch/csrc/utils/tensor_numpy.cpp:77.)\n",
      "  _EPS_SMOOTHING = tensor(1e-16)\n"
     ]
    }
   ],
   "source": [
    "import MOICVAE.SNF as snf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "from torchmetrics import MeanSquaredError\n",
    "from sklearn.impute import KNNImputer\n",
    "import scripts\n",
    "from functools import lru_cache\n",
    "import optuna\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import warnings\n",
    "from torch.utils.data import Dataset\n",
    "# 경고 무시 설정\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de47a1d7-b286-494c-8332-9a9cf70630d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmicsDataset_dict(Dataset): \n",
    "    def __init__(self, omic_dict, drug_dict, data): \n",
    "        self.omic_dict = omic_dict\n",
    "        self.drug_dict = drug_dict\n",
    "        self.cell_mapped_ids = {key:i for i, key in enumerate(self.omic_dict.keys())}\n",
    "        # omic_dict의 키를 고유한 인덱스로 매핑\n",
    "        # enumerate는 키들을 순서대로 열거하여 (인덱스, 키) 형태의 튜플로 반환\n",
    "        # 딕셔너리 컴프레헨션: 각 키를 key로, 각 키의 인덱스를 i로 사용하여 {key:i}형태로 매핑된 딕셔너리 만듬.\n",
    "        self.drug_mapped_ids = {key:i for i, key in enumerate(self.drug_dict.keys())}\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx): # idx = train_data\n",
    "        instance = self.data.iloc[idx] \n",
    "        cell_id = instance.iloc[0]\n",
    "        drug_id = instance.iloc[1]\n",
    "        target = instance.iloc[2]\n",
    "        \n",
    "        #omics_data = { # usage of dictionary here causes a problem or crash with collate_fn function in Dataloader \n",
    "        #    cell_id : {\n",
    "        #        data_type: self.omic_dict[cell_id][data_type] for data_type in self.omic_dict[cell_id].keys()\n",
    "        #    }\n",
    "        #}\n",
    "        \n",
    "        return (torch.cat([self.omic_dict[cell_id][modality] for modality in self.omic_dict[cell_id].keys()]), \n",
    "                self.drug_dict[drug_id],\n",
    "                torch.Tensor([target]),\n",
    "                torch.Tensor([self.cell_mapped_ids[cell_id]]),\n",
    "                torch.Tensor([self.drug_mapped_ids[drug_id]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a70cb18-226e-46ee-bb31-67e48878158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, num_modalities = 1,  hidden_dim_encoders = 150, embed_dim = 75, fusion_dim = 150, dropout_encoders = 0.2):\n",
    "        # get input as a dictionary\n",
    "        super(MultimodalAutoencoder, self).__init__()\n",
    "        # EEEEEEEEEEncoder\n",
    "        self.input_dim = input_dim\n",
    "        self.num_modalities = num_modalities\n",
    "        self.do = nn.Dropout(dropout_encoders)\n",
    "\n",
    "        self.omics_encoder = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim_encoders), # input \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim_encoders, embed_dim) # encoder hidden layer: 150, 75 as the value from the paper. so we start from this \n",
    "            )                                 # I dont get why they used 150, 75 for dimension, but we can tune it later\n",
    "            for _ in range(num_modalities)\n",
    "        ])\n",
    "        # fused latent feature \n",
    "        self.fusion_layer = nn.Sequential( # I think we need a fusion layer here, to combine the data modalities\n",
    "            nn.Linear(embed_dim * num_modalities, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fusion_dim, embed_dim) # This concatenate latent features of all omics data, and fusion them and make its dim final latent dim\n",
    "        )                                     # This is the only way I can think of to fuse omics data\n",
    "        # decoder\n",
    "        self.omics_decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim_encoders),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_encoders, input_dim * num_modalities)\n",
    "        )\n",
    "        # I actually dont understand this step in paper. they said that decoder has symmetric structure as encoder,\n",
    "        # but the data after MDA they provided, has weird dimension(363x90) which makes no sense. this is the point that i cant understand\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_features = [] # get dictionary as an input \n",
    "        for i, encoder in enumerate(self.omics_encoder):\n",
    "            start_idx = i * self.input_dim\n",
    "            end_idx = start_idx + self.input_dim\n",
    "            x_modality = x[:, start_idx:end_idx]\n",
    "            latent_features.append(encoder(self.do(x_modality)))\n",
    "            \n",
    "        latent_fused = torch.cat(latent_features, dim=1)\n",
    "        latent_final = self.fusion_layer(latent_fused)\n",
    "        decoded = self.omics_decoder(latent_final)\n",
    "        return decoded, latent_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d787ac75-9a90-4c05-85be-a2e6dda20ad6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The Model that is modified as pedro said in eamil\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, embed_dim=256, hidden_dim=1024, dropout=0.1, n_layers=6, norm=\"layernorm\"):\n",
    "        super().__init__()\n",
    "        self.mlps = nn.ModuleList()\n",
    "        \n",
    "        # Determine normalization layer\n",
    "        if norm == \"layernorm\":\n",
    "            norm_layer = nn.LayerNorm\n",
    "        elif norm == \"batchnorm\":\n",
    "            norm_layer = nn.BatchNorm1d\n",
    "        else:\n",
    "            norm_layer = nn.Identity\n",
    "        \n",
    "        # Create MLP layers\n",
    "        for _ in range(n_layers):\n",
    "            self.mlps.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(embed_dim, hidden_dim),\n",
    "                    norm_layer(hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(hidden_dim, embed_dim)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.lin = nn.Linear(embed_dim, 1)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.mlps:\n",
    "            x = (layer(x) + x) / 2  # Residual connection\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "class Main_Model(nn.Module):\n",
    "    def __init__(self, embed_dim=256, hidden_dim=1024, dropout=0.1, n_layers=6, norm=\"layernorm\", \n",
    "                 dropout_omics=0.4, dropout_omics_finetuning=0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ResNet as backbone\n",
    "        self.resnet = ResNet(embed_dim, hidden_dim, dropout, n_layers, norm)\n",
    "        \n",
    "        # Modified embed_d: Two-layer MLP with dropout after ReLU\n",
    "        self.embed_d = nn.Sequential(\n",
    "            nn.LazyLinear(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Modified embed_c: Two-layer MLP with dropout after ReLU and before first Linear Layer\n",
    "        self.embed_c = nn.Sequential(\n",
    "            nn.Dropout(dropout_omics_finetuning),\n",
    "            nn.LazyLinear(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_omics),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, c, d):\n",
    "        # Combine embedded inputs and pass through ResNet\n",
    "        return self.resnet(self.embed_d(d) + self.embed_c(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9908faaa-375e-4e92-86a7-66cec1e1ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def get_data_corr(n_fold = 0, fp_radius = 2, transform_into_corr = True, typ = [\"rnaseq\", \"mutations\", \"cnvs\"]):\n",
    "    # drug\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    \n",
    "    # loading all datasets\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    \n",
    "    proteomics = pd.read_csv(\"data/proteomics.csv\", index_col=0)\n",
    "    \n",
    "    mutation = pd.read_csv(\"data/binary_mutations.csv\")\n",
    "    mutation.columns = mutation.iloc[0]\n",
    "    mutation = mutation.iloc[2:,:].set_index(\"gene_symbol\")\n",
    "    driver_columns = mutation.columns.isin(driver_genes)\n",
    "    filtered_mut = mutation.loc[:, driver_columns]\n",
    "    filtered_mut = filtered_mut.astype(float)\n",
    "\n",
    "    methylations = pd.read_csv(\"data/methylations.csv\",index_col = 0).sort_index(ascending = True)\n",
    "\n",
    "    cnvs = pd.read_csv(\"data/copy_number_variations.csv\",index_col= 0)\n",
    "\n",
    "    # concatenate all dataset \n",
    "    # inner join based on index: model_ids with NaN are automatically filtered out \n",
    "    data_concat = pd.concat([filtered_rna, proteomics, filtered_mut, methylations, cnvs], axis=1, join='inner')\n",
    "    \n",
    "    \n",
    "    # Filter data by common indices in all modalities\n",
    "    filtered_rna = filtered_rna[filtered_rna.index.isin(data_concat.index)]\n",
    "    proteomics = proteomics[proteomics.index.isin(data_concat.index)]\n",
    "    filtered_mut = filtered_mut[filtered_mut.index.isin(data_concat.index)]\n",
    "    methylations = methylations[methylations.index.isin(data_concat.index)]\n",
    "    cnvs = cnvs[cnvs.index.isin(data_concat.index)]\n",
    "    \n",
    "    # Initialize cell_dict\n",
    "    cell_dict = {}\n",
    "\n",
    "    if not transform_into_corr:\n",
    "        for cell in data_concat.index:\n",
    "            # Initialize a sub-dictionary for each cell\n",
    "            concatenated_data = []\n",
    "            \n",
    "            # Add data for each type specified in typ\n",
    "            if \"rnaseq\" in typ:\n",
    "                concatenated_data.append(filtered_rna.loc[cell].to_numpy())\n",
    "            if \"proteomics\" in typ:\n",
    "                concatenated_data.append(proteomics.loc[cell].to_numpy())\n",
    "            if \"mutations\" in typ:\n",
    "                concatenated_data.append(filtered_mut.loc[cell].to_numpy())\n",
    "            if \"methylations\" in typ:\n",
    "                concatenated_data.append(methylations.loc[cell].to_numpy())\n",
    "            if \"cnvs\" in typ:\n",
    "                concatenated_data.append(cnvs.loc[cell].to_numpy())\n",
    "\n",
    "            cell_dict[cell] = torch.Tensor(np.concatenate(concatenated_data))\n",
    "\n",
    "    # GDSC\n",
    "    GDSC1 = pd.read_csv(\"data/GDSC1.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = GDSC1.query(\"SANGER_MODEL_ID in @data_concat.index & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "\n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    test_lines = folds[0] \n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold] \n",
    "\n",
    "        # no change needed, query works fine with some missing\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "    \n",
    "    if transform_into_corr:\n",
    "        # train, val, test among filtered data\n",
    "        # these are valid train_, val_ and test_data index\n",
    "        \n",
    "        \n",
    "        n_train = len(train_lines)  \n",
    "        n_val = len(validation_lines)      \n",
    "        n_test = len(test_lines)\n",
    "        \n",
    "        # Precompute similarity matrices for each data type\n",
    "        similarity_matrices = {}\n",
    "        \n",
    "        if \"rnaseq\" in typ:\n",
    "            exp_com = np.corrcoef(np.vstack([filtered_rna.loc[train_lines], \n",
    "                                             filtered_rna.loc[validation_lines], \n",
    "                                             filtered_rna.loc[test_lines]]), rowvar=True)\n",
    "            train = exp_com[:n_train, :n_train]\n",
    "            val = exp_com[n_train:n_train+n_val, :n_train]\n",
    "            test = exp_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"rnaseq\"] = sim_combined\n",
    "        \n",
    "        if \"proteomics\" in typ:\n",
    "            prot_com = np.corrcoef(np.vstack([proteomics.loc[train_lines], \n",
    "                                              proteomics.loc[validation_lines], \n",
    "                                              proteomics.loc[test_lines]]), rowvar=True)\n",
    "            train = prot_com[:n_train, :n_train]\n",
    "            val = prot_com[n_train:n_train+n_val, :n_train]\n",
    "            test = prot_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"proteomics\"] = sim_combined\n",
    "        \n",
    "        if \"mutations\" in typ:\n",
    "            train_snp = filtered_mut.loc[train_lines].astype(bool)\n",
    "            val_snp = filtered_mut.loc[validation_lines].astype(bool)\n",
    "            test_snp = filtered_mut.loc[test_lines].astype(bool)\n",
    "            \n",
    "            train = 1 - pairwise_distances(train_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            val = 1 - pairwise_distances(val_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            test = 1 - pairwise_distances(test_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "    \n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"mutations\"] = sim_combined\n",
    "        \n",
    "        if \"methylations\" in typ:\n",
    "            methyl_com = np.corrcoef(np.vstack([methylations.loc[train_lines], \n",
    "                                                methylations.loc[validation_lines], \n",
    "                                                methylations.loc[test_lines]]), rowvar=True)\n",
    "            train = methyl_com[:n_train, :n_train]\n",
    "            val = methyl_com[n_train:n_train+n_val, :n_train]\n",
    "            test = methyl_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"mathylations\"] = sim_combined\n",
    "        \n",
    "        if \"cnvs\" in typ:\n",
    "            cnv_com = np.nan_to_num(np.corrcoef(np.vstack([cnvs.loc[train_lines], # nan-generation problem fixed \n",
    "                                             cnvs.loc[validation_lines], \n",
    "                                             cnvs.loc[test_lines]]), rowvar=True))\n",
    "            train= cnv_com[:n_train, :n_train]\n",
    "            val= cnv_com[n_train:n_train+n_val, :n_train]\n",
    "            test= cnv_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"cnvs\"] = sim_combined\n",
    "            \n",
    "        cell_dict = {}\n",
    "\n",
    "        # \n",
    "        for cell in unique_cell_lines:\n",
    "            cell_dict[cell] = {}\n",
    "            for data_type in typ:\n",
    "                sim_matrices = similarity_matrices[data_type]\n",
    "                sim_tensor = torch.Tensor(sim_matrices)\n",
    "                cell_idx = np.where(unique_cell_lines == cell)[0][0]\n",
    "                cell_dict[cell][data_type] = sim_tensor[cell_idx]\n",
    "    \n",
    "        return (OmicsDataset_dict(cell_dict, drug_dict, train_data),\n",
    "        OmicsDataset_dict(cell_dict, drug_dict, validation_data),\n",
    "        OmicsDataset_dict(cell_dict, drug_dict, test_data))\n",
    "\n",
    "    return (scripts.OmicsDataset(cell_dict, drug_dict, train_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, validation_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, test_data))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b48f9c5f-8d6d-45c2-82c4-783062baeb55",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:01:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:01:48] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# loading all datasets\u001b[39;00m\n\u001b[1;32m     13\u001b[0m driver_genes \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/driver_genes.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m---> 15\u001b[0m rnaseq \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/rnaseq_normcount.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m driver_columns \u001b[38;5;241m=\u001b[39m rnaseq\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39misin(driver_genes)\n\u001b[1;32m     17\u001b[0m filtered_rna \u001b[38;5;241m=\u001b[39m rnaseq\u001b[38;5;241m.\u001b[39mloc[:, driver_columns]\n",
      "File \u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.conda/envs/jupyter/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "# to use data \n",
    "n_fold = 0\n",
    "fp_radius = 2\n",
    "transform_into_corr = True\n",
    "typ = [\"rnaseq\", \"mutations\", \"cnvs\"]\n",
    "\n",
    "# drug\n",
    "smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "\n",
    "# loading all datasets\n",
    "driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "\n",
    "rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "\n",
    "proteomics = pd.read_csv(\"data/proteomics.csv\", index_col=0)\n",
    "\n",
    "mutation = pd.read_csv(\"data/binary_mutations.csv\")\n",
    "mutation.columns = mutation.iloc[0]\n",
    "mutation = mutation.iloc[2:,:].set_index(\"gene_symbol\")\n",
    "driver_columns = mutation.columns.isin(driver_genes)\n",
    "filtered_mut = mutation.loc[:, driver_columns]\n",
    "filtered_mut = filtered_mut.astype(float)\n",
    "\n",
    "methylations = pd.read_csv(\"data/methylations.csv\",index_col = 0).sort_index(ascending = True)\n",
    "\n",
    "cnvs = pd.read_csv(\"data/copy_number_variations.csv\",index_col= 0)\n",
    "\n",
    "# concatenate all dataset \n",
    "# inner join based on index: model_ids with NaN are automatically filtered out \n",
    "data_concat = pd.concat([filtered_rna, proteomics, filtered_mut, methylations, cnvs], axis=1, join='inner')\n",
    "\n",
    "\n",
    "# Filter data by common indices in all modalities\n",
    "filtered_rna = filtered_rna[filtered_rna.index.isin(data_concat.index)]\n",
    "proteomics = proteomics[proteomics.index.isin(data_concat.index)]\n",
    "filtered_mut = filtered_mut[filtered_mut.index.isin(data_concat.index)]\n",
    "methylations = methylations[methylations.index.isin(data_concat.index)]\n",
    "cnvs = cnvs[cnvs.index.isin(data_concat.index)]\n",
    "\n",
    "# Initialize cell_dict\n",
    "cell_dict = {}\n",
    "\n",
    "if not transform_into_corr:\n",
    "    for cell in data_concat.index:\n",
    "        # Initialize a sub-dictionary for each cell\n",
    "        concatenated_data = []\n",
    "        \n",
    "        # Add data for each type specified in typ\n",
    "        if \"rnaseq\" in typ:\n",
    "            concatenated_data.append(filtered_rna.loc[cell].to_numpy())\n",
    "        if \"proteomics\" in typ:\n",
    "            concatenated_data.append(proteomics.loc[cell].to_numpy())\n",
    "        if \"mutations\" in typ:\n",
    "            concatenated_data.append(filtered_mut.loc[cell].to_numpy())\n",
    "        if \"methylations\" in typ:\n",
    "            concatenated_data.append(methylations.loc[cell].to_numpy())\n",
    "        if \"cnvs\" in typ:\n",
    "            concatenated_data.append(cnvs.loc[cell].to_numpy())\n",
    "\n",
    "# GDSC\n",
    "GDSC1 = pd.read_csv(\"data/GDSC1.csv\", index_col=0)\n",
    "# default, remove data where lines or drugs are missing:\n",
    "data = GDSC1.query(\"SANGER_MODEL_ID in @data_concat.index & DRUG_ID in @drug_dict.keys()\")\n",
    "unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "\n",
    "np.random.seed(420) # for comparibility, don't change it!\n",
    "np.random.shuffle(unique_cell_lines)\n",
    "folds = np.array_split(unique_cell_lines, 10)\n",
    "test_lines = folds[0] \n",
    "train_idxs = list(range(10))\n",
    "train_idxs.remove(n_fold)\n",
    "np.random.seed(420)\n",
    "validation_idx = np.random.choice(train_idxs)\n",
    "train_idxs.remove(validation_idx)\n",
    "train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "validation_lines = folds[validation_idx]\n",
    "test_lines = folds[n_fold] \n",
    "\n",
    "    # no change needed, query works fine with some missing\n",
    "train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "511c410d-c320-4bac-8154-12c1ca9c68d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform_into_corr = False\n",
    "if transform_into_corr:\n",
    "    # train, val, test among filtered data\n",
    "    # these are valid train_, val_ and test_data index\n",
    "    \n",
    "    \n",
    "    n_train = len(train_lines)  \n",
    "    n_val = len(validation_lines)      \n",
    "    n_test = len(test_lines)\n",
    "    \n",
    "    # Precompute similarity matrices for each data type\n",
    "    similarity_matrices = {}\n",
    "    \n",
    "    if \"rnaseq\" in typ:\n",
    "        exp_com = np.corrcoef(np.vstack([filtered_rna.loc[train_lines], \n",
    "                                         filtered_rna.loc[validation_lines], \n",
    "                                         filtered_rna.loc[test_lines]]), rowvar=True)\n",
    "        train = exp_com[:n_train, :n_train]\n",
    "        val = exp_com[n_train:n_train+n_val, :n_train]\n",
    "        test = exp_com[n_train+n_val:, :n_train]\n",
    "        sim_combined = np.vstack([train, val, test])\n",
    "        similarity_matrices[\"rnaseq\"] = sim_combined\n",
    "    \n",
    "    if \"proteomics\" in typ:\n",
    "        prot_com = np.corrcoef(np.vstack([proteomics.loc[train_lines], \n",
    "                                          proteomics.loc[validation_lines], \n",
    "                                          proteomics.loc[test_lines]]), rowvar=True)\n",
    "        train = prot_com[:n_train, :n_train]\n",
    "        val = prot_com[n_train:n_train+n_val, :n_train]\n",
    "        test = prot_com[n_train+n_val:, :n_train]\n",
    "        sim_combined = np.vstack([train, val, test])\n",
    "        similarity_matrices[\"proteomics\"] = sim_combined\n",
    "    \n",
    "    if \"mutations\" in typ:\n",
    "        train_snp = filtered_mut.loc[train_lines].astype(bool)\n",
    "        val_snp = filtered_mut.loc[validation_lines].astype(bool)\n",
    "        test_snp = filtered_mut.loc[test_lines].astype(bool)\n",
    "        \n",
    "        train = 1 - pairwise_distances(train_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "        val = 1 - pairwise_distances(val_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "        test = 1 - pairwise_distances(test_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "\n",
    "        sim_combined = np.vstack([train, val, test])\n",
    "        similarity_matrices[\"mutations\"] = sim_combined\n",
    "    \n",
    "    if \"methylations\" in typ:\n",
    "        methyl_com = np.corrcoef(np.vstack([methylations.loc[train_lines], \n",
    "                                            methylations.loc[validation_lines], \n",
    "                                            methylations.loc[test_lines]]), rowvar=True)\n",
    "        train = methyl_com[:n_train, :n_train]\n",
    "        val = methyl_com[n_train:n_train+n_val, :n_train]\n",
    "        test = methyl_com[n_train+n_val:, :n_train]\n",
    "        sim_combined = np.vstack([train, val, test])\n",
    "        similarity_matrices[\"mathylations\"] = sim_combined\n",
    "    \n",
    "    if \"cnvs\" in typ:\n",
    "        cnv_com = np.nan_to_num(np.corrcoef(np.vstack([cnvs.loc[train_lines], # nan-generation problem fixed \n",
    "                                         cnvs.loc[validation_lines], \n",
    "                                         cnvs.loc[test_lines]]), rowvar=True))\n",
    "        train= cnv_com[:n_train, :n_train]\n",
    "        val= cnv_com[n_train:n_train+n_val, :n_train]\n",
    "        test= cnv_com[n_train+n_val:, :n_train]\n",
    "        sim_combined = np.vstack([train, val, test])\n",
    "        similarity_matrices[\"cnvs\"] = sim_combined\n",
    "        \n",
    "    cell_dict = {}\n",
    "\n",
    "    # \n",
    "    for cell in unique_cell_lines:\n",
    "            cell_dict[cell] = {}\n",
    "            for data_type in typ:\n",
    "                sim_matrices = similarity_matrices[data_type]\n",
    "                sim_tensor = torch.Tensor(sim_matrices)\n",
    "                cell_idx = np.where(unique_cell_lines == cell)[0][0]\n",
    "                cell_dict[cell][data_type] = sim_tensor[cell_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8596e9e-cdf5-43e1-87c5-4053a6f3364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"features\" : {\"fp_radius\":2,\n",
    "                        \"use_correlation_representation\": True,\n",
    "                        \"num_modalities\": 3},\n",
    "          \"optimizer\": {\"batch_size\": 220,\n",
    "                        \"clip_norm\":19,\n",
    "                        \"learning_rate\": 0.0004592646200179472,\n",
    "                        \"stopping_patience\":15,\n",
    "                        \"pre_batch_size\": 200,\n",
    "                        \"lr_pretraining\": 0.0004592646200179472},\n",
    "          \"model\":{\"embed_dim\":485, # shared\n",
    "                 \"hidden_dim\":696, \n",
    "                 \"dropout\":0.48541242824674574, \n",
    "                 \"n_layers\": 4, \n",
    "                 \"norm\": \"batchnorm\", \n",
    "                 \"hidden_dim_encoders\": 696, # ENCODER\n",
    "                 \"fusion_dim\": 700, # ENCODER\n",
    "                 \"dropout_encoders\": 0.2,\n",
    "                 \"dropout_omics\": 0.4, # second\n",
    "                 \"dropout_omics_finetuning\": 0.4, # first\n",
    "                 \"pre_training_epochs\": 100}, \n",
    "         \"env\": {\"fold\": 0,  \n",
    "                \"device\":\"cuda:0\", \n",
    "                 \"max_epochs\": 100, \n",
    "                 \"search_hyperparameters\":False}} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c33f7-e4f1-4b95-a40a-c56e7f3ed775",
   "metadata": {},
   "source": [
    "# Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "626095e5-8277-4a15-814f-a3557eec4c4f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[09:39:33] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "pre_train_dataset, pre_val_dataset, pre_test_dataset = get_data_corr(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"], \n",
    "                                                           transform_into_corr = config[\"features\"][\"use_correlation_representation\"],\n",
    "                                                           typ = (\"rnaseq\", \"mutations\", \"cnvs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62796ca2-b096-4083-92a5-c66178b13b1a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.00216\n",
      "Epoch 2/100, Loss: 0.00053\n",
      "Epoch 3/100, Loss: 0.00029\n",
      "Epoch 4/100, Loss: 0.00018\n",
      "Epoch 5/100, Loss: 0.00014\n",
      "Epoch 6/100, Loss: 0.00011\n",
      "Epoch 7/100, Loss: 0.00009\n",
      "Epoch 8/100, Loss: 0.00007\n",
      "Epoch 9/100, Loss: 0.00007\n",
      "Epoch 10/100, Loss: 0.00006\n",
      "Epoch 11/100, Loss: 0.00005\n",
      "Epoch 12/100, Loss: 0.00005\n",
      "Epoch 13/100, Loss: 0.00004\n",
      "Epoch 14/100, Loss: 0.00004\n",
      "Epoch 15/100, Loss: 0.00004\n",
      "Epoch 16/100, Loss: 0.00004\n",
      "Epoch 17/100, Loss: 0.00003\n",
      "Epoch 18/100, Loss: 0.00003\n",
      "Epoch 19/100, Loss: 0.00003\n",
      "Epoch 20/100, Loss: 0.00003\n",
      "Epoch 21/100, Loss: 0.00003\n",
      "Epoch 22/100, Loss: 0.00003\n",
      "Epoch 23/100, Loss: 0.00003\n",
      "Epoch 24/100, Loss: 0.00003\n",
      "Epoch 25/100, Loss: 0.00002\n",
      "Epoch 26/100, Loss: 0.00002\n",
      "Epoch 27/100, Loss: 0.00002\n",
      "Epoch 28/100, Loss: 0.00002\n",
      "Epoch 29/100, Loss: 0.00002\n",
      "Epoch 30/100, Loss: 0.00002\n",
      "Epoch 31/100, Loss: 0.00002\n",
      "Epoch 32/100, Loss: 0.00002\n",
      "Epoch 33/100, Loss: 0.00002\n",
      "Epoch 34/100, Loss: 0.00002\n",
      "Epoch 35/100, Loss: 0.00002\n",
      "Epoch 36/100, Loss: 0.00002\n",
      "Epoch 37/100, Loss: 0.00002\n",
      "Epoch 38/100, Loss: 0.00002\n",
      "Epoch 39/100, Loss: 0.00002\n",
      "Epoch 40/100, Loss: 0.00001\n",
      "Epoch 41/100, Loss: 0.00001\n",
      "Epoch 42/100, Loss: 0.00001\n",
      "Epoch 43/100, Loss: 0.00001\n",
      "Epoch 44/100, Loss: 0.00001\n",
      "Epoch 45/100, Loss: 0.00001\n",
      "Epoch 46/100, Loss: 0.00001\n",
      "Epoch 47/100, Loss: 0.00001\n",
      "Epoch 48/100, Loss: 0.00001\n",
      "Epoch 49/100, Loss: 0.00001\n",
      "Epoch 50/100, Loss: 0.00001\n",
      "Epoch 51/100, Loss: 0.00001\n",
      "Epoch 52/100, Loss: 0.00001\n",
      "Epoch 53/100, Loss: 0.00001\n",
      "Epoch 54/100, Loss: 0.00001\n",
      "Epoch 55/100, Loss: 0.00001\n",
      "Epoch 56/100, Loss: 0.00001\n",
      "Epoch 57/100, Loss: 0.00001\n",
      "Epoch 58/100, Loss: 0.00001\n",
      "Epoch 59/100, Loss: 0.00001\n",
      "Epoch 60/100, Loss: 0.00001\n",
      "Epoch 61/100, Loss: 0.00001\n",
      "Epoch 62/100, Loss: 0.00001\n",
      "Epoch 63/100, Loss: 0.00001\n",
      "Epoch 64/100, Loss: 0.00001\n",
      "Epoch 65/100, Loss: 0.00001\n",
      "Epoch 66/100, Loss: 0.00001\n",
      "Epoch 67/100, Loss: 0.00001\n",
      "Epoch 68/100, Loss: 0.00001\n",
      "Epoch 69/100, Loss: 0.00001\n",
      "Epoch 70/100, Loss: 0.00001\n",
      "Epoch 71/100, Loss: 0.00001\n",
      "Epoch 72/100, Loss: 0.00001\n",
      "Epoch 73/100, Loss: 0.00001\n",
      "Epoch 74/100, Loss: 0.00001\n",
      "Epoch 75/100, Loss: 0.00001\n",
      "Epoch 76/100, Loss: 0.00001\n",
      "Epoch 77/100, Loss: 0.00001\n",
      "Epoch 78/100, Loss: 0.00001\n",
      "Epoch 79/100, Loss: 0.00001\n",
      "Epoch 80/100, Loss: 0.00001\n",
      "Epoch 81/100, Loss: 0.00001\n",
      "Epoch 82/100, Loss: 0.00001\n",
      "Epoch 83/100, Loss: 0.00001\n",
      "Epoch 84/100, Loss: 0.00001\n",
      "Epoch 85/100, Loss: 0.00001\n",
      "Epoch 86/100, Loss: 0.00001\n",
      "Epoch 87/100, Loss: 0.00001\n",
      "Epoch 88/100, Loss: 0.00001\n",
      "Epoch 89/100, Loss: 0.00001\n",
      "Epoch 90/100, Loss: 0.00001\n",
      "Epoch 91/100, Loss: 0.00001\n",
      "Epoch 92/100, Loss: 0.00001\n",
      "Epoch 93/100, Loss: 0.00001\n",
      "Epoch 94/100, Loss: 0.00001\n",
      "Epoch 95/100, Loss: 0.00001\n",
      "Epoch 96/100, Loss: 0.00001\n",
      "Epoch 97/100, Loss: 0.00001\n",
      "Epoch 98/100, Loss: 0.00001\n",
      "Epoch 99/100, Loss: 0.00001\n",
      "Epoch 100/100, Loss: 0.00001\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "pre_train_loader = torch.utils.data.DataLoader(\n",
    "        pre_train_dataset,\n",
    "        batch_size = config[\"optimizer\"][\"pre_batch_size\"],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "#if validation_dataset is not None:\n",
    "#    val_loader = torch.utils.data.DataLoader(val_dataset,batch_size = batch_size,shuffle=True,drop_last=True)\n",
    "    \n",
    "sample_batch = next(iter(train_loader))\n",
    "sample_omics_data = sample_batch[0]  # 첫 번째 요소는 omics_data (dict)\n",
    "mod_len = sample_omics_data.shape[1] // config[\"features\"][\"num_modalities\"]\n",
    "#input_dict = {f\"modality_{i+1}\": mod_len for i in range(num_modality)}\n",
    "\n",
    "pre_model = MultimodalAutoencoder(input_dim = mod_len, \n",
    "                              hidden_dim_encoders = config[\"model\"][\"hidden_dim_encoders\"],\n",
    "                              embed_dim = config[\"model\"][\"embed_dim\"],\n",
    "                              fusion_dim = config[\"model\"][\"fusion_dim\"],\n",
    "                              num_modalities = config[\"features\"][\"num_modalities\"], \n",
    "                              dropout_encoders = config[\"model\"][\"dropout_encoders\"])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"optimizer\"][\"lr_pretraining\"])\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pre_model.to(device)\n",
    "\n",
    "# model training\n",
    "for epoch in range(config[\"model\"][\"pre_training_epochs\"]):\n",
    "    total_loss = []\n",
    "    pre_model.train()\n",
    "\n",
    "\n",
    "#    if validation_dataset is not None:\n",
    "#        validation_metrics = evaluate_step(model,val_loader, metrics, device)\n",
    "#        if epoch > 0 & use_momentum:\n",
    "#            val_target = 0.2*val_target + 0.8*validation_metrics['R_cellwise_residuals']\n",
    "#        else:\n",
    "#            val_target = validation_metrics['R_cellwise_residuals']\n",
    "\n",
    "    \n",
    "    # train step\n",
    "    for batch in pre_train_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "\n",
    "        optimizer.zero_grad() # gradient initialization\n",
    "        reconstructed, latent = pre_model(inputs)\n",
    "\n",
    "        #target_data = torch.cat([inputs[modality] for modality in inputs], dim=1)\n",
    "        # I think, here inappropriate loss is used. \n",
    "        # From what I know, Autoencoder reconstructs input feature, and calculate loss through comparing input feature and reconstructed feature.\n",
    "        # So, calculating MSE comparing with target_data, which is drug sensitivity, is nonsense. \n",
    "        #loss = criterion(reconstructed, target_data)\n",
    "        \n",
    "        # here is a new loss function, but it still outputs nan value as a loss...\n",
    "        loss = criterion(reconstructed, inputs)\n",
    "        \n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config[\"optimizer\"][\"clip_norm\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "    train_loss = np.mean(total_loss)\n",
    "    scheduler.step(train_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{config[\"model\"][\"pre_training_epochs\"]}, Loss: {train_loss:.5f}')\n",
    "\n",
    "torch.save(pre_model.fusion_layer.state_dict(), \"trained_models/pretrained_omics.pth\") # save pre-trained weights from fusion layer\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "158e976d-fba3-4450-a6fb-70288a921d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MSE of Model: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# evaluation MDA\n",
    "\n",
    "pre_eval_loader = torch.utils.data.DataLoader(\n",
    "        pre_test_dataset,\n",
    "        batch_size = config[\"optimizer\"][\"pre_batch_size\"],\n",
    "        shuffle=False, # we dont need this in evaluation step => repeatable result, for comparision\n",
    "        drop_last=False # to use all data\n",
    "    )\n",
    "\n",
    "pre_model.to(device)\n",
    "pre_model.eval()\n",
    "\n",
    "eval_losses = []\n",
    "\n",
    "for x in pre_eval_loader: \n",
    "    with torch.no_grad(): # no autograd => in evaluation step we dont need to update weights\n",
    "        inputs = batch[0].to(device)\n",
    "        reconstructed, _ = pre_model(inputs)\n",
    "        evel_loss = criterion(reconstructed,inputs)\n",
    "        eval_losses.append(evel_loss.item())\n",
    "\n",
    "final_eval_loss = np.mean(eval_losses)\n",
    "print(f\"Final MSE of Model: {final_eval_loss:.5f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a95f1-d6ee-4791-9a99-ef186f8852e0",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99a313f5-7380-4163-aca1-cea2ce314e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 계획에 따라, embed_c가 저 latent feature를 받아야함\n",
    "\n",
    "def train_step(model, optimizer, loader, config, device):\n",
    "    loss = nn.MSELoss()\n",
    "    ls = []\n",
    "    model.train()\n",
    "    for x in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x[0].to(device), x[1].to(device))\n",
    "        l = loss(out.squeeze(), x[2].to(device).squeeze())\n",
    "        l.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"optimizer\"][\"clip_norm\"])\n",
    "        ls += [l.item()]\n",
    "        optimizer.step()\n",
    "    return np.mean(ls)\n",
    "\n",
    "def finetune_model_train(config, train_dataset, validation_dataset=None, use_momentum=True, callback_epoch = None):\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                           drop_last=True,\n",
    "                                          shuffle=True)\n",
    "    if validation_dataset is not None:\n",
    "        val_loader = torch.utils.data.DataLoader(validation_dataset,\n",
    "                                               batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                               drop_last=False,\n",
    "                                              shuffle=False)\n",
    "        \n",
    "    model = Main_Model(embed_dim=config[\"model\"][\"embed_dim\"],\n",
    "                       hidden_dim=config[\"model\"][\"hidden_dim\"], \n",
    "                       dropout=config[\"model\"][\"dropout\"], \n",
    "                       n_layers=config[\"model\"][\"n_layers\"],  \n",
    "                       dropout_omics=config[\"model\"][\"dropout_omics\"], \n",
    "                       dropout_omics_finetuning=config[\"model\"][\"dropout_omics_finetuning\"],\n",
    "                       norm=config[\"model\"][\"norm\"])\n",
    "    \n",
    "    model.embed_c.load_state_dict(torch.load(\"trained_models/pretrained_omics.pth\"), strict=False) # pre-trained weights\n",
    "    optimizer = torch.optim.Adam(model.parameters(), config[\"optimizer\"][\"learning_rate\"])\n",
    "    device = torch.device(config[\"env\"][\"device\"])\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)\n",
    "    early_stop = scripts.EarlyStop(config[\"optimizer\"][\"stopping_patience\"])\n",
    "    model.to(device)\n",
    "    metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(\n",
    "    {\"R_cellwise_residuals\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"drugs\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=True),\n",
    "    \"R_cellwise\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"cell_lines\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=False),\n",
    "    \"MSE\":torchmetrics.MeanSquaredError()}))\n",
    "    metrics.to(device)\n",
    "    use_momentum = True    \n",
    "    for epoch in range(config[\"env\"][\"max_epochs\"]):\n",
    "        train_loss = train_step(model, optimizer, train_loader, config, device)\n",
    "        lr_scheduler.step(train_loss)\n",
    "        if validation_dataset is not None:\n",
    "            validation_metrics = evaluate_step(model,val_loader, metrics, device)\n",
    "            if epoch > 0 & use_momentum:\n",
    "                val_target = 0.2*val_target + 0.8*validation_metrics['R_cellwise_residuals']\n",
    "            else:\n",
    "                val_target = validation_metrics['R_cellwise_residuals']\n",
    "        else:\n",
    "            val_target = None\n",
    "        if callback_epoch is None:\n",
    "            print(f\"epoch : {epoch}: train loss: {train_loss} Smoothed R interaction (validation) {val_target}\")\n",
    "        else:\n",
    "            callback_epoch(epoch, val_target)\n",
    "        if early_stop(train_loss):\n",
    "            break\n",
    "    return val_target, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12fa84fe-34b7-4802-a511-45a23b4c89de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:05] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n",
      "[12:04:06] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "fine_train_dataset, fine_val_dataset, fine_test_dataset = get_data_corr(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"], \n",
    "                                                           transform_into_corr = False,\n",
    "                                                           typ = (\"rnaseq\", \"mutations\", \"cnvs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "18a07a55-ed81-4f02-843f-8d8a6c037c6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: FutureWarning: The default value for `maximize` will be changed from `True` to `None` in v1.7.0 of TorchMetrics,will automatically infer the value based on the `higher_is_better` attribute of the metric (if such attribute exists) or raise an error if it does not. If you are explicitly setting the `maximize` argument to either `True` or `False` already, you can ignore this warning.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0: train loss: 2.101567306830945 Smoothed R interaction (validation) None\n",
      "epoch : 1: train loss: 1.6592423534216485 Smoothed R interaction (validation) None\n",
      "epoch : 2: train loss: 1.5798197993536667 Smoothed R interaction (validation) None\n",
      "epoch : 3: train loss: 1.5325949221517896 Smoothed R interaction (validation) None\n",
      "epoch : 4: train loss: 1.5012005019394223 Smoothed R interaction (validation) None\n",
      "epoch : 5: train loss: 1.4624819291389475 Smoothed R interaction (validation) None\n",
      "epoch : 6: train loss: 1.4385447586127942 Smoothed R interaction (validation) None\n",
      "epoch : 7: train loss: 1.4035751995698482 Smoothed R interaction (validation) None\n",
      "epoch : 8: train loss: 1.3859120936712022 Smoothed R interaction (validation) None\n",
      "epoch : 9: train loss: 1.3628191977407789 Smoothed R interaction (validation) None\n",
      "epoch : 10: train loss: 1.3396166744279332 Smoothed R interaction (validation) None\n",
      "epoch : 11: train loss: 1.3202774819396483 Smoothed R interaction (validation) None\n",
      "epoch : 12: train loss: 1.3051924435847768 Smoothed R interaction (validation) None\n",
      "epoch : 13: train loss: 1.2919417061233993 Smoothed R interaction (validation) None\n",
      "epoch : 14: train loss: 1.2737873408791456 Smoothed R interaction (validation) None\n",
      "epoch : 15: train loss: 1.2569785094526407 Smoothed R interaction (validation) None\n",
      "epoch : 16: train loss: 1.2479746052154947 Smoothed R interaction (validation) None\n",
      "epoch : 17: train loss: 1.2321509997393794 Smoothed R interaction (validation) None\n",
      "epoch : 18: train loss: 1.2227604744313527 Smoothed R interaction (validation) None\n",
      "epoch : 19: train loss: 1.2125546594043302 Smoothed R interaction (validation) None\n",
      "epoch : 20: train loss: 1.196783455694267 Smoothed R interaction (validation) None\n",
      "epoch : 21: train loss: 1.1902752984292133 Smoothed R interaction (validation) None\n",
      "epoch : 22: train loss: 1.1833510818086537 Smoothed R interaction (validation) None\n",
      "epoch : 23: train loss: 1.166460287364658 Smoothed R interaction (validation) None\n",
      "epoch : 24: train loss: 1.1599806463173206 Smoothed R interaction (validation) None\n",
      "epoch : 25: train loss: 1.1511986878952667 Smoothed R interaction (validation) None\n",
      "epoch : 26: train loss: 1.141982836393255 Smoothed R interaction (validation) None\n",
      "epoch : 27: train loss: 1.1328196496102954 Smoothed R interaction (validation) None\n",
      "epoch : 28: train loss: 1.1241216322105512 Smoothed R interaction (validation) None\n",
      "epoch : 29: train loss: 1.1180281943384884 Smoothed R interaction (validation) None\n",
      "epoch : 30: train loss: 1.108152268769862 Smoothed R interaction (validation) None\n",
      "epoch : 31: train loss: 1.1005303198828538 Smoothed R interaction (validation) None\n",
      "epoch : 32: train loss: 1.092089364717269 Smoothed R interaction (validation) None\n",
      "epoch : 33: train loss: 1.0822618067338234 Smoothed R interaction (validation) None\n",
      "epoch : 34: train loss: 1.0795108236842161 Smoothed R interaction (validation) None\n",
      "epoch : 35: train loss: 1.0739634524759167 Smoothed R interaction (validation) None\n",
      "epoch : 36: train loss: 1.0661895212490566 Smoothed R interaction (validation) None\n",
      "epoch : 37: train loss: 1.0625805135710424 Smoothed R interaction (validation) None\n",
      "epoch : 38: train loss: 1.0586639801857645 Smoothed R interaction (validation) None\n",
      "epoch : 39: train loss: 1.0515243647567127 Smoothed R interaction (validation) None\n",
      "epoch : 40: train loss: 1.045653151198577 Smoothed R interaction (validation) None\n",
      "epoch : 41: train loss: 1.0416639514258235 Smoothed R interaction (validation) None\n",
      "epoch : 42: train loss: 1.036586594891047 Smoothed R interaction (validation) None\n",
      "epoch : 43: train loss: 1.0303219081592205 Smoothed R interaction (validation) None\n",
      "epoch : 44: train loss: 1.025089617049591 Smoothed R interaction (validation) None\n",
      "epoch : 45: train loss: 1.019092547731435 Smoothed R interaction (validation) None\n",
      "epoch : 46: train loss: 1.016030026896775 Smoothed R interaction (validation) None\n",
      "epoch : 47: train loss: 1.013219681820557 Smoothed R interaction (validation) None\n",
      "epoch : 48: train loss: 1.0071480826777492 Smoothed R interaction (validation) None\n",
      "epoch : 49: train loss: 1.002121893083504 Smoothed R interaction (validation) None\n",
      "epoch : 50: train loss: 0.9993257930016488 Smoothed R interaction (validation) None\n",
      "epoch : 51: train loss: 0.9915104492191037 Smoothed R interaction (validation) None\n",
      "epoch : 52: train loss: 0.9887293968418768 Smoothed R interaction (validation) None\n",
      "epoch : 53: train loss: 0.9864802075404908 Smoothed R interaction (validation) None\n",
      "epoch : 54: train loss: 0.9821421894508475 Smoothed R interaction (validation) None\n",
      "epoch : 55: train loss: 0.9785909882582015 Smoothed R interaction (validation) None\n",
      "epoch : 56: train loss: 0.9731441003135757 Smoothed R interaction (validation) None\n",
      "epoch : 57: train loss: 0.971471034404816 Smoothed R interaction (validation) None\n",
      "epoch : 58: train loss: 0.9619383357363961 Smoothed R interaction (validation) None\n",
      "epoch : 59: train loss: 0.9635347259796152 Smoothed R interaction (validation) None\n",
      "epoch : 60: train loss: 0.9582110544659741 Smoothed R interaction (validation) None\n",
      "epoch : 61: train loss: 0.9556243545634785 Smoothed R interaction (validation) None\n",
      "epoch : 62: train loss: 0.9538928946694278 Smoothed R interaction (validation) None\n",
      "epoch : 63: train loss: 0.9525537876911894 Smoothed R interaction (validation) None\n",
      "epoch : 64: train loss: 0.9435916355130699 Smoothed R interaction (validation) None\n",
      "epoch : 65: train loss: 0.9419437916523447 Smoothed R interaction (validation) None\n",
      "epoch : 66: train loss: 0.9389763924630522 Smoothed R interaction (validation) None\n",
      "epoch : 67: train loss: 0.93590301643638 Smoothed R interaction (validation) None\n",
      "epoch : 68: train loss: 0.9334912557979155 Smoothed R interaction (validation) None\n",
      "epoch : 69: train loss: 0.9344077704863436 Smoothed R interaction (validation) None\n",
      "epoch : 70: train loss: 0.9249185269194274 Smoothed R interaction (validation) None\n",
      "epoch : 71: train loss: 0.9215036997541657 Smoothed R interaction (validation) None\n",
      "epoch : 72: train loss: 0.9215126495131456 Smoothed R interaction (validation) None\n",
      "epoch : 73: train loss: 0.9194033906250566 Smoothed R interaction (validation) None\n",
      "epoch : 74: train loss: 0.9151947794502834 Smoothed R interaction (validation) None\n",
      "epoch : 75: train loss: 0.9087544835836837 Smoothed R interaction (validation) None\n",
      "epoch : 76: train loss: 0.9101491859286324 Smoothed R interaction (validation) None\n",
      "epoch : 77: train loss: 0.9076138975004331 Smoothed R interaction (validation) None\n",
      "epoch : 78: train loss: 0.903091866389488 Smoothed R interaction (validation) None\n",
      "epoch : 79: train loss: 0.9036602575345741 Smoothed R interaction (validation) None\n",
      "epoch : 80: train loss: 0.8985795933472359 Smoothed R interaction (validation) None\n",
      "epoch : 81: train loss: 0.8918337261278932 Smoothed R interaction (validation) None\n",
      "epoch : 82: train loss: 0.8896524695442398 Smoothed R interaction (validation) None\n",
      "epoch : 83: train loss: 0.8869162486274399 Smoothed R interaction (validation) None\n",
      "epoch : 84: train loss: 0.8835718664162239 Smoothed R interaction (validation) None\n",
      "epoch : 85: train loss: 0.8846596306275084 Smoothed R interaction (validation) None\n",
      "epoch : 86: train loss: 0.8812858849137616 Smoothed R interaction (validation) None\n",
      "epoch : 87: train loss: 0.8773234576465763 Smoothed R interaction (validation) None\n",
      "epoch : 88: train loss: 0.8759181078785871 Smoothed R interaction (validation) None\n",
      "epoch : 89: train loss: 0.8764639541009448 Smoothed R interaction (validation) None\n",
      "epoch : 90: train loss: 0.8674268475716429 Smoothed R interaction (validation) None\n",
      "epoch : 91: train loss: 0.8670623832933687 Smoothed R interaction (validation) None\n",
      "epoch : 92: train loss: 0.8647897858702208 Smoothed R interaction (validation) None\n",
      "epoch : 93: train loss: 0.8599265882347069 Smoothed R interaction (validation) None\n",
      "epoch : 94: train loss: 0.861486682638397 Smoothed R interaction (validation) None\n",
      "epoch : 95: train loss: 0.8583310468235181 Smoothed R interaction (validation) None\n",
      "epoch : 96: train loss: 0.8531878607529497 Smoothed R interaction (validation) None\n",
      "epoch : 97: train loss: 0.8507476347338578 Smoothed R interaction (validation) None\n",
      "epoch : 98: train loss: 0.8517176182514657 Smoothed R interaction (validation) None\n",
      "epoch : 99: train loss: 0.8493799539225357 Smoothed R interaction (validation) None\n"
     ]
    }
   ],
   "source": [
    "_, main_model = finetune_model_train(config, torch.utils.data.ConcatDataset([fine_train_dataset, fine_val_dataset]), None, use_momentum=False)\n",
    "device = torch.device(config[\"env\"][\"device\"])\n",
    "metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(\n",
    "    {\"R_cellwise_residuals\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"drugs\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=True),\n",
    "    \"R_cellwise\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"cell_lines\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=False),\n",
    "    \"MSE\":torchmetrics.MeanSquaredError()}))\n",
    "metrics.to(device)\n",
    "fine_test_dataloader = torch.utils.data.DataLoader(fine_test_dataset,\n",
    "                                       batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                       drop_last=False,\n",
    "                                      shuffle=False,\n",
    "                                      pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bc37a1cf-70ad-422b-972a-631ee8b7c070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/kim14/project_work/scripts/models.py:69: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/TensorShape.cpp:3277.)\n",
      "  return torch.linalg.solve(A, Xy).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to: results/pred_pretrained_NoOptHyper_sim_matrix_20250117_12:39:33_26ac88f3-9881-4cde-9d43-1aa1e1a82008.csv\n",
      "main model final metrics: {'MSE': 1.5677202939987183, 'R_cellwise': 0.8969780206680298, 'R_cellwise_residuals': 0.3421277403831482}\n"
     ]
    }
   ],
   "source": [
    "main_final_metrics_nohyper = scripts.evaluate_step(main_model, fine_test_dataloader, metrics, device, save_predictions = True, model_name = \"pretrained_NoOptHyper\", dataset_name = \"sim_matrix\")\n",
    "print(f\"main model final metrics: {main_final_metrics_nohyper}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e964f-d635-4f6b-9dec-9ef63155e174",
   "metadata": {},
   "source": [
    "huh it got best MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81623a07-4201-4a10-a9c3-3e44b49aefa6",
   "metadata": {},
   "source": [
    "# Narrow-sense Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e3ff08-b94a-4af7-aa22-0d51fbe41096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config for hyperparameter estimation\n",
    "\n",
    "config[\"model\"] = {\"embed_dim\":trial.suggest_int(\"embed_dim\", 64, 512),\n",
    "                 \"hidden_dim\":696, # hidden layer의 차원\n",
    "                 \"dropout\":0.48541242824674574, # 40퍼센트의 노드를 랜덤하게 드랍아웃 \n",
    "                 \"n_layers\": 4, # 3개의 hidden layer를 사용\n",
    "                 \"norm\": \"batchnorm\", # batch normalization을 사용하여 모델이 학습 중 출력 분포를 정규화하여 학습을 안정화\n",
    "                 \"hidden_dim_encoders\": trial.suggest_int(\"hidden_dim_encoders\", 64, 2048),\n",
    "                 \"fusion_dim\": trial.suggest_int(\"fusion_dim\", 64, 2048),\n",
    "                 \"dropout_encoders\": trial.suggest_float(\"dropout_omics\", 0.0, 0.5),\n",
    "                 \"dropout_omics\": trial.suggest_float(\"dropout_omics\", 0.0, 0.9), # what is this for?\n",
    "                 \"dropout_omics_finetuning\": trial.suggest_float(\"dropout_omics_finetuning\", 0.0, 0.9),\n",
    "                 \"pre_training_epochs\": trial.suggest_int(\"pre_training_epochs\", 1, 500)}\n",
    "\n",
    "config[\"optimizer\"] = {\"batch_size\": 220,\n",
    "                        \"clip_norm\":19,\n",
    "                        \"learning_rate\": 0.0004592646200179472,\n",
    "                        \"stopping_patience\":15,\n",
    "                        \"pre_batch_size\": trial.suggest_int(\"pre_batch_size\", 128, 512),\n",
    "                        \"lr_pretraining\": trial.suggest_float(\"lr_pretraining\", 1e-6, 1e-1, log=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9341b18-b33d-4a32-9b0a-32e6c5837d43",
   "metadata": {},
   "source": [
    "# Mülleimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81276835-348b-420d-aee3-dc30de9deaba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def get_data_corr_with_filtering(n_fold = 0, fp_radius = 2, transform_into_corr = True, typ = [\"rnaseq\", \"mutations\", \"cnvs\"]):\n",
    "    # drug\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    \n",
    "    # loading all datasets\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    \n",
    "    proteomics = pd.read_csv(\"data/proteomics.csv\", index_col=0)\n",
    "    \n",
    "    mutation = pd.read_csv(\"data/binary_mutations.csv\")\n",
    "    mutation.columns = mutation.iloc[0]\n",
    "    mutation = mutation.iloc[2:,:].set_index(\"gene_symbol\")\n",
    "    driver_columns = mutation.columns.isin(driver_genes)\n",
    "    filtered_mut = mutation.loc[:, driver_columns]\n",
    "    filtered_mut = filtered_mut.astype(float)\n",
    "\n",
    "    methylations = pd.read_csv(\"data/methylations.csv\",index_col = 0).sort_index(ascending = True)\n",
    "\n",
    "    cnvs = pd.read_csv(\"data/copy_number_variations.csv\",index_col= 0)\n",
    "\n",
    "    # concatenate all dataset \n",
    "    # inner join based on index: model_ids with NaN are automatically filtered out \n",
    "    data_concat = pd.concat([filtered_rna, proteomics, filtered_mut, methylations, cnvs], axis=1, join='inner')\n",
    "    \n",
    "    \n",
    "    # Filter data by common indices in all modalities\n",
    "    filtered_rna = filtered_rna[filtered_rna.index.isin(data_concat.index)]\n",
    "    proteomics = proteomics[proteomics.index.isin(data_concat.index)]\n",
    "    filtered_mut = filtered_mut[filtered_mut.index.isin(data_concat.index)]\n",
    "    methylations = methylations[methylations.index.isin(data_concat.index)]\n",
    "    cnvs = cnvs[cnvs.index.isin(data_concat.index)]\n",
    "    \n",
    "    # Initialize cell_dict\n",
    "    cell_dict = {}\n",
    "\n",
    "    if not transform_into_corr:\n",
    "        for cell in data_concat.index:\n",
    "            # Initialize a sub-dictionary for each cell\n",
    "            cell_dict[cell] = {}\n",
    "            \n",
    "            # Add data for each type specified in typ\n",
    "            if \"rnaseq\" in typ:\n",
    "                cell_dict[cell][\"rnaseq\"] = torch.Tensor(filtered_rna.loc[cell].to_numpy())\n",
    "            if \"proteomics\" in typ:\n",
    "                cell_dict[cell][\"proteomics\"] = torch.Tensor(proteomics.loc[cell].to_numpy())\n",
    "            if \"mutations\" in typ:\n",
    "                cell_dict[cell][\"mutations\"] = torch.Tensor(filtered_mut.loc[cell].to_numpy())\n",
    "            if \"methylations\" in typ:\n",
    "                cell_dict[cell][\"methylations\"] = torch.Tensor(methylations.loc[cell].to_numpy())\n",
    "            if \"cnvs\" in typ:\n",
    "                cell_dict[cell][\"cnvs\"] = torch.Tensor(cnvs.loc[cell].to_numpy())\n",
    "\n",
    "    # GDSC\n",
    "    GDSC1 = pd.read_csv(\"data/GDSC1.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = GDSC1.query(\"SANGER_MODEL_ID in @data_concat.index & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "\n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    test_lines = folds[0] \n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold] \n",
    "    \n",
    "    if transform_into_corr:\n",
    "        # ic50 filtering\n",
    "        ic50_mat = data.pivot(index = 'SANGER_MODEL_ID', columns = 'DRUG_ID', values = 'LN_IC50')\n",
    "        drug_nan_ratio = ic50_mat.isna().mean(axis=0) \n",
    "        cellline_nan_ratio = ic50_mat.isna().mean(axis=1)\n",
    "        filtered_ic50 = ic50_mat.loc[cellline_nan_ratio < 0.3, drug_nan_ratio < 0.3]\n",
    "        imputer = KNNImputer(n_neighbors=5)  # k-NN에서 k=5\n",
    "        imputed_ic50 = pd.DataFrame(\n",
    "            imputer.fit_transform(filtered_ic50),\n",
    "            index=filtered_ic50.index,\n",
    "            columns=filtered_ic50.columns)      \n",
    "        t = imputed_ic50.median()\n",
    "        binarized_ic50 = imputed_ic50.apply(lambda x: x.apply(lambda v: 1 if v <= t[x.name] else 0), axis=0)\n",
    "        \n",
    "        # index filtering, here only exp, mutation, cnv data are used\n",
    "        cell_line_index = binarized_ic50.index.intersection(data_concat.index)\n",
    "        ic50 = binarized_ic50.loc[cell_line_index]\n",
    "        \n",
    "        # train, val, test among filtered data\n",
    "        # these are valid train_, val_ and test_data index\n",
    "        train_lines = np.intersect1d(train_lines, ic50.index)\n",
    "        valid_validation_lines = np.intersect1d(validation_lines, ic50.index)\n",
    "        valid_test_lines = np.intersect1d(test_lines, ic50.index)\n",
    "        \n",
    "        n_train = len(train_lines)  \n",
    "        n_val = len(valid_validation_lines)      \n",
    "        n_test = len(valid_test_lines)\n",
    "        \n",
    "        # Precompute similarity matrices for each data type\n",
    "        similarity_matrices = {}\n",
    "        \n",
    "        if \"rnaseq\" in typ:\n",
    "            exp_com = np.corrcoef(np.vstack([filtered_rna.loc[train_lines], \n",
    "                                             filtered_rna.loc[valid_validation_lines], \n",
    "                                             filtered_rna.loc[valid_test_lines]]), rowvar=True)\n",
    "            train = exp_com[:n_train, :n_train]\n",
    "            val = exp_com[n_train:n_train+n_val, :n_train]\n",
    "            test = exp_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"rnaseq\"] = sim_combined\n",
    "        \n",
    "        if \"proteomics\" in typ:\n",
    "            prot_com = np.corrcoef(np.vstack([proteomics.loc[train_lines], \n",
    "                                              proteomics.loc[valid_validation_lines], \n",
    "                                              proteomics.loc[valid_test_lines]]), rowvar=True)\n",
    "            train = prot_com[:n_train, :n_train]\n",
    "            val = prot_com[n_train:n_train+n_val, :n_train]\n",
    "            test = prot_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"proteomics\"] = sim_combined\n",
    "        \n",
    "        if \"mutations\" in typ:\n",
    "            train_snp = filtered_mut.loc[train_lines].astype(bool)\n",
    "            val_snp = filtered_mut.loc[valid_validation_lines].astype(bool)\n",
    "            test_snp = filtered_mut.loc[valid_test_lines].astype(bool)\n",
    "            \n",
    "            train = 1 - pairwise_distances(train_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            val = 1 - pairwise_distances(val_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            test = 1 - pairwise_distances(test_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "    \n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"mutations\"] = sim_combined\n",
    "        \n",
    "        if \"methylations\" in typ:\n",
    "            methyl_com = np.corrcoef(np.vstack([methylations.loc[train_lines], \n",
    "                                                methylations.loc[valid_validation_lines], \n",
    "                                                methylations.loc[valid_test_lines]]), rowvar=True)\n",
    "            train = methyl_com[:n_train, :n_train]\n",
    "            val = methyl_com[n_train:n_train+n_val, :n_train]\n",
    "            test = methyl_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"mathylations\"] = sim_combined\n",
    "        \n",
    "        if \"cnvs\" in typ:\n",
    "            cnv_com = np.corrcoef(np.vstack([cnvs.loc[train_lines], \n",
    "                                             cnvs.loc[valid_validation_lines], \n",
    "                                             cnvs.loc[valid_test_lines]]), rowvar=True)\n",
    "            train= cnv_com[:n_train, :n_train]\n",
    "            val= cnv_com[n_train:n_train+n_val, :n_train]\n",
    "            test= cnv_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"cnvs\"] = sim_combined\n",
    "            \n",
    "        cell_dict = {}\n",
    "\n",
    "        # \n",
    "        for cell in cell_line_index:\n",
    "            cell_dict[cell] = {}\n",
    "            for data_type in typ:\n",
    "                sim_matrices = similarity_matrices[data_type]\n",
    "                sim_tensor = torch.Tensor(sim_matrices)\n",
    "                cell_idx = cell_line_index.get_loc(cell)\n",
    "                cell_dict[cell][data_type] = sim_tensor[cell_idx]\n",
    "                \n",
    "        train_lines = train_lines\n",
    "        validation_lines = valid_validation_lines\n",
    "        test_lines = valid_test_lines\n",
    "\n",
    "    # no change needed, query works fine with some missing\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "    \n",
    "    return (OmicsDataset_dict(cell_dict, drug_dict, train_data),\n",
    "    OmicsDataset_dict(cell_dict, drug_dict, validation_data),\n",
    "    OmicsDataset_dict(cell_dict, drug_dict, test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
