{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a3912e-5f0b-4454-af45-415bb4c1249c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 29 12:35:01 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-PCIE-16GB           On  | 00000000:21:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              23W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE-16GB           On  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              24W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed4ebb8d-24c1-4ae0-86f8-28b17754179f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import MOICVAE.SNF as snf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "from torchmetrics import MeanSquaredError\n",
    "from sklearn.impute import KNNImputer\n",
    "import scripts\n",
    "from functools import lru_cache\n",
    "import optuna\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import warnings\n",
    "from torch.utils.data import Dataset\n",
    "# 경고 무시 설정\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de47a1d7-b286-494c-8332-9a9cf70630d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmicsDataset_dict(Dataset): \n",
    "    def __init__(self, omic_dict, drug_dict, data): \n",
    "        self.omic_dict = omic_dict\n",
    "        self.drug_dict = drug_dict\n",
    "        self.cell_mapped_ids = {key:i for i, key in enumerate(self.omic_dict.keys())}\n",
    "        # omic_dict의 키를 고유한 인덱스로 매핑\n",
    "        # enumerate는 키들을 순서대로 열거하여 (인덱스, 키) 형태의 튜플로 반환\n",
    "        # 딕셔너리 컴프레헨션: 각 키를 key로, 각 키의 인덱스를 i로 사용하여 {key:i}형태로 매핑된 딕셔너리 만듬.\n",
    "        self.drug_mapped_ids = {key:i for i, key in enumerate(self.drug_dict.keys())}\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx): # idx = train_data\n",
    "        instance = self.data.iloc[idx] \n",
    "        cell_id = instance.iloc[0]\n",
    "        drug_id = instance.iloc[1]\n",
    "        target = instance.iloc[2]\n",
    "        \n",
    "        #omics_data = { # usage of dictionary here causes a problem or crash with collate_fn function in Dataloader \n",
    "        #    cell_id : {\n",
    "        #        data_type: self.omic_dict[cell_id][data_type] for data_type in self.omic_dict[cell_id].keys()\n",
    "        #    }\n",
    "        #}\n",
    "        \n",
    "        return (torch.cat([self.omic_dict[cell_id][modality] for modality in self.omic_dict[cell_id].keys()]), \n",
    "                self.drug_dict[drug_id],\n",
    "                torch.Tensor([target]),\n",
    "                torch.Tensor([self.cell_mapped_ids[cell_id]]),\n",
    "                torch.Tensor([self.drug_mapped_ids[drug_id]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81276835-348b-420d-aee3-dc30de9deaba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def get_data_corr_with_filtering(n_fold = 0, fp_radius = 2, transform_into_corr = True, typ = [\"rnaseq\", \"mutations\", \"cnvs\"]):\n",
    "    # drug\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    \n",
    "    # loading all datasets\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    \n",
    "    proteomics = pd.read_csv(\"data/proteomics.csv\", index_col=0)\n",
    "    \n",
    "    mutation = pd.read_csv(\"data/binary_mutations.csv\")\n",
    "    mutation.columns = mutation.iloc[0]\n",
    "    mutation = mutation.iloc[2:,:].set_index(\"gene_symbol\")\n",
    "    driver_columns = mutation.columns.isin(driver_genes)\n",
    "    filtered_mut = mutation.loc[:, driver_columns]\n",
    "    filtered_mut = filtered_mut.astype(float)\n",
    "\n",
    "    methylations = pd.read_csv(\"data/methylations.csv\",index_col = 0).sort_index(ascending = True)\n",
    "\n",
    "    cnvs = pd.read_csv(\"data/copy_number_variations.csv\",index_col= 0)\n",
    "\n",
    "    # concatenate all dataset \n",
    "    # inner join based on index: model_ids with NaN are automatically filtered out \n",
    "    data_concat = pd.concat([filtered_rna, proteomics, filtered_mut, methylations, cnvs], axis=1, join='inner')\n",
    "    \n",
    "    \n",
    "    # Filter data by common indices in all modalities\n",
    "    filtered_rna = filtered_rna[filtered_rna.index.isin(data_concat.index)]\n",
    "    proteomics = proteomics[proteomics.index.isin(data_concat.index)]\n",
    "    filtered_mut = filtered_mut[filtered_mut.index.isin(data_concat.index)]\n",
    "    methylations = methylations[methylations.index.isin(data_concat.index)]\n",
    "    cnvs = cnvs[cnvs.index.isin(data_concat.index)]\n",
    "    \n",
    "    # Initialize cell_dict\n",
    "    cell_dict = {}\n",
    "\n",
    "    if not transform_into_corr:\n",
    "        for cell in data_concat.index:\n",
    "            # Initialize a sub-dictionary for each cell\n",
    "            cell_dict[cell] = {}\n",
    "            \n",
    "            # Add data for each type specified in typ\n",
    "            if \"rnaseq\" in typ:\n",
    "                cell_dict[cell][\"rnaseq\"] = torch.Tensor(filtered_rna.loc[cell].to_numpy())\n",
    "            if \"proteomics\" in typ:\n",
    "                cell_dict[cell][\"proteomics\"] = torch.Tensor(proteomics.loc[cell].to_numpy())\n",
    "            if \"mutations\" in typ:\n",
    "                cell_dict[cell][\"mutations\"] = torch.Tensor(filtered_mut.loc[cell].to_numpy())\n",
    "            if \"methylations\" in typ:\n",
    "                cell_dict[cell][\"methylations\"] = torch.Tensor(methylations.loc[cell].to_numpy())\n",
    "            if \"cnvs\" in typ:\n",
    "                cell_dict[cell][\"cnvs\"] = torch.Tensor(cnvs.loc[cell].to_numpy())\n",
    "\n",
    "    # GDSC\n",
    "    GDSC1 = pd.read_csv(\"data/GDSC1.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = GDSC1.query(\"SANGER_MODEL_ID in @data_concat.index & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "\n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    test_lines = folds[0] \n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold] \n",
    "    \n",
    "    if transform_into_corr:\n",
    "        # ic50 filtering\n",
    "        ic50_mat = data.pivot(index = 'SANGER_MODEL_ID', columns = 'DRUG_ID', values = 'LN_IC50')\n",
    "        drug_nan_ratio = ic50_mat.isna().mean(axis=0) \n",
    "        cellline_nan_ratio = ic50_mat.isna().mean(axis=1)\n",
    "        filtered_ic50 = ic50_mat.loc[cellline_nan_ratio < 0.3, drug_nan_ratio < 0.3]\n",
    "        imputer = KNNImputer(n_neighbors=5)  # k-NN에서 k=5\n",
    "        imputed_ic50 = pd.DataFrame(\n",
    "            imputer.fit_transform(filtered_ic50),\n",
    "            index=filtered_ic50.index,\n",
    "            columns=filtered_ic50.columns)      \n",
    "        t = imputed_ic50.median()\n",
    "        binarized_ic50 = imputed_ic50.apply(lambda x: x.apply(lambda v: 1 if v <= t[x.name] else 0), axis=0)\n",
    "        \n",
    "        # index filtering, here only exp, mutation, cnv data are used\n",
    "        cell_line_index = binarized_ic50.index.intersection(data_concat.index)\n",
    "        ic50 = binarized_ic50.loc[cell_line_index]\n",
    "        \n",
    "        # train, val, test among filtered data\n",
    "        # these are valid train_, val_ and test_data index\n",
    "        train_lines = np.intersect1d(train_lines, ic50.index)\n",
    "        valid_validation_lines = np.intersect1d(validation_lines, ic50.index)\n",
    "        valid_test_lines = np.intersect1d(test_lines, ic50.index)\n",
    "        \n",
    "        n_train = len(train_lines)  \n",
    "        n_val = len(valid_validation_lines)      \n",
    "        n_test = len(valid_test_lines)\n",
    "        \n",
    "        # Precompute similarity matrices for each data type\n",
    "        similarity_matrices = {}\n",
    "        \n",
    "        if \"rnaseq\" in typ:\n",
    "            exp_com = np.corrcoef(np.vstack([filtered_rna.loc[train_lines], \n",
    "                                             filtered_rna.loc[valid_validation_lines], \n",
    "                                             filtered_rna.loc[valid_test_lines]]), rowvar=True)\n",
    "            train = exp_com[:n_train, :n_train]\n",
    "            val = exp_com[n_train:n_train+n_val, :n_train]\n",
    "            test = exp_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"rnaseq\"] = sim_combined\n",
    "        \n",
    "        if \"proteomics\" in typ:\n",
    "            prot_com = np.corrcoef(np.vstack([proteomics.loc[train_lines], \n",
    "                                              proteomics.loc[valid_validation_lines], \n",
    "                                              proteomics.loc[valid_test_lines]]), rowvar=True)\n",
    "            train = prot_com[:n_train, :n_train]\n",
    "            val = prot_com[n_train:n_train+n_val, :n_train]\n",
    "            test = prot_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"proteomics\"] = sim_combined\n",
    "        \n",
    "        if \"mutations\" in typ:\n",
    "            train_snp = filtered_mut.loc[train_lines].astype(bool)\n",
    "            val_snp = filtered_mut.loc[valid_validation_lines].astype(bool)\n",
    "            test_snp = filtered_mut.loc[valid_test_lines].astype(bool)\n",
    "            \n",
    "            train = 1 - pairwise_distances(train_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            val = 1 - pairwise_distances(val_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            test = 1 - pairwise_distances(test_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "    \n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"mutations\"] = sim_combined\n",
    "        \n",
    "        if \"methylations\" in typ:\n",
    "            methyl_com = np.corrcoef(np.vstack([methylations.loc[train_lines], \n",
    "                                                methylations.loc[valid_validation_lines], \n",
    "                                                methylations.loc[valid_test_lines]]), rowvar=True)\n",
    "            train = methyl_com[:n_train, :n_train]\n",
    "            val = methyl_com[n_train:n_train+n_val, :n_train]\n",
    "            test = methyl_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"mathylations\"] = sim_combined\n",
    "        \n",
    "        if \"cnvs\" in typ:\n",
    "            cnv_com = np.corrcoef(np.vstack([cnvs.loc[train_lines], \n",
    "                                             cnvs.loc[valid_validation_lines], \n",
    "                                             cnvs.loc[valid_test_lines]]), rowvar=True)\n",
    "            train= cnv_com[:n_train, :n_train]\n",
    "            val= cnv_com[n_train:n_train+n_val, :n_train]\n",
    "            test= cnv_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"cnvs\"] = sim_combined\n",
    "            \n",
    "        cell_dict = {}\n",
    "\n",
    "        # \n",
    "        for cell in cell_line_index:\n",
    "            cell_dict[cell] = {}\n",
    "            for data_type in typ:\n",
    "                sim_matrices = similarity_matrices[data_type]\n",
    "                sim_tensor = torch.Tensor(sim_matrices)\n",
    "                cell_idx = cell_line_index.get_loc(cell)\n",
    "                cell_dict[cell][data_type] = sim_tensor[cell_idx]\n",
    "                \n",
    "        train_lines = train_lines\n",
    "        validation_lines = valid_validation_lines\n",
    "        test_lines = valid_test_lines\n",
    "\n",
    "    # no change needed, query works fine with some missing\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "    \n",
    "    return (OmicsDataset_dict(cell_dict, drug_dict, train_data),\n",
    "    OmicsDataset_dict(cell_dict, drug_dict, validation_data),\n",
    "    OmicsDataset_dict(cell_dict, drug_dict, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9908faaa-375e-4e92-86a7-66cec1e1ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def get_data_corr(n_fold = 0, fp_radius = 2, transform_into_corr = True, typ = [\"rnaseq\", \"mutations\", \"cnvs\"]):\n",
    "    # drug\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    \n",
    "    # loading all datasets\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    \n",
    "    proteomics = pd.read_csv(\"data/proteomics.csv\", index_col=0)\n",
    "    \n",
    "    mutation = pd.read_csv(\"data/binary_mutations.csv\")\n",
    "    mutation.columns = mutation.iloc[0]\n",
    "    mutation = mutation.iloc[2:,:].set_index(\"gene_symbol\")\n",
    "    driver_columns = mutation.columns.isin(driver_genes)\n",
    "    filtered_mut = mutation.loc[:, driver_columns]\n",
    "    filtered_mut = filtered_mut.astype(float)\n",
    "\n",
    "    methylations = pd.read_csv(\"data/methylations.csv\",index_col = 0).sort_index(ascending = True)\n",
    "\n",
    "    cnvs = pd.read_csv(\"data/copy_number_variations.csv\",index_col= 0)\n",
    "\n",
    "    # concatenate all dataset \n",
    "    # inner join based on index: model_ids with NaN are automatically filtered out \n",
    "    data_concat = pd.concat([filtered_rna, proteomics, filtered_mut, methylations, cnvs], axis=1, join='inner')\n",
    "    \n",
    "    \n",
    "    # Filter data by common indices in all modalities\n",
    "    filtered_rna = filtered_rna[filtered_rna.index.isin(data_concat.index)]\n",
    "    proteomics = proteomics[proteomics.index.isin(data_concat.index)]\n",
    "    filtered_mut = filtered_mut[filtered_mut.index.isin(data_concat.index)]\n",
    "    methylations = methylations[methylations.index.isin(data_concat.index)]\n",
    "    cnvs = cnvs[cnvs.index.isin(data_concat.index)]\n",
    "    \n",
    "    # Initialize cell_dict\n",
    "    cell_dict = {}\n",
    "\n",
    "    if not transform_into_corr:\n",
    "        for cell in data_concat.index:\n",
    "            # Initialize a sub-dictionary for each cell\n",
    "            cell_dict[cell] = {}\n",
    "            \n",
    "            # Add data for each type specified in typ\n",
    "            if \"rnaseq\" in typ:\n",
    "                cell_dict[cell][\"rnaseq\"] = torch.Tensor(filtered_rna.loc[cell].to_numpy())\n",
    "            if \"proteomics\" in typ:\n",
    "                cell_dict[cell][\"proteomics\"] = torch.Tensor(proteomics.loc[cell].to_numpy())\n",
    "            if \"mutations\" in typ:\n",
    "                cell_dict[cell][\"mutations\"] = torch.Tensor(filtered_mut.loc[cell].to_numpy())\n",
    "            if \"methylations\" in typ:\n",
    "                cell_dict[cell][\"methylations\"] = torch.Tensor(methylations.loc[cell].to_numpy())\n",
    "            if \"cnvs\" in typ:\n",
    "                cell_dict[cell][\"cnvs\"] = torch.Tensor(cnvs.loc[cell].to_numpy())\n",
    "\n",
    "    # GDSC\n",
    "    GDSC1 = pd.read_csv(\"data/GDSC1.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = GDSC1.query(\"SANGER_MODEL_ID in @data_concat.index & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "\n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    test_lines = folds[0] \n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold] \n",
    "\n",
    "        # no change needed, query works fine with some missing\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "    \n",
    "    if transform_into_corr:\n",
    "        # train, val, test among filtered data\n",
    "        # these are valid train_, val_ and test_data index\n",
    "        \n",
    "        \n",
    "        n_train = len(train_lines)  \n",
    "        n_val = len(validation_lines)      \n",
    "        n_test = len(test_lines)\n",
    "        \n",
    "        # Precompute similarity matrices for each data type\n",
    "        similarity_matrices = {}\n",
    "        \n",
    "        if \"rnaseq\" in typ:\n",
    "            exp_com = np.corrcoef(np.vstack([filtered_rna.loc[train_lines], \n",
    "                                             filtered_rna.loc[validation_lines], \n",
    "                                             filtered_rna.loc[test_lines]]), rowvar=True)\n",
    "            train = exp_com[:n_train, :n_train]\n",
    "            val = exp_com[n_train:n_train+n_val, :n_train]\n",
    "            test = exp_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"rnaseq\"] = sim_combined\n",
    "        \n",
    "        if \"proteomics\" in typ:\n",
    "            prot_com = np.corrcoef(np.vstack([proteomics.loc[train_lines], \n",
    "                                              proteomics.loc[validation_lines], \n",
    "                                              proteomics.loc[test_lines]]), rowvar=True)\n",
    "            train = prot_com[:n_train, :n_train]\n",
    "            val = prot_com[n_train:n_train+n_val, :n_train]\n",
    "            test = prot_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"proteomics\"] = sim_combined\n",
    "        \n",
    "        if \"mutations\" in typ:\n",
    "            train_snp = filtered_mut.loc[train_lines].astype(bool)\n",
    "            val_snp = filtered_mut.loc[validation_lines].astype(bool)\n",
    "            test_snp = filtered_mut.loc[test_lines].astype(bool)\n",
    "            \n",
    "            train = 1 - pairwise_distances(train_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            val = 1 - pairwise_distances(val_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            test = 1 - pairwise_distances(test_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "    \n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"mutations\"] = sim_combined\n",
    "        \n",
    "        if \"methylations\" in typ:\n",
    "            methyl_com = np.corrcoef(np.vstack([methylations.loc[train_lines], \n",
    "                                                methylations.loc[validation_lines], \n",
    "                                                methylations.loc[test_lines]]), rowvar=True)\n",
    "            train = methyl_com[:n_train, :n_train]\n",
    "            val = methyl_com[n_train:n_train+n_val, :n_train]\n",
    "            test = methyl_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"mathylations\"] = sim_combined\n",
    "        \n",
    "        if \"cnvs\" in typ:\n",
    "            cnv_com = np.corrcoef(np.vstack([cnvs.loc[train_lines], \n",
    "                                             cnvs.loc[validation_lines], \n",
    "                                             cnvs.loc[test_lines]]), rowvar=True)\n",
    "            train= cnv_com[:n_train, :n_train]\n",
    "            val= cnv_com[n_train:n_train+n_val, :n_train]\n",
    "            test= cnv_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"cnvs\"] = sim_combined\n",
    "            \n",
    "        cell_dict = {}\n",
    "\n",
    "        # \n",
    "        for cell in unique_cell_lines:\n",
    "            cell_dict[cell] = {}\n",
    "            for data_type in typ:\n",
    "                sim_matrices = similarity_matrices[data_type]\n",
    "                sim_tensor = torch.Tensor(sim_matrices)\n",
    "                cell_idx = np.where(unique_cell_lines == cell)[0][0]\n",
    "                cell_dict[cell][data_type] = sim_tensor[cell_idx]\n",
    "    \n",
    "    return (OmicsDataset_dict(cell_dict, drug_dict, train_data),\n",
    "    OmicsDataset_dict(cell_dict, drug_dict, validation_data),\n",
    "    OmicsDataset_dict(cell_dict, drug_dict, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b48f9c5f-8d6d-45c2-82c4-783062baeb55",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:09:55] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "n_fold = 0\n",
    "fp_radius = 2\n",
    "transform_into_corr = True\n",
    "typ = [\"rnaseq\", \"mutations\", \"cnvs\"]\n",
    "\n",
    "# drug\n",
    "smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "\n",
    "# loading all datasets\n",
    "driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "\n",
    "rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "\n",
    "proteomics = pd.read_csv(\"data/proteomics.csv\", index_col=0)\n",
    "\n",
    "mutation = pd.read_csv(\"data/binary_mutations.csv\")\n",
    "mutation.columns = mutation.iloc[0]\n",
    "mutation = mutation.iloc[2:,:].set_index(\"gene_symbol\")\n",
    "driver_columns = mutation.columns.isin(driver_genes)\n",
    "filtered_mut = mutation.loc[:, driver_columns]\n",
    "filtered_mut = filtered_mut.astype(float)\n",
    "\n",
    "methylations = pd.read_csv(\"data/methylations.csv\",index_col = 0).sort_index(ascending = True)\n",
    "\n",
    "cnvs = pd.read_csv(\"data/copy_number_variations.csv\",index_col= 0)\n",
    "\n",
    "# concatenate all dataset \n",
    "# inner join based on index: model_ids with NaN are automatically filtered out \n",
    "data_concat = pd.concat([filtered_rna, proteomics, filtered_mut, methylations, cnvs], axis=1, join='inner')\n",
    "\n",
    "\n",
    "# Filter data by common indices in all modalities\n",
    "filtered_rna = filtered_rna[filtered_rna.index.isin(data_concat.index)]\n",
    "proteomics = proteomics[proteomics.index.isin(data_concat.index)]\n",
    "filtered_mut = filtered_mut[filtered_mut.index.isin(data_concat.index)]\n",
    "methylations = methylations[methylations.index.isin(data_concat.index)]\n",
    "cnvs = cnvs[cnvs.index.isin(data_concat.index)]\n",
    "\n",
    "# Initialize cell_dict\n",
    "cell_dict = {}\n",
    "\n",
    "if not transform_into_corr:\n",
    "    for cell in data_concat.index:\n",
    "        # Initialize a sub-dictionary for each cell\n",
    "        cell_dict[cell] = {}\n",
    "        \n",
    "        # Add data for each type specified in typ\n",
    "        if \"rnaseq\" in typ:\n",
    "            cell_dict[cell][\"rnaseq\"] = torch.Tensor(filtered_rna.loc[cell].to_numpy())\n",
    "        if \"proteomics\" in typ:\n",
    "            cell_dict[cell][\"proteomics\"] = torch.Tensor(proteomics.loc[cell].to_numpy())\n",
    "        if \"mutations\" in typ:\n",
    "            cell_dict[cell][\"mutations\"] = torch.Tensor(filtered_mut.loc[cell].to_numpy())\n",
    "        if \"methylations\" in typ:\n",
    "            cell_dict[cell][\"methylations\"] = torch.Tensor(methylations.loc[cell].to_numpy())\n",
    "        if \"cnvs\" in typ:\n",
    "            cell_dict[cell][\"cnvs\"] = torch.Tensor(cnvs.loc[cell].to_numpy())\n",
    "\n",
    "# GDSC\n",
    "GDSC1 = pd.read_csv(\"data/GDSC1.csv\", index_col=0)\n",
    "# default, remove data where lines or drugs are missing:\n",
    "data = GDSC1.query(\"SANGER_MODEL_ID in @data_concat.index & DRUG_ID in @drug_dict.keys()\")\n",
    "unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "\n",
    "np.random.seed(420) # for comparibility, don't change it!\n",
    "np.random.shuffle(unique_cell_lines)\n",
    "folds = np.array_split(unique_cell_lines, 10)\n",
    "test_lines = folds[0] \n",
    "train_idxs = list(range(10))\n",
    "train_idxs.remove(n_fold)\n",
    "np.random.seed(420)\n",
    "validation_idx = np.random.choice(train_idxs)\n",
    "train_idxs.remove(validation_idx)\n",
    "train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "validation_lines = folds[validation_idx]\n",
    "test_lines = folds[n_fold] \n",
    "\n",
    "    # no change needed, query works fine with some missing\n",
    "train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "511c410d-c320-4bac-8154-12c1ca9c68d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform_into_corr = True\n",
    "if transform_into_corr:\n",
    "    # train, val, test among filtered data\n",
    "    # these are valid train_, val_ and test_data index\n",
    "    \n",
    "    \n",
    "    n_train = len(train_lines)  \n",
    "    n_val = len(validation_lines)      \n",
    "    n_test = len(test_lines)\n",
    "    \n",
    "    # Precompute similarity matrices for each data type\n",
    "    similarity_matrices = {}\n",
    "    \n",
    "    if \"rnaseq\" in typ:\n",
    "        exp_com = np.corrcoef(np.vstack([filtered_rna.loc[train_lines], \n",
    "                                         filtered_rna.loc[validation_lines], \n",
    "                                         filtered_rna.loc[test_lines]]), rowvar=True)\n",
    "        train = exp_com[:n_train, :n_train]\n",
    "        val = exp_com[n_train:n_train+n_val, :n_train]\n",
    "        test = exp_com[n_train+n_val:, :n_train]\n",
    "        sim_combined = np.vstack([train, val, test])\n",
    "        similarity_matrices[\"rnaseq\"] = sim_combined\n",
    "    \n",
    "    if \"proteomics\" in typ:\n",
    "        prot_com = np.corrcoef(np.vstack([proteomics.loc[train_lines], \n",
    "                                          proteomics.loc[validation_lines], \n",
    "                                          proteomics.loc[test_lines]]), rowvar=True)\n",
    "        train = prot_com[:n_train, :n_train]\n",
    "        val = prot_com[n_train:n_train+n_val, :n_train]\n",
    "        test = prot_com[n_train+n_val:, :n_train]\n",
    "        sim_combined = np.vstack([train, val, test])\n",
    "        similarity_matrices[\"proteomics\"] = sim_combined\n",
    "    \n",
    "    if \"mutations\" in typ:\n",
    "        train_snp = filtered_mut.loc[train_lines].astype(bool)\n",
    "        val_snp = filtered_mut.loc[validation_lines].astype(bool)\n",
    "        test_snp = filtered_mut.loc[test_lines].astype(bool)\n",
    "        \n",
    "        train = 1 - pairwise_distances(train_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "        val = 1 - pairwise_distances(val_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "        test = 1 - pairwise_distances(test_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "\n",
    "        sim_combined = np.vstack([train, val, test])\n",
    "        similarity_matrices[\"mutations\"] = sim_combined\n",
    "    \n",
    "    if \"methylations\" in typ:\n",
    "        methyl_com = np.corrcoef(np.vstack([methylations.loc[train_lines], \n",
    "                                            methylations.loc[validation_lines], \n",
    "                                            methylations.loc[test_lines]]), rowvar=True)\n",
    "        train = methyl_com[:n_train, :n_train]\n",
    "        val = methyl_com[n_train:n_train+n_val, :n_train]\n",
    "        test = methyl_com[n_train+n_val:, :n_train]\n",
    "        sim_combined = np.vstack([train, val, test])\n",
    "        similarity_matrices[\"mathylations\"] = sim_combined\n",
    "    \n",
    "    if \"cnvs\" in typ:\n",
    "        cnv_com = np.corrcoef(np.vstack([cnvs.loc[train_lines], \n",
    "                                         cnvs.loc[validation_lines], \n",
    "                                         cnvs.loc[test_lines]]), rowvar=True)\n",
    "        train= cnv_com[:n_train, :n_train]\n",
    "        val= cnv_com[n_train:n_train+n_val, :n_train]\n",
    "        test= cnv_com[n_train+n_val:, :n_train]\n",
    "        sim_combined = np.vstack([train, val, test])\n",
    "        similarity_matrices[\"cnvs\"] = sim_combined\n",
    "        \n",
    "    cell_dict = {}\n",
    "\n",
    "    # \n",
    "    for cell in unique_cell_lines:\n",
    "            cell_dict[cell] = {}\n",
    "            for data_type in typ:\n",
    "                sim_matrices = similarity_matrices[data_type]\n",
    "                sim_tensor = torch.Tensor(sim_matrices)\n",
    "                cell_idx = np.where(unique_cell_lines == cell)[0][0]\n",
    "                cell_dict[cell][data_type] = sim_tensor[cell_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8596e9e-cdf5-43e1-87c5-4053a6f3364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"features\" : {\"fp_radius\":2,\n",
    "                        \"transform_into_corr\": True},\n",
    "          \"optimizer\": {\"batch_size\": 220,\n",
    "                        \"clip_norm\":19,\n",
    "                        \"learning_rate\": 0.0004592646200179472,\n",
    "                        \"stopping_patience\":15},\n",
    "          \"model\":{\"embed_dim\":485,\n",
    "                 \"hidden_dim\":696, # hidden layer의 차원\n",
    "                 \"dropout\":0.48541242824674574, # 40퍼센트의 노드를 랜덤하게 드랍아웃 \n",
    "                 \"n_layers\": 4, # 3개의 hidden layer를 사용\n",
    "                 \"norm\": \"batchnorm\"}, # batch normalization을 사용하여 모델이 학습 중 출력 분포를 정규화하여 학습을 안정화\n",
    "         \"env\": {\"fold\": 0, # 0번째 fold를 사용하여 학습. 이는 음 n_fold에 들어갈 값을 의미하는 듯 하다. \n",
    "                \"device\":\"cuda:1\", # GPU자원을 사용할 장치를 지정한다. \n",
    "                 \"max_epochs\": 100, # 최대 epoch 수 \n",
    "                 \"search_hyperparameters\":False}} # hyper parameter 이미 있으니 안쓴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "626095e5-8277-4a15-814f-a3557eec4c4f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:44] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:28:45] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset = get_data_corr(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"], \n",
    "                                                           transform_into_corr = config[\"features\"][\"transform_into_corr\"],\n",
    "                                                           typ = (\"rnaseq\", \"mutations\", \"cnvs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a70cb18-226e-46ee-bb31-67e48878158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, num_modalities = 1,  hidden_dim = 150, latent_dim = 75, fusion_dim = 150, dropout = 0.2):\n",
    "        # get input as a dictionary\n",
    "        super(MultimodalAutoencoder, self).__init__()\n",
    "        # EEEEEEEEEEncoder\n",
    "        self.input_dim = input_dim\n",
    "        self.num_modalities = num_modalities\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "        ############# 여기서 input_dict를 사용하지 않도록 모델 구성을 바꿔야함.. 이런씨발\n",
    "        self.omics_encoder = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim), # input \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, latent_dim) # encoder hidden layer: 150, 75 as the value from the paper. so we start from this \n",
    "            )                                 # I dont get why they used 150, 75 for dimension, but we can tune it later\n",
    "            for _ in range(num_modalities)\n",
    "        ])\n",
    "        # fused latent feature \n",
    "        self.fusion_layer = nn.Sequential( # I think we need a fusion layer here, to combine the data modalities\n",
    "            nn.Linear(latent_dim * num_modalities, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fusion_dim, latent_dim) # This concatenate latent features of all omics data, and fusion them and make its dim final latent dim\n",
    "        )                                     # This is the only way I can think of to fuse omics data\n",
    "        # decoder\n",
    "        self.omics_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim * num_modalities)\n",
    "        )\n",
    "        # I actually dont understand this step in paper. they said that decoder has symmetric structure as encoder,\n",
    "        # but the data after MDA they provided, has weird dimension(363x90) which makes no sense. this is the point that i cant understand\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_features = [] # get dictionary as an input \n",
    "        for i, encoder in enumerate(self.omics_encoder):\n",
    "            start_idx = i * self.input_dim\n",
    "            end_idx = start_idx + self.input_dim\n",
    "            x_modality = x[:, start_idx:end_idx]\n",
    "            latent_features.append(encoder(self.do(x_modality)))\n",
    "            \n",
    "        latent_fused = torch.cat(latent_features, dim=1)\n",
    "        latent_final = self.fusion_layer(latent_fused)\n",
    "        decoded = self.omics_decoder(latent_final)\n",
    "        return decoded, latent_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a204aa-6598-46d3-b9eb-54a0b5a0dfb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b3c33f7-e4f1-4b95-a40a-c56e7f3ed775",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70aa70c1-9f64-4bc7-bb1d-74c2da8ba74a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalAutoencoder(\n",
       "  (do): Dropout(p=0.8, inplace=False)\n",
       "  (omics_encoder): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=587, out_features=150, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=150, out_features=75, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=587, out_features=150, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=150, out_features=75, bias=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=587, out_features=150, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=150, out_features=75, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fusion_layer): Sequential(\n",
       "    (0): Linear(in_features=225, out_features=150, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=150, out_features=75, bias=True)\n",
       "  )\n",
       "  (omics_decoder): Sequential(\n",
       "    (0): Linear(in_features=75, out_features=150, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=150, out_features=1761, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_rate = 0.5\n",
    "learning_rate = 1e-3\n",
    "batch_size = 256\n",
    "num_epochs = 1000\n",
    "num_modality = 3\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "sample_batch = next(iter(train_loader))\n",
    "sample_omics_data = sample_batch[0]  # 첫 번째 요소는 omics_data (dict)\n",
    "mod_len = sample_omics_data.shape[1] // num_modality\n",
    "#input_dict = {f\"modality_{i+1}\": mod_len for i in range(num_modality)}\n",
    "\n",
    "model = MultimodalAutoencoder(input_dim = mod_len, num_modalities = 3, dropout = 0.8)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e5237d5-c6cf-4a27-8dda-e10b975253ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2081,  0.2020, -0.0809,  ...,  0.0844,  0.0484, -0.0141],\n",
       "        [-0.3419, -0.3384,  0.2526,  ...,  0.1527, -0.0380,  0.1270],\n",
       "        [-0.3253, -0.4495,  0.2317,  ...,  0.0783,  0.2118,  0.2633],\n",
       "        ...,\n",
       "        [-0.0866, -0.1850,  0.0264,  ...,  0.1203,  0.1833,  0.2244],\n",
       "        [ 0.1804,  0.3409, -0.0882,  ...,  0.0277,  0.2924,  0.0820],\n",
       "        [-0.0522, -0.1777,  0.1209,  ..., -0.0610,  0.2157,  0.1006]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omics_data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62796ca2-b096-4083-92a5-c66178b13b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[-0.0871, -0.0700,  0.2474,  ...,  0.0938,  0.5390,  0.0820],\n",
      "        [ 0.1755,  0.2270, -0.3000,  ..., -0.0374,  0.4547,  0.0854],\n",
      "        [ 0.3202,  0.3223, -0.2270,  ...,  0.1257,  0.2778,  0.1543],\n",
      "        ...,\n",
      "        [-0.0774,  0.0447, -0.1058,  ..., -0.0303,  0.3275,  0.1131],\n",
      "        [-0.1593, -0.0764, -0.0935,  ..., -0.0150,  0.2229,  0.2515],\n",
      "        [-0.1343, -0.2704,  0.1957,  ...,  0.0759,  0.0411, -0.0151]],\n",
      "       device='cuda:0')\n",
      "Reconstructed: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Inputs: tensor([[-0.0124,  0.0450, -0.1593,  ...,  0.0401,  0.3951,  0.0974],\n",
      "        [-0.1035, -0.1015,  0.3489,  ...,  0.1084,  0.4336,  0.0931],\n",
      "        [ 0.0218, -0.0903,  0.2141,  ..., -0.0173,  0.1873,  0.4414],\n",
      "        ...,\n",
      "        [ 0.1718,  0.1819, -0.1106,  ...,  0.0749,  0.2051,  0.1624],\n",
      "        [ 0.2292,  0.2264, -0.2498,  ..., -0.0194,  0.0678, -0.0119],\n",
      "        [-0.1050, -0.0429, -0.0865,  ...,  0.0335,  0.2622,  0.0891]],\n",
      "       device='cuda:0')\n",
      "Reconstructed: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Inputs: tensor([[-0.0876, -0.1515,  0.1133,  ..., -0.0611,  0.2140,  0.1043],\n",
      "        [-0.1918, -0.2682,  0.0793,  ...,  0.0117, -0.0625, -0.0279],\n",
      "        [ 0.0359,  0.1408, -0.0298,  ...,  0.1243,  0.4655,  0.1101],\n",
      "        ...,\n",
      "        [-0.0660, -0.1846,  0.1689,  ..., -0.0332,  0.1787,  0.0947],\n",
      "        [ 0.1329,  0.1299, -0.1806,  ..., -0.0213,  0.2299,  0.1719],\n",
      "        [-0.1638, -0.1914,  0.2829,  ..., -0.0173,  0.1873,  0.2154]],\n",
      "       device='cuda:0')\n",
      "Reconstructed: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Epoch 1/1000, Loss: nan\n",
      "Epoch 2/1000, Loss: nan\n",
      "Epoch 3/1000, Loss: nan\n",
      "Epoch 4/1000, Loss: nan\n",
      "Epoch 5/1000, Loss: nan\n",
      "Epoch 6/1000, Loss: nan\n",
      "Epoch 7/1000, Loss: nan\n",
      "Epoch 8/1000, Loss: nan\n",
      "Epoch 9/1000, Loss: nan\n",
      "Epoch 10/1000, Loss: nan\n",
      "Epoch 11/1000, Loss: nan\n",
      "Epoch 12/1000, Loss: nan\n",
      "Epoch 13/1000, Loss: nan\n",
      "Epoch 14/1000, Loss: nan\n",
      "Epoch 15/1000, Loss: nan\n",
      "Epoch 16/1000, Loss: nan\n",
      "Epoch 17/1000, Loss: nan\n",
      "Epoch 18/1000, Loss: nan\n",
      "Epoch 19/1000, Loss: nan\n",
      "Epoch 20/1000, Loss: nan\n",
      "Epoch 21/1000, Loss: nan\n",
      "Epoch 22/1000, Loss: nan\n",
      "Epoch 23/1000, Loss: nan\n",
      "Epoch 24/1000, Loss: nan\n",
      "Epoch 25/1000, Loss: nan\n",
      "Epoch 26/1000, Loss: nan\n",
      "Epoch 27/1000, Loss: nan\n",
      "Epoch 28/1000, Loss: nan\n",
      "Epoch 29/1000, Loss: nan\n",
      "Epoch 30/1000, Loss: nan\n",
      "Epoch 31/1000, Loss: nan\n",
      "Epoch 32/1000, Loss: nan\n",
      "Epoch 33/1000, Loss: nan\n",
      "Epoch 34/1000, Loss: nan\n",
      "Epoch 35/1000, Loss: nan\n",
      "Epoch 36/1000, Loss: nan\n",
      "Epoch 37/1000, Loss: nan\n",
      "Epoch 38/1000, Loss: nan\n",
      "Epoch 39/1000, Loss: nan\n",
      "Epoch 40/1000, Loss: nan\n",
      "Epoch 41/1000, Loss: nan\n",
      "Epoch 42/1000, Loss: nan\n",
      "Epoch 43/1000, Loss: nan\n",
      "Epoch 44/1000, Loss: nan\n",
      "Epoch 45/1000, Loss: nan\n",
      "Epoch 46/1000, Loss: nan\n",
      "Epoch 47/1000, Loss: nan\n",
      "Epoch 48/1000, Loss: nan\n",
      "Epoch 49/1000, Loss: nan\n",
      "Epoch 50/1000, Loss: nan\n",
      "Epoch 51/1000, Loss: nan\n",
      "Epoch 52/1000, Loss: nan\n",
      "Epoch 53/1000, Loss: nan\n",
      "Epoch 54/1000, Loss: nan\n",
      "Epoch 55/1000, Loss: nan\n",
      "Epoch 56/1000, Loss: nan\n",
      "Epoch 57/1000, Loss: nan\n",
      "Epoch 58/1000, Loss: nan\n",
      "Epoch 59/1000, Loss: nan\n",
      "Epoch 60/1000, Loss: nan\n",
      "Epoch 61/1000, Loss: nan\n",
      "Epoch 62/1000, Loss: nan\n",
      "Epoch 63/1000, Loss: nan\n",
      "Epoch 64/1000, Loss: nan\n",
      "Epoch 65/1000, Loss: nan\n",
      "Epoch 66/1000, Loss: nan\n",
      "Epoch 67/1000, Loss: nan\n",
      "Epoch 68/1000, Loss: nan\n",
      "Epoch 69/1000, Loss: nan\n",
      "Epoch 70/1000, Loss: nan\n",
      "Epoch 71/1000, Loss: nan\n",
      "Epoch 72/1000, Loss: nan\n",
      "Epoch 73/1000, Loss: nan\n",
      "Epoch 74/1000, Loss: nan\n",
      "Epoch 75/1000, Loss: nan\n",
      "Epoch 76/1000, Loss: nan\n",
      "Epoch 77/1000, Loss: nan\n",
      "Epoch 78/1000, Loss: nan\n",
      "Epoch 79/1000, Loss: nan\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "x = 0\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = []\n",
    "    model.train()\n",
    "    \n",
    "    # train step\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed, latent = model(inputs)\n",
    "\n",
    "        #target_data = torch.cat([inputs[modality] for modality in inputs], dim=1)\n",
    "        # I think, here inappropriate loss is used. \n",
    "        # From what I know, Autoencoder reconstructs input feature, and calculate loss through comparing input feature and reconstructed feature.\n",
    "        # So, calculating MSE comparing with target_data, which is drug sensitivity, is nonsense. \n",
    "        #loss = criterion(reconstructed, target_data)\n",
    "        \n",
    "        # here is a new loss function, but it still outputs nan value as a loss...\n",
    "        loss = criterion(reconstructed, inputs)\n",
    "        if x < 3:\n",
    "            \n",
    "            print(f\"Inputs: {inputs}\")\n",
    "            print(f\"Reconstructed: {reconstructed}\")\n",
    "        x += 1\n",
    "        \n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config[\"optimizer\"][\"clip_norm\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "    train_loss = np.mean(total_loss)\n",
    "    scheduler.step(train_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.5f}\")\n",
    "    \n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d787ac75-9a90-4c05-85be-a2e6dda20ad6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1638\n",
      "1638\n"
     ]
    }
   ],
   "source": [
    "# checking structure of the variables \n",
    "\n",
    "x = 0\n",
    "for batch in train_loader:\n",
    "    #omics_data = {modality: data.to(device) for modality, data in batch.items()}\n",
    "    #print(type(batch))\n",
    "#    print(batch)\n",
    "    \n",
    "    x +=1\n",
    "    if x > 0:\n",
    "        break\n",
    "\n",
    "#input_dict.items()\n",
    "print(sample_omics_data.shape[1])\n",
    "#print(n_train*3)\n",
    "print(len(batch[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a1a9c9-3420-4c6a-b3f2-8a554d7606c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
