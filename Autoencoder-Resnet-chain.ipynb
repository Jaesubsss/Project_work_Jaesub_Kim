{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ce831d-3d8b-4bdf-9d5c-8f96227a2b8c",
   "metadata": {},
   "source": [
    "# Package & Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9dff727-3bc9-4647-9d03-3aa3bdeb368e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "from torchmetrics import MeanSquaredError\n",
    "from sklearn.impute import KNNImputer\n",
    "import scripts\n",
    "from functools import lru_cache\n",
    "import optuna\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import warnings\n",
    "from torch.utils.data import Dataset\n",
    "# 경고 무시 설정\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6edd50e5-91f8-4df6-8d58-e5e712430d34",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dataset for sim_matrix\n",
    "class OmicsDataset_dict(Dataset): \n",
    "    def __init__(self, omic_dict, drug_dict, data): \n",
    "        self.omic_dict = omic_dict\n",
    "        self.drug_dict = drug_dict\n",
    "        self.cell_mapped_ids = {key:i for i, key in enumerate(self.omic_dict.keys())}\n",
    "        # omic_dict의 키를 고유한 인덱스로 매핑\n",
    "        # enumerate는 키들을 순서대로 열거하여 (인덱스, 키) 형태의 튜플로 반환\n",
    "        # 딕셔너리 컴프레헨션: 각 키를 key로, 각 키의 인덱스를 i로 사용하여 {key:i}형태로 매핑된 딕셔너리 만듬.\n",
    "        self.drug_mapped_ids = {key:i for i, key in enumerate(self.drug_dict.keys())}\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx): # idx = train_data\n",
    "        instance = self.data.iloc[idx] \n",
    "        cell_id = instance.iloc[0]\n",
    "        drug_id = instance.iloc[1]\n",
    "        target = instance.iloc[2]\n",
    "        \n",
    "        #omics_data = { # usage of dictionary here causes a problem or crash with collate_fn function in Dataloader \n",
    "        #    cell_id : {\n",
    "        #        data_type: self.omic_dict[cell_id][data_type] for data_type in self.omic_dict[cell_id].keys()\n",
    "        #    }\n",
    "        #}\n",
    "        \n",
    "        return (torch.cat([self.omic_dict[cell_id][modality] for modality in self.omic_dict[cell_id].keys()]), \n",
    "                self.drug_dict[drug_id],\n",
    "                torch.Tensor([target]),\n",
    "                torch.Tensor([self.cell_mapped_ids[cell_id]]),\n",
    "                torch.Tensor([self.drug_mapped_ids[drug_id]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deaf58b2-acde-4a0b-92c0-c32057378a9e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# MDA model\n",
    "class MultimodalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dims,  hidden_dim_encoders = 150, embed_dim = 75, fusion_dim = 150, dropout_encoders = 0.2):\n",
    "        # get input as a dictionary\n",
    "        super(MultimodalAutoencoder, self).__init__()\n",
    "        # EEEEEEEEEEncoder\n",
    "        self.input_dims = input_dims\n",
    "        self.num_modalities = len(input_dims)\n",
    "        self.do = nn.Dropout(dropout_encoders)\n",
    "\n",
    "        self.omics_encoder = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(mod_input_dim, hidden_dim_encoders), # input \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim_encoders, embed_dim) # encoder hidden layer: 150, 75 as the value from the paper. so we start from this \n",
    "            )                                 # I dont get why they used 150, 75 for dimension, but we can tune it later\n",
    "            for mod_input_dim in self.input_dims\n",
    "        ])\n",
    "        # fused latent feature \n",
    "        self.fusion_layer = nn.Sequential( # I think we need a fusion layer here, to combine the data modalities\n",
    "            nn.Linear(embed_dim * self.num_modalities, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fusion_dim, embed_dim) # This concatenate latent features of all omics data, and fusion them and make its dim final latent dim\n",
    "        )                                     # This is the only way I can think of to fuse omics data\n",
    "        # decoder\n",
    "        self.omics_decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim_encoders),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_encoders, sum(input_dims))\n",
    "        )\n",
    "        # I actually dont understand this step in paper. they said that decoder has symmetric structure as encoder,\n",
    "        # but the data after MDA they provided, has weird dimension(363x90) which makes no sense. this is the point that i cant understand\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_features = [] # get dictionary as an input \n",
    "        start_idx = 0\n",
    "        for i, encoder in enumerate(self.omics_encoder):\n",
    "            mod_input_dim = self.input_dims[i]\n",
    "            x_modality = x[:, start_idx:start_idx + mod_input_dim]\n",
    "            latent_features.append(encoder(self.do(x_modality)))\n",
    "            start_idx += mod_input_dim\n",
    "            \n",
    "        latent_fused = torch.cat(latent_features, dim=1)\n",
    "        latent_final = self.fusion_layer(latent_fused)\n",
    "        decoded = self.omics_decoder(latent_final)\n",
    "        return decoded, latent_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d90f4c86-c210-4d9e-a83b-a8cddf993ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main resnet model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, embed_dim=256, hidden_dim=1024, dropout=0.1, n_layers=6, norm=\"layernorm\"):\n",
    "        super().__init__()\n",
    "        self.mlps = nn.ModuleList()\n",
    "        \n",
    "        # Determine normalization layer\n",
    "        if norm == \"layernorm\":\n",
    "            norm_layer = nn.LayerNorm\n",
    "        elif norm == \"batchnorm\":\n",
    "            norm_layer = nn.BatchNorm1d\n",
    "        else:\n",
    "            norm_layer = nn.Identity\n",
    "        \n",
    "        # Create MLP layers\n",
    "        for _ in range(n_layers):\n",
    "            self.mlps.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(embed_dim, hidden_dim),\n",
    "                    norm_layer(hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(hidden_dim, embed_dim)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.lin = nn.Linear(embed_dim, 1)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.mlps:\n",
    "            \n",
    "            x = (layer(x) + x) / 2  # Residual connection\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "class Main_model(nn.Module):\n",
    "    def __init__(self, embed_dim=256, hidden_dim=1024, dropout=0.1, n_layers=6, norm=\"layernorm\", \n",
    "                 dropout_omics=0.4, dropout_omics_finetuning=0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ResNet as backbone\n",
    "        self.resnet = ResNet(embed_dim, hidden_dim, dropout, n_layers, norm)\n",
    "        \n",
    "        # Modified embed_d: Two-layer MLP with dropout after ReLU\n",
    "        self.embed_d = nn.Sequential(\n",
    "            nn.LazyLinear(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Modified embed_c: Two-layer MLP with dropout after ReLU and before first Linear Layer\n",
    "        self.embed_c = nn.Sequential(\n",
    "            nn.Dropout(dropout_omics_finetuning),\n",
    "            nn.LazyLinear(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_omics),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, c, d):\n",
    "        # Combine embedded inputs and pass through ResNet\n",
    "        return self.resnet(self.embed_d(d) + self.embed_c(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a73df737-dd85-452a-9b4b-1ffd3dc25eb9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# get Data function\n",
    "@lru_cache(maxsize=None)\n",
    "def get_data_corr(n_fold = 0, fp_radius = 2, transform_into_corr = True, typ = [\"rnaseq\", \"mutations\", \"cnvs\"],\n",
    "                  #reconstructed = None\n",
    "                 ):\n",
    "    # drug\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    \n",
    "    # loading all datasets\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    \n",
    "    proteomics = pd.read_csv(\"data/proteomics.csv\", index_col=0)\n",
    "    \n",
    "    mutation = pd.read_csv(\"data/binary_mutations.csv\")\n",
    "    mutation.columns = mutation.iloc[0]\n",
    "    mutation = mutation.iloc[2:,:].set_index(\"gene_symbol\")\n",
    "    driver_columns = mutation.columns.isin(driver_genes)\n",
    "    filtered_mut = mutation.loc[:, driver_columns]\n",
    "    filtered_mut = filtered_mut.astype(float)\n",
    "\n",
    "    methylations = pd.read_csv(\"data/methylations.csv\",index_col = 0).sort_index(ascending = True)\n",
    "\n",
    "    cnvs = pd.read_csv(\"data/copy_number_variations.csv\",index_col= 0)\n",
    "\n",
    "    # concatenate all dataset \n",
    "    # inner join based on index: model_ids with NaN are automatically filtered out \n",
    "    data_concat = pd.concat([filtered_rna, proteomics, filtered_mut, methylations, cnvs], axis=1, join='inner')\n",
    "    \n",
    "    \n",
    "    # Filter data by common indices in all modalities\n",
    "    filtered_rna = filtered_rna[filtered_rna.index.isin(data_concat.index)]\n",
    "    proteomics = proteomics[proteomics.index.isin(data_concat.index)]\n",
    "    filtered_mut = filtered_mut[filtered_mut.index.isin(data_concat.index)]\n",
    "    methylations = methylations[methylations.index.isin(data_concat.index)]\n",
    "    cnvs = cnvs[cnvs.index.isin(data_concat.index)]\n",
    "    \n",
    "    # Initialize cell_dict\n",
    "    cell_dict = {}\n",
    "\n",
    "    if not transform_into_corr : #and reconstructed is None:\n",
    "\n",
    "        dims = []\n",
    "        if \"rnaseq\" in typ:\n",
    "            dims.append(filtered_rna.shape[1])\n",
    "        if \"proteomics\" in typ:\n",
    "            dims.append(proteomics.shape[1])\n",
    "        if \"mutations\" in typ:\n",
    "            dims.append(filtered_mut.shape[1])\n",
    "        if \"methylations\" in typ:\n",
    "            dims.append(methylations.shape[1])\n",
    "        if \"cnvs\" in typ:\n",
    "            dims.append(cnvs.shape[1])\n",
    "        \n",
    "        for cell in data_concat.index:\n",
    "            # Initialize a sub-dictionary for each cell\n",
    "            concatenated_data = []\n",
    "            \n",
    "            # Add data for each type specified in typ\n",
    "            if \"rnaseq\" in typ:\n",
    "                concatenated_data.append(filtered_rna.loc[cell].to_numpy())\n",
    "            if \"proteomics\" in typ:\n",
    "                concatenated_data.append(proteomics.loc[cell].to_numpy())\n",
    "            if \"mutations\" in typ:\n",
    "                concatenated_data.append(filtered_mut.loc[cell].to_numpy())\n",
    "            if \"methylations\" in typ:\n",
    "                concatenated_data.append(methylations.loc[cell].to_numpy())\n",
    "            if \"cnvs\" in typ:\n",
    "                concatenated_data.append(cnvs.loc[cell].to_numpy())\n",
    "\n",
    "            cell_dict[cell] = torch.Tensor(np.concatenate(concatenated_data))\n",
    "            \n",
    "#    if reconstructed is not None:\n",
    "#        for cell_idx, cell in enumerate(data_concat.index):\n",
    "#            # cell_dict에 reconstructed 텐서의 각 행(cell 데이터) 저장\n",
    "#            cell_dict[cell] = reconstructed[cell_idx]\n",
    "        \n",
    "\n",
    "    # GDSC\n",
    "    GDSC1 = pd.read_csv(\"data/GDSC1.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = GDSC1.query(\"SANGER_MODEL_ID in @data_concat.index & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "\n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    test_lines = folds[0] \n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold] \n",
    "\n",
    "        # no change needed, query works fine with some missing\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "    \n",
    "    if transform_into_corr: #and reconstructed is None:\n",
    "        # train, val, test among filtered data\n",
    "        # these are valid train_, val_ and test_data index\n",
    "        \n",
    "        \n",
    "        n_train = len(train_lines)  \n",
    "        n_val = len(validation_lines)      \n",
    "        n_test = len(test_lines)\n",
    "        \n",
    "        # Precompute similarity matrices for each data type\n",
    "        similarity_matrices = {}\n",
    "        dims = []\n",
    "        \n",
    "        if \"rnaseq\" in typ:\n",
    "            exp_com = np.corrcoef(np.vstack([filtered_rna.loc[train_lines], \n",
    "                                             filtered_rna.loc[validation_lines], \n",
    "                                             filtered_rna.loc[test_lines]]), rowvar=True)\n",
    "            train = exp_com[:n_train, :n_train]\n",
    "            val = exp_com[n_train:n_train+n_val, :n_train]\n",
    "            test = exp_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"rnaseq\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"rnaseq\"][0]))\n",
    "        \n",
    "        if \"proteomics\" in typ:\n",
    "            prot_com = np.corrcoef(np.vstack([proteomics.loc[train_lines], \n",
    "                                              proteomics.loc[validation_lines], \n",
    "                                              proteomics.loc[test_lines]]), rowvar=True)\n",
    "            train = prot_com[:n_train, :n_train]\n",
    "            val = prot_com[n_train:n_train+n_val, :n_train]\n",
    "            test = prot_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"proteomics\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"proteomics\"][0]))\n",
    "        \n",
    "        if \"mutations\" in typ:\n",
    "            train_snp = filtered_mut.loc[train_lines].astype(bool)\n",
    "            val_snp = filtered_mut.loc[validation_lines].astype(bool)\n",
    "            test_snp = filtered_mut.loc[test_lines].astype(bool)\n",
    "            \n",
    "            train = 1 - pairwise_distances(train_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            val = 1 - pairwise_distances(val_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            test = 1 - pairwise_distances(test_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "    \n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"mutations\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"mutations\"][0]))\n",
    "        \n",
    "        if \"methylations\" in typ:\n",
    "            methyl_com = np.nan_to_num(np.corrcoef(np.vstack([methylations.loc[train_lines], \n",
    "                                                methylations.loc[validation_lines], \n",
    "                                                methylations.loc[test_lines]]), rowvar=True))\n",
    "            train = methyl_com[:n_train, :n_train]\n",
    "            val = methyl_com[n_train:n_train+n_val, :n_train]\n",
    "            test = methyl_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"methylations\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"methylations\"][0]))\n",
    "        \n",
    "        if \"cnvs\" in typ:\n",
    "            cnv_com = np.nan_to_num(np.corrcoef(np.vstack([cnvs.loc[train_lines], # nan-generation problem fixed \n",
    "                                             cnvs.loc[validation_lines], \n",
    "                                             cnvs.loc[test_lines]]), rowvar=True))\n",
    "            train= cnv_com[:n_train, :n_train]\n",
    "            val= cnv_com[n_train:n_train+n_val, :n_train]\n",
    "            test= cnv_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"cnvs\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"cnvs\"][0]))\n",
    "            \n",
    "        cell_dict = {}\n",
    "\n",
    "        # \n",
    "        for cell in unique_cell_lines:\n",
    "            cell_dict[cell] = {}\n",
    "            for data_type in typ:\n",
    "                sim_matrices = similarity_matrices[data_type]\n",
    "                sim_tensor = torch.Tensor(sim_matrices)\n",
    "                cell_idx = np.where(unique_cell_lines == cell)[0][0]\n",
    "                cell_dict[cell][data_type] = sim_tensor[cell_idx]\n",
    "    \n",
    "        return (OmicsDataset_dict(cell_dict, drug_dict, train_data),\n",
    "        OmicsDataset_dict(cell_dict, drug_dict, validation_data),\n",
    "        OmicsDataset_dict(cell_dict, drug_dict, test_data),\n",
    "        dims)\n",
    "\n",
    "    return (scripts.OmicsDataset(cell_dict, drug_dict, train_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, validation_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, test_data),\n",
    "    dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e757bd2-4119-456e-aaac-944a4f56547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_train_step(autoencoder, resnet, optimizer, loader, config, device):\n",
    "    \"\"\"\n",
    "    Autoencoder와 ResNet을 동시에 학습하는 End-to-End Training Step\n",
    "    \"\"\"\n",
    "\n",
    "    # 손실 함수 정의\n",
    "    loss_fn_recon = nn.MSELoss()  # Autoencoder의 Reconstruction Loss\n",
    "    loss_fn_pred = nn.MSELoss()   # ResNet의 Prediction Loss\n",
    "    ls = []  # ResNet의 Loss 저장\n",
    "    ls_recon = []  # Autoencoder의 Reconstruction Loss 저장\n",
    "\n",
    "    autoencoder.train()  # Autoencoder 학습 모드\n",
    "    resnet.train()  # ResNet 학습 모드\n",
    "    \n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1️⃣ Autoencoder Forward (Latent Feature 생성)\n",
    "        reconstructed, latent_features = autoencoder(batch[0].to(device))  # (batch_size, latent_dim)\n",
    "        \n",
    "        # 2️⃣ ResNet Forward (Latent Feature + Drug 데이터)\n",
    "        out = resnet(latent_features, batch[1].to(device))\n",
    "\n",
    "        # 3️⃣ Loss 계산\n",
    "        loss_recon = loss_fn_recon(reconstructed, batch[0].to(device))  # Autoencoder Loss\n",
    "        loss_pred = loss_fn_pred(out.squeeze(), batch[2].to(device).squeeze())  # ResNet Loss\n",
    "\n",
    "        # 4️⃣ Total Loss = Reconstruction Loss + Prediction Loss\n",
    "        loss = loss_recon + loss_pred\n",
    "\n",
    "        # 5️⃣ Backpropagation\n",
    "        loss.backward() # 두 losses를 모두 backpropagation한다.\n",
    "        torch.nn.utils.clip_grad_norm_(list(autoencoder.parameters()) + list(resnet.parameters()), config[\"optimizer\"][\"clip_norm\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        # 6️⃣ Loss 저장\n",
    "        ls.append(loss_pred.item())\n",
    "        ls_recon.append(loss_recon.item())\n",
    "\n",
    "    return np.mean(ls), np.mean(ls_recon)\n",
    "\n",
    "\n",
    "def train_auto_resnet_chain(config, train_dataset, validation_dataset=None, use_momentum=True, callback_epoch=None):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "        drop_last=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    if validation_dataset is not None:\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            validation_dataset,\n",
    "            batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "            drop_last=False,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    autoencoder = MultimodalAutoencoder(\n",
    "        input_dims=input_dims,\n",
    "        hidden_dim_encoders=config[\"model\"][\"hidden_dim_encoders\"],\n",
    "        embed_dim=config[\"model\"][\"embed_dim\"],\n",
    "        fusion_dim=config[\"model\"][\"fusion_dim\"],\n",
    "        dropout_encoders=config[\"model\"][\"dropout_encoders\"]\n",
    "    )\n",
    "\n",
    "    resnet = Main_model(\n",
    "        embed_dim=config[\"model\"][\"embed_dim\"],\n",
    "        hidden_dim=config[\"model\"][\"hidden_dim\"], \n",
    "        dropout=config[\"model\"][\"dropout\"], \n",
    "        n_layers=config[\"model\"][\"n_layers\"],  \n",
    "        dropout_omics=config[\"model\"][\"dropout_omics\"], \n",
    "        dropout_omics_finetuning=config[\"model\"][\"dropout_omics_finetuning\"],\n",
    "        norm=config[\"model\"][\"norm\"]\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(autoencoder.parameters()) + list(resnet.parameters()), config[\"optimizer\"][\"learning_rate\"])\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)\n",
    "    early_stop = scripts.EarlyStop(config[\"optimizer\"][\"stopping_patience\"])\n",
    "    device = torch.device(config[\"env\"][\"device\"])\n",
    "    autoencoder.to(device)\n",
    "    resnet.to(device)\n",
    "    autoencoder.train()\n",
    "    resnet.train()\n",
    "\n",
    "    metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection({\n",
    "        \"R_cellwise_residuals\": scripts.GroupwiseMetric(\n",
    "            metric=torchmetrics.functional.pearson_corrcoef,\n",
    "            grouping=\"drugs\",\n",
    "            average=\"macro\",\n",
    "            residualize=True\n",
    "        ),\n",
    "        \"R_cellwise\": scripts.GroupwiseMetric(\n",
    "            metric=torchmetrics.functional.pearson_corrcoef,\n",
    "            grouping=\"cell_lines\",\n",
    "            average=\"macro\",\n",
    "            residualize=False\n",
    "        ),\n",
    "        \"MSE\": torchmetrics.MeanSquaredError()\n",
    "    }))\n",
    "    metrics.to(device)\n",
    "\n",
    "    num_epochs = config[\"env\"][\"max_epochs\"]\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        ### 1️⃣ Training Step ###\n",
    "        train_loss_pred, train_loss_recon = chain_train_step(autoencoder, resnet, optimizer, train_loader, config, device)\n",
    "        total_train_loss = train_loss_pred + train_loss_recon\n",
    "            \n",
    "        # Learning Rate Scheduler 업데이트\n",
    "        lr_scheduler.step(total_train_loss)\n",
    "\n",
    "        ### 2️⃣ Validation Step ###\n",
    "        if validation_dataset is not None:\n",
    "            with torch.no_grad():\n",
    "                validation_metrics = evaluate_step(autoencoder, resnet, val_loader, metrics, device)\n",
    "    \n",
    "                # Momentum을 사용한 Validation Metric 업데이트\n",
    "                if epoch > 0 and use_momentum:\n",
    "                    val_target = 0.2 * val_target + 0.8 * validation_metrics['R_cellwise_residuals']\n",
    "                else:\n",
    "                    val_target = validation_metrics['R_cellwise_residuals']\n",
    "        else:\n",
    "            val_target = None\n",
    "        \n",
    "        # 로그 출력\n",
    "        if callback_epoch is None:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {total_train_loss} | Validation R: {val_target}\")\n",
    "        else:\n",
    "            callback_epoch(epoch, val_target)\n",
    "\n",
    "        # Early Stopping 체크\n",
    "        if early_stop(total_train_loss):\n",
    "            print(\"⏹ Early Stopping Triggered. Stopping Training.\")\n",
    "            break\n",
    "\n",
    "    print(\"✅ Training Complete!\")\n",
    "\n",
    "    return val_target, autoencoder, resnet\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "423a3b62-5c29-4360-8eab-265db337cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation step\n",
    "def evaluate_step(autoencoder, resnet, loader, metrics, device):\n",
    "    metrics.increment()\n",
    "    autoencoder.eval()  # Autoencoder Evaluation Mode\n",
    "    resnet.eval()  # ResNet Evaluation Mode\n",
    "\n",
    "    predictions = {\"cell_line\": [], \"drug_id\": [], \"prediction\": [], \"target\": []}\n",
    "\n",
    "    for batch in loader:\n",
    "        with torch.no_grad():\n",
    "            _, latent_features = autoencoder(batch[0].to(device))  # Autoencoder에서 Latent Feature 생성\n",
    "            out = resnet(latent_features, batch[1].to(device))  # ResNet 예측\n",
    "\n",
    "            metrics.update(out.squeeze(),\n",
    "                           batch[2].to(device).squeeze(),\n",
    "                           cell_lines=batch[3].to(device).squeeze(),\n",
    "                           drugs=batch[4].to(device).squeeze())\n",
    "\n",
    "            # 결과 저장\n",
    "            predictions[\"cell_line\"].extend(batch[3].squeeze().tolist())  \n",
    "            predictions[\"drug_id\"].extend(batch[4].squeeze().cpu().tolist())    \n",
    "            predictions[\"prediction\"].extend(out.squeeze().tolist()) \n",
    "            predictions[\"target\"].extend(batch[2].squeeze().cpu().tolist())    \n",
    "\n",
    "    return {it[0]: it[1].item() for it in metrics.compute().items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f0d3fae-2e79-4c4f-b99f-31088038ff49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# config\n",
    "config = {\"features\" : {\"fp_radius\":2,\n",
    "                        \"use_correlation_representation\": True,\n",
    "                        \"num_modalities\": 4},\n",
    "          \"optimizer\": {\"batch_size\": 220,\n",
    "                        \"clip_norm\":19,\n",
    "                        \"learning_rate\": 0.0004592646200179472,\n",
    "                        \"stopping_patience\":15,\n",
    "                        \"pre_batch_size\": 200,\n",
    "                        \"lr_pretraining\": 0.0004592646200179472},\n",
    "          \"model\":{\"embed_dim\":485, # shared\n",
    "                 \"hidden_dim\":696, \n",
    "                 \"dropout\":0.48541242824674574, \n",
    "                 \"n_layers\": 4, \n",
    "                 \"norm\": \"batchnorm\", \n",
    "                 \"hidden_dim_encoders\": 256, # ENCODER\n",
    "                 \"fusion_dim\": 700, # ENCODER\n",
    "                 \"dropout_encoders\": 0.2,\n",
    "                 \"dropout_omics\": 0.4, # second\n",
    "                 \"dropout_omics_finetuning\": 0.4, # first\n",
    "                 \"pre_training_epochs\": 100}, \n",
    "         \"env\": {\"fold\": 0,  \n",
    "                \"device\":\"cuda:0\", \n",
    "                 \"max_epochs\": 100, \n",
    "                 \"search_hyperparameters\":False}} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f618a072-0eaa-4603-905d-ca49bd5ed2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Hyperparameters: {'embed_dim': 493, 'hidden_dim_encoders': 868, 'fusion_dim': 592, 'dropout_encoders': 0.2670033232985919, 'pre_training_epochs': 160, 'hidden_dim': 1982, 'dropout': 0.2965716546354523, 'n_layers': 5, 'dropout_omics': 0.8626326149215306, 'dropout_omics_finetuning': 0.5489897453017749, 'batch_size': 261, 'clip_norm': 10, 'learning_rate': 3.3761769665931243e-06, 'lr_pretraining': 1.435539966576822e-06, 'pre_batch_size': 81}\n",
    "# new best config\n",
    "# config\n",
    "config = {\"features\" : {\"fp_radius\":2,\n",
    "                        \"use_correlation_representation\": True,\n",
    "                        \"num_modalities\": 4},\n",
    "          \"optimizer\": {\"batch_size\": 261,\n",
    "                        \"clip_norm\":10,\n",
    "                        \"learning_rate\": 3.3761769665931243e-06,\n",
    "                        \"stopping_patience\":15,\n",
    "                        \"pre_batch_size\": 81,\n",
    "                        \"lr_pretraining\": 1.435539966576822e-06},\n",
    "          \"model\":{\"embed_dim\":493, # shared\n",
    "                 \"hidden_dim\":1982, \n",
    "                 \"dropout\":0.2965716546354523, \n",
    "                 \"n_layers\": 5, \n",
    "                 \"norm\": \"batchnorm\", \n",
    "                 \"hidden_dim_encoders\": 868, # ENCODER\n",
    "                 \"fusion_dim\": 592, # ENCODER\n",
    "                 \"dropout_encoders\": 0.2670033232985919,\n",
    "                 \"dropout_omics\": 0.8626326149215306, # second\n",
    "                 \"dropout_omics_finetuning\": 0.5489897453017749, # first\n",
    "                 \"pre_training_epochs\": 160}, \n",
    "         \"env\": {\"fold\": 0,  \n",
    "                \"device\":\"cuda:0\", \n",
    "                 \"max_epochs\": 100, \n",
    "                 \"search_hyperparameters\":False}} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a2735-570b-4331-a27b-616ca1182b72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fbb24a9-529a-42db-bc7e-c03f7029833e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train_model_optuna(trial, config):\\n\\n    ### 1️⃣ Pre-training 하이퍼파라미터 샘플링 ###\\n    config[\"model\"][\"embed_dim\"] = trial.suggest_int(\"embed_dim\", 64, 512)\\n    config[\"model\"][\"hidden_dim_encoders\"] = trial.suggest_int(\"hidden_dim_encoders\", 64, 2048)\\n    config[\"model\"][\"fusion_dim\"] = trial.suggest_int(\"fusion_dim\", 64, 2048)\\n    config[\"model\"][\"dropout_encoders\"] = trial.suggest_float(\"dropout_encoders\", 0.0, 0.5)\\n    config[\"model\"][\"pre_training_epochs\"] = trial.suggest_int(\"pre_training_epochs\", 1, 500)\\n    config[\"optimizer\"][\"pre_batch_size\"] = trial.suggest_int(\"pre_batch_size\", 128, 512)\\n    config[\"optimizer\"][\"lr_pretraining\"] = trial.suggest_float(\"lr_pretraining\", 1e-6, 1e-1, log=True)\\n\\n    ### 2️⃣ Pre-training 실행 ###\\n    try:\\n        #pre_raw_dataset = torch.utils.data.ConcatDataset([pre_raw_train_dataset, pre_raw_val_dataset])\\n        pre_trained_model, pre_training_losses = pre_train_step(config, pre_raw_train_dataset, input_dims=pre_raw_dims)\\n\\n        # Pre-training 동안의 최소 loss 반환 (낮을수록 좋음)\\n        pre_training_score = min(pre_training_losses)\\n        print(f\"Trial {trial.number}: Pre-training Loss = {pre_training_score:.5f}\")\\n\\n    except Exception as e:\\n        print(f\"Pre-training Error: {e}\")\\n        return float(\"inf\")  # Pre-training 실패 시, 최대 loss 반환\\n\\n    ### 3️⃣ Fine-tuning 하이퍼파라미터 샘플링 ###\\n    config[\"model\"][\"dropout_omics\"] = trial.suggest_float(\"dropout_omics\", 0.0, 0.9)\\n    config[\"model\"][\"dropout_omics_finetuning\"] = trial.suggest_float(\"dropout_omics_finetuning\", 0.0, 0.9)\\n\\n    ### 4️⃣ Fine-tuning 실행 (Latent Representation 생성 포함) ###\\n    try:\\n        fine_tuned_model, val_target = finetune_model_train(\\n            config, fine_raw_train_dataset, pre_trained_model, fine_raw_val_dataset\\n        )\\n\\n        print(f\"Trial {trial.number}: Fine-tuning R_cellwise_residuals = {val_target:.5f}\")\\n        return val_target  # Fine-tuning 성능 반환 (높을수록 좋음)\\n\\n    except Exception as e:\\n        print(f\"Fine-tuning Error: {e}\")\\n        return -float(\"inf\")  # Fine-tuning 실패 시, 최소 성능 반환\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def train_model_optuna(trial, config):\n",
    "    \"\"\"\n",
    "    Optuna를 활용한 하이퍼파라미터 최적화 함수\n",
    "    - Autoencoder + ResNet을 함께 최적화\n",
    "    \"\"\"\n",
    "\n",
    "    ### 1️⃣ 하이퍼파라미터 샘플링 ###\n",
    "    # Autoencoder 관련\n",
    "    config[\"model\"][\"embed_dim\"] = trial.suggest_int(\"embed_dim\", 64, 512)\n",
    "    config[\"model\"][\"hidden_dim_encoders\"] = trial.suggest_int(\"hidden_dim_encoders\", 64, 1024)\n",
    "    config[\"model\"][\"fusion_dim\"] = trial.suggest_int(\"fusion_dim\", 64, 1024)\n",
    "    config[\"model\"][\"dropout_encoders\"] = trial.suggest_float(\"dropout_encoders\", 0.1, 0.5)\n",
    "    config[\"model\"][\"pre_training_epochs\"] = trial.suggest_int(\"pre_training_epochs\", 10, 200)\n",
    "\n",
    "    # ResNet 관련\n",
    "    config[\"model\"][\"hidden_dim\"] = trial.suggest_int(\"hidden_dim\", 256, 2048)\n",
    "    config[\"model\"][\"dropout\"] = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    config[\"model\"][\"n_layers\"] = trial.suggest_int(\"n_layers\", 2, 10)\n",
    "    config[\"model\"][\"dropout_omics\"] = trial.suggest_float(\"dropout_omics\", 0.1, 0.9)\n",
    "    config[\"model\"][\"dropout_omics_finetuning\"] = trial.suggest_float(\"dropout_omics_finetuning\", 0.1, 0.9)\n",
    "\n",
    "    # Optimizer 관련\n",
    "    config[\"optimizer\"][\"batch_size\"] = trial.suggest_int(\"batch_size\", 64, 512)\n",
    "    config[\"optimizer\"][\"clip_norm\"] = trial.suggest_int(\"clip_norm\", 5, 20)\n",
    "    config[\"optimizer\"][\"learning_rate\"] = trial.suggest_float(\"learning_rate\", 1e-6, 1e-2, log=True)\n",
    "    config[\"optimizer\"][\"lr_pretraining\"] = trial.suggest_float(\"lr_pretraining\", 1e-6, 1e-2, log=True)\n",
    "    config[\"optimizer\"][\"pre_batch_size\"] = trial.suggest_int(\"pre_batch_size\", 64, 512)\n",
    "\n",
    "    ### 2️⃣ 데이터 로딩 ###\n",
    "    train_dataset, val_dataset, test_dataset, input_dims = get_data_corr(\n",
    "        n_fold=config[\"env\"][\"fold\"],\n",
    "        fp_radius=config[\"features\"][\"fp_radius\"],\n",
    "        transform_into_corr=config[\"features\"][\"use_correlation_representation\"],\n",
    "        typ=(\"rnaseq\", \"proteomics\", \"mutations\", \"methylations\")\n",
    "    )\n",
    "\n",
    "    ### 3️⃣ 모델 학습 (Autoencoder + ResNet 동시 학습) ###\n",
    "    try:\n",
    "        val_target, autoencoder, resnet = train_auto_resnet_chain(\n",
    "            config, train_dataset, val_dataset, use_momentum=True\n",
    "        )\n",
    "        print(f\"Trial {trial.number}: Validation R_cellwise_residuals = {val_target:.5f}\")\n",
    "\n",
    "        return val_target  # 최적화 목표 (Validation R값 최대화)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in trial {trial.number}: {e}\")\n",
    "        return -float(\"inf\")  # 실패 시 최소 값 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa8f8e12-d0e9-4395-90a2-bd181ae56cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 1️⃣ Optuna를 사용한 Hyperparameter Optimization (Validation 데이터 필요)\\nif config[\"env\"][\"search_hyperparameters\"]:\\n    study_name = \"raw_pretrained\"\\n    storage_name = f\"sqlite:///studies/{study_name}.db\"\\n\\n    study = optuna.create_study(\\n        study_name=study_name,\\n        storage=storage_name,\\n        direction=\\'maximize\\',  # Fine-tuning 성능을 최적화 (R_cellwise_residuals 최대화)\\n        load_if_exists=True,\\n        pruner=optuna.pruners.MedianPruner(n_startup_trials=30, n_warmup_steps=5, interval_steps=5)\\n    )\\n\\n    study.optimize(lambda trial: train_model_optuna(trial, config), n_trials=40)\\n\\n    best_config = study.best_params\\n    print(\"Best Hyperparameters:\", best_config)\\n\\n    # 최적의 하이퍼파라미터를 config에 반영\\n    config[\"model\"][\"embed_dim\"] = best_config[\"embed_dim\"]\\n    config[\"model\"][\"hidden_dim_encoders\"] = best_config[\"hidden_dim_encoders\"]\\n    config[\"model\"][\"fusion_dim\"] = best_config[\"fusion_dim\"]\\n    config[\"model\"][\"dropout_encoders\"] = best_config[\"dropout_encoders\"]\\n    config[\"model\"][\"pre_training_epochs\"] = best_config[\"pre_training_epochs\"]\\n    config[\"optimizer\"][\"pre_batch_size\"] = best_config[\"pre_batch_size\"]\\n    config[\"optimizer\"][\"lr_pretraining\"] = best_config[\"lr_pretraining\"]\\n    config[\"model\"][\"dropout_omics\"] = best_config[\"dropout_omics\"]\\n    config[\"model\"][\"dropout_omics_finetuning\"] = best_config[\"dropout_omics_finetuning\"]\\n\\n# 2️⃣ 최적 하이퍼파라미터를 적용한 후, Training (Train + Validation 합쳐서 사용)\\npre_raw_dataset = torch.utils.data.ConcatDataset([pre_raw_train_dataset, pre_raw_val_dataset])\\n\\n# Pre-training 실행\\npre_trained_model, pre_training_losses = pre_train_step(config, pre_raw_dataset, input_dims=pre_raw_dims)\\n\\n# Fine-tuning 실행 (이제 Train + Validation을 합쳐서 사용)\\nfine_tuned_model, final_target = finetune_model_train(\\n    config, pre_raw_dataset, pre_trained_model, validation_dataset=None  # ✅ Validation 없이 학습\\n)\\n\\nprint(f\"Final Training Complete! Final R_cellwise_residuals: {final_target:.5f}\")\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if config[\"env\"][\"search_hyperparameters\"]:\n",
    "    study_name = \"auto_resnet_chain_opt\"\n",
    "    storage_name = f\"sqlite:///studies/{study_name}.db\"\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=storage_name,\n",
    "        direction='maximize',  # Validation R 값을 최대화\n",
    "        load_if_exists=True,\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=30, n_warmup_steps=5, interval_steps=5)\n",
    "    )\n",
    "\n",
    "    study.optimize(lambda trial: train_model_optuna(trial, config), n_trials=40)\n",
    "\n",
    "    best_config = study.best_params\n",
    "    print(\"Best Hyperparameters:\", best_config)\n",
    "\n",
    "    # 최적의 하이퍼파라미터를 config에 반영\n",
    "    config[\"model\"][\"embed_dim\"] = best_config[\"embed_dim\"]\n",
    "    config[\"model\"][\"hidden_dim_encoders\"] = best_config[\"hidden_dim_encoders\"]\n",
    "    config[\"model\"][\"fusion_dim\"] = best_config[\"fusion_dim\"]\n",
    "    config[\"model\"][\"dropout_encoders\"] = best_config[\"dropout_encoders\"]\n",
    "    config[\"model\"][\"pre_training_epochs\"] = best_config[\"pre_training_epochs\"]\n",
    "    config[\"model\"][\"hidden_dim\"] = best_config[\"hidden_dim\"]\n",
    "    config[\"model\"][\"dropout\"] = best_config[\"dropout\"]\n",
    "    config[\"model\"][\"n_layers\"] = best_config[\"n_layers\"]\n",
    "    config[\"model\"][\"dropout_omics\"] = best_config[\"dropout_omics\"]\n",
    "    config[\"model\"][\"dropout_omics_finetuning\"] = best_config[\"dropout_omics_finetuning\"]\n",
    "    config[\"optimizer\"][\"batch_size\"] = best_config[\"batch_size\"]\n",
    "    config[\"optimizer\"][\"clip_norm\"] = best_config[\"clip_norm\"]\n",
    "    config[\"optimizer\"][\"learning_rate\"] = best_config[\"learning_rate\"]\n",
    "    config[\"optimizer\"][\"lr_pretraining\"] = best_config[\"lr_pretraining\"]\n",
    "    config[\"optimizer\"][\"pre_batch_size\"] = best_config[\"pre_batch_size\"]\n",
    "\n",
    "# 2️⃣ 최적 하이퍼파라미터를 적용한 후 Training 실행\n",
    "train_dataset, val_dataset, test_dataset, input_dims = get_data_corr(\n",
    "    n_fold=config[\"env\"][\"fold\"],\n",
    "    fp_radius=config[\"features\"][\"fp_radius\"],\n",
    "    transform_into_corr=config[\"features\"][\"use_correlation_representation\"],\n",
    "    typ=(\"rnaseq\", \"proteomics\", \"mutations\", \"methylations\")\n",
    ")\n",
    "\n",
    "val_target, autoencoder, resnet = train_auto_resnet_chain(config, train_dataset, val_dataset, use_momentum=True)\n",
    "\n",
    "print(f\"Final Training Complete! Final R_cellwise_residuals: {val_target:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62307b7-6612-43b1-a2b6-8cc7d48c9f02",
   "metadata": {},
   "source": [
    "# Model training \n",
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af043824-988a-437e-8b6b-df4e9133ce0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:51] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[23:11:52] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "# data loading with raw data\n",
    "train_dataset, val_dataset, test_dataset, input_dims= get_data_corr(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"], \n",
    "                                                           transform_into_corr = False,\n",
    "                                                           typ = (\"rnaseq\", \"proteomics\", \"mutations\", \"methylations\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f544e44c-523b-4ee0-99a5-ea8ec63e789f",
   "metadata": {},
   "source": [
    "여기서 Autoencoder의 evaluation은 수행하지 않는다. Autoencoder를 단독으로 평가하는 것이 Test distribution과 맞지 않을 수 있기 때문이다. 여기서 이걸 먼저 평가하게 되면 Test set의 정보가 노출되어 편향된 결과가 나타날 수 있다. \n",
    "\n",
    "또한, 단순 Reconstruction Loss로 이를 중간에 평가하는 것이 애초에 의미가 없을 가능성이 높다. \n",
    "\n",
    "그렇다면, 두 모델을 따로 트레이닝 할 것인지, 한번에 트레이닝 할 것인지 정해야겠다. \n",
    "\n",
    "1. Autoencoder pretraining -> encoder frozen, used as feature extractor -> Resnet training\n",
    "   - Autoencoder가 단순 Reconstruction Loss로 학습되므로, ResNet에 필요한 Feature를 충분히 학습하지 못할 가능성이 있음.\n",
    "3. Autoencoder -> latent feature -> Resnet\n",
    "   - 설계가 복잡해짐.\n",
    "   - 두 모델을 한번에 트레이닝하므로, 학습이 불안정할 수 있음. \n",
    "\n",
    "\n",
    "둘다 해볼건데, 여기서는 2번을 할거다. 간략적인 플로우는 다음과 같다.\n",
    "\n",
    "- autoencoder와 resnet을 동시에 학습.\n",
    "- 이 과정에서 reconstruction loss와 prediction loss를 합산하여 backpropagation한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84584108-f676-4d60-98c1-adccbc220f0a",
   "metadata": {},
   "source": [
    "## Autoencoder training\n",
    "\n",
    "여기서는 Autoencoder와 Resnet을 한 시스템으로 묶어버리도록 하겠다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c552b7a5-c3ad-4c21-b221-88d772f67552",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 4.755145411885786 | Validation R: None\n",
      "Epoch 2/100 | Train Loss: 2.0263939628552516 | Validation R: None\n",
      "Epoch 3/100 | Train Loss: 1.9226895056460565 | Validation R: None\n",
      "Epoch 4/100 | Train Loss: 1.8374938751545333 | Validation R: None\n",
      "Epoch 5/100 | Train Loss: 1.7872781598335092 | Validation R: None\n",
      "Epoch 6/100 | Train Loss: 1.7407941228384192 | Validation R: None\n",
      "Epoch 7/100 | Train Loss: 1.7052333491698761 | Validation R: None\n",
      "Epoch 8/100 | Train Loss: 1.681087858386567 | Validation R: None\n",
      "Epoch 9/100 | Train Loss: 1.6516902321269884 | Validation R: None\n",
      "Epoch 10/100 | Train Loss: 1.6366012364824116 | Validation R: None\n",
      "Epoch 11/100 | Train Loss: 1.6168349851991173 | Validation R: None\n",
      "Epoch 12/100 | Train Loss: 1.60428890237834 | Validation R: None\n",
      "Epoch 13/100 | Train Loss: 1.5756864525681222 | Validation R: None\n",
      "Epoch 14/100 | Train Loss: 1.5626385465933201 | Validation R: None\n",
      "Epoch 15/100 | Train Loss: 1.5503592393976495 | Validation R: None\n",
      "Epoch 16/100 | Train Loss: 1.536023580658862 | Validation R: None\n",
      "Epoch 17/100 | Train Loss: 1.5245412695326115 | Validation R: None\n",
      "Epoch 18/100 | Train Loss: 1.5017158673365514 | Validation R: None\n",
      "Epoch 19/100 | Train Loss: 1.4898528548597 | Validation R: None\n",
      "Epoch 20/100 | Train Loss: 1.4745665550269094 | Validation R: None\n",
      "Epoch 21/100 | Train Loss: 1.4668585877750977 | Validation R: None\n",
      "Epoch 22/100 | Train Loss: 1.4526893386891357 | Validation R: None\n",
      "Epoch 23/100 | Train Loss: 1.4403877429918062 | Validation R: None\n",
      "Epoch 24/100 | Train Loss: 1.426571150856125 | Validation R: None\n",
      "Epoch 25/100 | Train Loss: 1.416127781878386 | Validation R: None\n",
      "Epoch 26/100 | Train Loss: 1.4063792253100464 | Validation R: None\n",
      "Epoch 27/100 | Train Loss: 1.392326978887629 | Validation R: None\n",
      "Epoch 28/100 | Train Loss: 1.3834167781656765 | Validation R: None\n",
      "Epoch 29/100 | Train Loss: 1.373038954701482 | Validation R: None\n",
      "Epoch 30/100 | Train Loss: 1.3590706894305826 | Validation R: None\n",
      "Epoch 31/100 | Train Loss: 1.353128925080435 | Validation R: None\n",
      "Epoch 32/100 | Train Loss: 1.3363244762537336 | Validation R: None\n",
      "Epoch 33/100 | Train Loss: 1.325744030983222 | Validation R: None\n",
      "Epoch 34/100 | Train Loss: 1.3117259580607923 | Validation R: None\n",
      "Epoch 35/100 | Train Loss: 1.3016290213798147 | Validation R: None\n",
      "Epoch 36/100 | Train Loss: 1.2902653988323802 | Validation R: None\n",
      "Epoch 37/100 | Train Loss: 1.2864400305578536 | Validation R: None\n",
      "Epoch 38/100 | Train Loss: 1.2775681693128567 | Validation R: None\n",
      "Epoch 39/100 | Train Loss: 1.2674377485800068 | Validation R: None\n",
      "Epoch 40/100 | Train Loss: 1.2642889026574113 | Validation R: None\n",
      "Epoch 41/100 | Train Loss: 1.2598521797015878 | Validation R: None\n",
      "Epoch 42/100 | Train Loss: 1.2538147878524222 | Validation R: None\n",
      "Epoch 43/100 | Train Loss: 1.2465347083686469 | Validation R: None\n",
      "Epoch 44/100 | Train Loss: 1.2405427735676666 | Validation R: None\n",
      "Epoch 45/100 | Train Loss: 1.2354081339688223 | Validation R: None\n",
      "Epoch 46/100 | Train Loss: 1.2359275518980803 | Validation R: None\n",
      "Epoch 47/100 | Train Loss: 1.2246593092959175 | Validation R: None\n",
      "Epoch 48/100 | Train Loss: 1.2192120249596048 | Validation R: None\n",
      "Epoch 49/100 | Train Loss: 1.2166137877628764 | Validation R: None\n",
      "Epoch 50/100 | Train Loss: 1.2143041724733954 | Validation R: None\n",
      "Epoch 51/100 | Train Loss: 1.2092201104256033 | Validation R: None\n",
      "Epoch 52/100 | Train Loss: 1.2049298002458149 | Validation R: None\n",
      "Epoch 53/100 | Train Loss: 1.197093594343969 | Validation R: None\n",
      "Epoch 54/100 | Train Loss: 1.1949227683088204 | Validation R: None\n",
      "Epoch 55/100 | Train Loss: 1.1927786526252406 | Validation R: None\n",
      "Epoch 56/100 | Train Loss: 1.1834800268414238 | Validation R: None\n",
      "Epoch 57/100 | Train Loss: 1.182102390921869 | Validation R: None\n",
      "Epoch 58/100 | Train Loss: 1.1800066085295413 | Validation R: None\n",
      "Epoch 59/100 | Train Loss: 1.1771203742787577 | Validation R: None\n",
      "Epoch 60/100 | Train Loss: 1.1704693974064646 | Validation R: None\n",
      "Epoch 61/100 | Train Loss: 1.1666876252582692 | Validation R: None\n",
      "Epoch 62/100 | Train Loss: 1.1601051629419536 | Validation R: None\n",
      "Epoch 63/100 | Train Loss: 1.1576848130809423 | Validation R: None\n",
      "Epoch 64/100 | Train Loss: 1.15438620375155 | Validation R: None\n",
      "Epoch 65/100 | Train Loss: 1.1554345642189408 | Validation R: None\n",
      "Epoch 66/100 | Train Loss: 1.1504075861275074 | Validation R: None\n",
      "Epoch 67/100 | Train Loss: 1.1485248240622894 | Validation R: None\n",
      "Epoch 68/100 | Train Loss: 1.142526780307008 | Validation R: None\n",
      "Epoch 69/100 | Train Loss: 1.1394539480674346 | Validation R: None\n",
      "Epoch 70/100 | Train Loss: 1.1361997662606036 | Validation R: None\n",
      "Epoch 71/100 | Train Loss: 1.1362391296102172 | Validation R: None\n",
      "Epoch 72/100 | Train Loss: 1.1264766728085893 | Validation R: None\n",
      "Epoch 73/100 | Train Loss: 1.131292244343587 | Validation R: None\n",
      "Epoch 74/100 | Train Loss: 1.1278413708042276 | Validation R: None\n",
      "Epoch 75/100 | Train Loss: 1.1236843093161348 | Validation R: None\n",
      "Epoch 76/100 | Train Loss: 1.1193685034554721 | Validation R: None\n",
      "Epoch 77/100 | Train Loss: 1.1152875062917433 | Validation R: None\n",
      "Epoch 78/100 | Train Loss: 1.1121863308869742 | Validation R: None\n",
      "Epoch 79/100 | Train Loss: 1.113420141318926 | Validation R: None\n",
      "Epoch 80/100 | Train Loss: 1.1087296530495978 | Validation R: None\n",
      "Epoch 81/100 | Train Loss: 1.1071674847659514 | Validation R: None\n",
      "Epoch 82/100 | Train Loss: 1.1014132505586234 | Validation R: None\n",
      "Epoch 83/100 | Train Loss: 1.1008960984760303 | Validation R: None\n",
      "Epoch 84/100 | Train Loss: 1.0971599063237607 | Validation R: None\n",
      "Epoch 85/100 | Train Loss: 1.095443833219112 | Validation R: None\n",
      "Epoch 86/100 | Train Loss: 1.0934059730779722 | Validation R: None\n",
      "Epoch 87/100 | Train Loss: 1.0893661405640351 | Validation R: None\n",
      "Epoch 88/100 | Train Loss: 1.0893510869717349 | Validation R: None\n",
      "Epoch 89/100 | Train Loss: 1.0858734380879618 | Validation R: None\n",
      "Epoch 90/100 | Train Loss: 1.0807053795121515 | Validation R: None\n",
      "Epoch 91/100 | Train Loss: 1.0813281568129574 | Validation R: None\n",
      "Epoch 92/100 | Train Loss: 1.08210682459733 | Validation R: None\n",
      "Epoch 93/100 | Train Loss: 1.0780783213167167 | Validation R: None\n",
      "Epoch 94/100 | Train Loss: 1.0723585129908269 | Validation R: None\n",
      "Epoch 95/100 | Train Loss: 1.072091292474552 | Validation R: None\n",
      "Epoch 96/100 | Train Loss: 1.073527889332408 | Validation R: None\n",
      "Epoch 97/100 | Train Loss: 1.0659354481883254 | Validation R: None\n",
      "Epoch 98/100 | Train Loss: 1.0698581346638147 | Validation R: None\n",
      "Epoch 99/100 | Train Loss: 1.0662165230894127 | Validation R: None\n",
      "Epoch 100/100 | Train Loss: 1.0606577044325018 | Validation R: None\n",
      "✅ Training Complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_, autoencoder, resnet = train_auto_resnet_chain(config, torch.utils.data.ConcatDataset([train_dataset, val_dataset]), validation_dataset=None, use_momentum=True, callback_epoch=None)\n",
    "\n",
    "metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(\n",
    "    {\"R_cellwise_residuals\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"drugs\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=True),\n",
    "    \"R_cellwise\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"cell_lines\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=False),\n",
    "    \"MSE\":torchmetrics.MeanSquaredError()}))\n",
    "device = torch.device(config[\"env\"][\"device\"])\n",
    "metrics.to(device)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                       batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                       drop_last=False,\n",
    "                                      shuffle=False,\n",
    "                                      pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2949c0b-2267-4732-a146-4ea178117fdf",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "트레이닝된 Autoencoder의 encoder와, Resnet을 하나의 시스템으로 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f5d20ff-ce7c-414f-8aa2-f36ab25ced43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/kim14/project_work/scripts/models.py:69: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/TensorShape.cpp:3277.)\n",
      "  return torch.linalg.solve(A, Xy).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main model final metrics: {'MSE': 1.5947002172470093, 'R_cellwise': 0.8947927355766296, 'R_cellwise_residuals': 0.3000485599040985}\n"
     ]
    }
   ],
   "source": [
    "autoencoder_resnet_chain = evaluate_step(autoencoder, resnet, test_dataloader, metrics, device)\n",
    "print(f\"main model final metrics: {autoencoder_resnet_chain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c576a-1470-4db3-bac0-f10847c91d26",
   "metadata": {},
   "source": [
    "# 디버깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38cc9f8a-e5d9-49e5-8179-71ba691c9190",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 오류 발생한 Trial 목록:\n",
      "\n",
      "🔹 Trial 0:\n",
      "   [I 2025-03-05 03:04:33,360] Trial 0 finished with value: -inf and parameters: {'embed_dim': 160, 'hidden_dim_encoders': 383, 'fusion_dim': 536, 'dropout_encoders': 0.3968670519414298, 'pre_training_epochs': 57, 'hidden_dim': 1972, 'dropout': 0.2785211502629435, 'n_layers': 8, 'dropout_omics': 0.13501467796110936, 'dropout_omics_finetuning': 0.520701483169283, 'batch_size': 448, 'clip_norm': 11, 'learning_rate': 0.00012791306882120737, 'lr_pretraining': 0.0005278582460096422, 'pre_batch_size': 212}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 1:\n",
      "   [I 2025-03-05 03:04:48,224] Trial 1 finished with value: -inf and parameters: {'embed_dim': 277, 'hidden_dim_encoders': 813, 'fusion_dim': 546, 'dropout_encoders': 0.29253026920656744, 'pre_training_epochs': 135, 'hidden_dim': 1684, 'dropout': 0.417402013600224, 'n_layers': 3, 'dropout_omics': 0.5554952178297965, 'dropout_omics_finetuning': 0.7135591469446555, 'batch_size': 233, 'clip_norm': 5, 'learning_rate': 2.7725748169172126e-06, 'lr_pretraining': 1.061500516920679e-06, 'pre_batch_size': 508}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 2:\n",
      "   [I 2025-03-05 03:05:06,145] Trial 2 finished with value: -inf and parameters: {'embed_dim': 301, 'hidden_dim_encoders': 133, 'fusion_dim': 284, 'dropout_encoders': 0.12657029467587477, 'pre_training_epochs': 199, 'hidden_dim': 1183, 'dropout': 0.3994236840948645, 'n_layers': 3, 'dropout_omics': 0.4081251135038674, 'dropout_omics_finetuning': 0.14240124476155494, 'batch_size': 148, 'clip_norm': 5, 'learning_rate': 8.445168844760818e-06, 'lr_pretraining': 4.063609724038821e-06, 'pre_batch_size': 480}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 3:\n",
      "   [I 2025-03-05 03:05:25,283] Trial 3 finished with value: -inf and parameters: {'embed_dim': 482, 'hidden_dim_encoders': 532, 'fusion_dim': 562, 'dropout_encoders': 0.275398877936193, 'pre_training_epochs': 16, 'hidden_dim': 1555, 'dropout': 0.3413364888051449, 'n_layers': 10, 'dropout_omics': 0.7129212919075564, 'dropout_omics_finetuning': 0.6441624108312314, 'batch_size': 244, 'clip_norm': 16, 'learning_rate': 3.5262346432967544e-06, 'lr_pretraining': 0.0012844906010005654, 'pre_batch_size': 450}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 4:\n",
      "   [I 2025-03-05 03:05:41,977] Trial 4 finished with value: -inf and parameters: {'embed_dim': 68, 'hidden_dim_encoders': 293, 'fusion_dim': 654, 'dropout_encoders': 0.2312920427309344, 'pre_training_epochs': 134, 'hidden_dim': 1077, 'dropout': 0.4804659746344885, 'n_layers': 9, 'dropout_omics': 0.5947538447783477, 'dropout_omics_finetuning': 0.5461917487884439, 'batch_size': 295, 'clip_norm': 13, 'learning_rate': 0.00026285114784738086, 'lr_pretraining': 0.0001187847377626669, 'pre_batch_size': 110}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 5:\n",
      "   [I 2025-03-05 03:06:03,537] Trial 5 finished with value: -inf and parameters: {'embed_dim': 265, 'hidden_dim_encoders': 464, 'fusion_dim': 837, 'dropout_encoders': 0.4058209824311999, 'pre_training_epochs': 53, 'hidden_dim': 1936, 'dropout': 0.4114051937905162, 'n_layers': 6, 'dropout_omics': 0.5641465141540086, 'dropout_omics_finetuning': 0.5185500530353584, 'batch_size': 146, 'clip_norm': 15, 'learning_rate': 4.212787012395409e-06, 'lr_pretraining': 0.0002090736193330708, 'pre_batch_size': 262}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 6:\n",
      "   [I 2025-03-05 03:06:15,631] Trial 6 finished with value: -inf and parameters: {'embed_dim': 240, 'hidden_dim_encoders': 159, 'fusion_dim': 283, 'dropout_encoders': 0.12334824941306688, 'pre_training_epochs': 63, 'hidden_dim': 1381, 'dropout': 0.3365209107935092, 'n_layers': 2, 'dropout_omics': 0.3722332720055397, 'dropout_omics_finetuning': 0.7936590438646969, 'batch_size': 373, 'clip_norm': 5, 'learning_rate': 4.366111300967434e-06, 'lr_pretraining': 2.936164050410941e-06, 'pre_batch_size': 253}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 7:\n",
      "   [I 2025-03-05 03:06:44,284] Trial 7 finished with value: -inf and parameters: {'embed_dim': 416, 'hidden_dim_encoders': 628, 'fusion_dim': 667, 'dropout_encoders': 0.14712039964267856, 'pre_training_epochs': 198, 'hidden_dim': 1703, 'dropout': 0.1439192421288852, 'n_layers': 10, 'dropout_omics': 0.45063680108386905, 'dropout_omics_finetuning': 0.596004442975289, 'batch_size': 130, 'clip_norm': 18, 'learning_rate': 0.004521611868425931, 'lr_pretraining': 9.912183367723513e-06, 'pre_batch_size': 408}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 8:\n",
      "   [I 2025-03-05 03:07:09,371] Trial 8 finished with value: -inf and parameters: {'embed_dim': 498, 'hidden_dim_encoders': 834, 'fusion_dim': 793, 'dropout_encoders': 0.473105310909709, 'pre_training_epochs': 91, 'hidden_dim': 489, 'dropout': 0.41113462166846126, 'n_layers': 6, 'dropout_omics': 0.10525934904894037, 'dropout_omics_finetuning': 0.2491042826241895, 'batch_size': 116, 'clip_norm': 5, 'learning_rate': 0.006244544821234552, 'lr_pretraining': 4.407436302898326e-06, 'pre_batch_size': 351}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 9:\n",
      "   [I 2025-03-05 03:07:27,387] Trial 9 finished with value: -inf and parameters: {'embed_dim': 124, 'hidden_dim_encoders': 196, 'fusion_dim': 465, 'dropout_encoders': 0.18737946183355372, 'pre_training_epochs': 102, 'hidden_dim': 2036, 'dropout': 0.2686163451809076, 'n_layers': 7, 'dropout_omics': 0.24390262425795106, 'dropout_omics_finetuning': 0.4902631280694789, 'batch_size': 217, 'clip_norm': 9, 'learning_rate': 0.005675800867985232, 'lr_pretraining': 5.906771510324443e-06, 'pre_batch_size': 351}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 10:\n",
      "   [I 2025-03-05 03:07:40,836] Trial 10 finished with value: -inf and parameters: {'embed_dim': 173, 'hidden_dim_encoders': 372, 'fusion_dim': 83, 'dropout_encoders': 0.38007483415830207, 'pre_training_epochs': 13, 'hidden_dim': 753, 'dropout': 0.22552707665120325, 'n_layers': 8, 'dropout_omics': 0.8418777166164086, 'dropout_omics_finetuning': 0.358483790777233, 'batch_size': 511, 'clip_norm': 10, 'learning_rate': 9.223265182033362e-05, 'lr_pretraining': 0.00701217875088424, 'pre_batch_size': 140}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 11:\n",
      "   [I 2025-03-05 03:07:53,915] Trial 11 finished with value: -inf and parameters: {'embed_dim': 350, 'hidden_dim_encoders': 982, 'fusion_dim': 430, 'dropout_encoders': 0.3422694537375308, 'pre_training_epochs': 145, 'hidden_dim': 1760, 'dropout': 0.21997182642434188, 'n_layers': 4, 'dropout_omics': 0.10559337830547576, 'dropout_omics_finetuning': 0.8454949616546699, 'batch_size': 411, 'clip_norm': 9, 'learning_rate': 5.7336952233710985e-05, 'lr_pretraining': 0.0007316571263619493, 'pre_batch_size': 196}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 12:\n",
      "   [I 2025-03-05 03:08:08,058] Trial 12 finished with value: -inf and parameters: {'embed_dim': 190, 'hidden_dim_encoders': 720, 'fusion_dim': 1006, 'dropout_encoders': 0.30474219339508496, 'pre_training_epochs': 142, 'hidden_dim': 1447, 'dropout': 0.491045409422963, 'n_layers': 5, 'dropout_omics': 0.27689621643166656, 'dropout_omics_finetuning': 0.7216895666617417, 'batch_size': 341, 'clip_norm': 20, 'learning_rate': 0.0005088901847155197, 'lr_pretraining': 2.7057091519613792e-05, 'pre_batch_size': 184}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 13:\n",
      "   [I 2025-03-05 03:08:22,013] Trial 13 finished with value: -inf and parameters: {'embed_dim': 333, 'hidden_dim_encoders': 841, 'fusion_dim': 325, 'dropout_encoders': 0.472563310715196, 'pre_training_epochs': 63, 'hidden_dim': 1768, 'dropout': 0.13068585259701046, 'n_layers': 8, 'dropout_omics': 0.6798472944869036, 'dropout_omics_finetuning': 0.39945552986386623, 'batch_size': 477, 'clip_norm': 11, 'learning_rate': 2.3298851509739614e-05, 'lr_pretraining': 3.5075513881579656e-05, 'pre_batch_size': 320}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 14:\n",
      "   [I 2025-03-05 03:08:33,968] Trial 14 finished with value: -inf and parameters: {'embed_dim': 195, 'hidden_dim_encoders': 637, 'fusion_dim': 610, 'dropout_encoders': 0.4160981011497926, 'pre_training_epochs': 173, 'hidden_dim': 1952, 'dropout': 0.2879274536075557, 'n_layers': 2, 'dropout_omics': 0.2920932413667896, 'dropout_omics_finetuning': 0.699969963396724, 'batch_size': 433, 'clip_norm': 7, 'learning_rate': 0.0005985155488713132, 'lr_pretraining': 0.00044042983515152654, 'pre_batch_size': 505}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 15:\n",
      "   [I 2025-03-05 03:08:49,323] Trial 15 finished with value: -inf and parameters: {'embed_dim': 113, 'hidden_dim_encoders': 992, 'fusion_dim': 755, 'dropout_encoders': 0.32429888797167994, 'pre_training_epochs': 123, 'hidden_dim': 1148, 'dropout': 0.3496944570574987, 'n_layers': 4, 'dropout_omics': 0.8529773903599952, 'dropout_omics_finetuning': 0.8880168537037436, 'batch_size': 240, 'clip_norm': 13, 'learning_rate': 1.3322164391191336e-06, 'lr_pretraining': 0.003349397608852403, 'pre_batch_size': 70}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 16:\n",
      "   [I 2025-03-05 03:09:29,558] Trial 16 finished with value: -inf and parameters: {'embed_dim': 389, 'hidden_dim_encoders': 408, 'fusion_dim': 455, 'dropout_encoders': 0.25132957659968647, 'pre_training_epochs': 83, 'hidden_dim': 1616, 'dropout': 0.21156519820908937, 'n_layers': 7, 'dropout_omics': 0.5297035112921167, 'dropout_omics_finetuning': 0.38301506986896405, 'batch_size': 65, 'clip_norm': 7, 'learning_rate': 2.8431971140706442e-05, 'lr_pretraining': 1.1430628864437012e-06, 'pre_batch_size': 213}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 17:\n",
      "   [I 2025-03-05 03:09:43,992] Trial 17 finished with value: -inf and parameters: {'embed_dim': 231, 'hidden_dim_encoders': 808, 'fusion_dim': 936, 'dropout_encoders': 0.35589411210319705, 'pre_training_epochs': 40, 'hidden_dim': 871, 'dropout': 0.4434453348973263, 'n_layers': 5, 'dropout_omics': 0.6821930974485941, 'dropout_omics_finetuning': 0.7273332026976881, 'batch_size': 314, 'clip_norm': 7, 'learning_rate': 0.0013015334435043224, 'lr_pretraining': 4.390813336833186e-05, 'pre_batch_size': 410}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 18:\n",
      "   [I 2025-03-05 03:10:03,651] Trial 18 finished with value: -inf and parameters: {'embed_dim': 145, 'hidden_dim_encoders': 290, 'fusion_dim': 145, 'dropout_encoders': 0.41871822974106443, 'pre_training_epochs': 165, 'hidden_dim': 1384, 'dropout': 0.3659528362615089, 'n_layers': 8, 'dropout_omics': 0.20361620932295657, 'dropout_omics_finetuning': 0.44802627794294386, 'batch_size': 205, 'clip_norm': 15, 'learning_rate': 1.2354210692867358e-06, 'lr_pretraining': 0.0018838050131306729, 'pre_batch_size': 297}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 19:\n",
      "   [I 2025-03-05 03:10:16,718] Trial 19 finished with value: -inf and parameters: {'embed_dim': 64, 'hidden_dim_encoders': 587, 'fusion_dim': 379, 'dropout_encoders': 0.20369717004577764, 'pre_training_epochs': 115, 'hidden_dim': 1829, 'dropout': 0.25970806121902906, 'n_layers': 3, 'dropout_omics': 0.34929889862390373, 'dropout_omics_finetuning': 0.6237964103320439, 'batch_size': 369, 'clip_norm': 11, 'learning_rate': 0.00021669093103291105, 'lr_pretraining': 0.00022290091836019217, 'pre_batch_size': 398}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 20:\n",
      "   [I 2025-03-05 03:10:33,324] Trial 20 finished with value: -inf and parameters: {'embed_dim': 309, 'hidden_dim_encoders': 713, 'fusion_dim': 536, 'dropout_encoders': 0.28826620459782265, 'pre_training_epochs': 84, 'hidden_dim': 424, 'dropout': 0.17045964150229018, 'n_layers': 7, 'dropout_omics': 0.4850646751867733, 'dropout_omics_finetuning': 0.26497753666470936, 'batch_size': 265, 'clip_norm': 12, 'learning_rate': 1.905269124881801e-05, 'lr_pretraining': 1.51430998150056e-05, 'pre_batch_size': 148}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 21:\n",
      "   [I 2025-03-05 03:10:50,340] Trial 21 finished with value: -inf and parameters: {'embed_dim': 275, 'hidden_dim_encoders': 76, 'fusion_dim': 249, 'dropout_encoders': 0.17225683113701018, 'pre_training_epochs': 194, 'hidden_dim': 1284, 'dropout': 0.42775816174794146, 'n_layers': 3, 'dropout_omics': 0.4203297182650896, 'dropout_omics_finetuning': 0.12326887964845495, 'batch_size': 171, 'clip_norm': 5, 'learning_rate': 1.3003879164187847e-05, 'lr_pretraining': 1.0452954410173678e-06, 'pre_batch_size': 491}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 22:\n",
      "   [I 2025-03-05 03:11:07,687] Trial 22 finished with value: -inf and parameters: {'embed_dim': 362, 'hidden_dim_encoders': 78, 'fusion_dim': 179, 'dropout_encoders': 0.49114662164126566, 'pre_training_epochs': 172, 'hidden_dim': 983, 'dropout': 0.38574255397853796, 'n_layers': 3, 'dropout_omics': 0.17292158743295835, 'dropout_omics_finetuning': 0.15454504403483582, 'batch_size': 181, 'clip_norm': 8, 'learning_rate': 1.0548791869862887e-05, 'lr_pretraining': 2.2085295205267067e-06, 'pre_batch_size': 463}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 23:\n",
      "   [I 2025-03-05 03:11:35,365] Trial 23 finished with value: -inf and parameters: {'embed_dim': 302, 'hidden_dim_encoders': 271, 'fusion_dim': 519, 'dropout_encoders': 0.10880225370441698, 'pre_training_epochs': 161, 'hidden_dim': 704, 'dropout': 0.32347188023419127, 'n_layers': 4, 'dropout_omics': 0.6021554434226739, 'dropout_omics_finetuning': 0.22656965145402463, 'batch_size': 81, 'clip_norm': 6, 'learning_rate': 4.7981347173223655e-05, 'lr_pretraining': 7.474710805367187e-05, 'pre_batch_size': 512}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 24:\n",
      "   [I 2025-03-05 03:11:50,869] Trial 24 finished with value: -inf and parameters: {'embed_dim': 440, 'hidden_dim_encoders': 484, 'fusion_dim': 370, 'dropout_encoders': 0.44436328189929697, 'pre_training_epochs': 183, 'hidden_dim': 1550, 'dropout': 0.4522503570944209, 'n_layers': 5, 'dropout_omics': 0.3513164697793604, 'dropout_omics_finetuning': 0.808380881959266, 'batch_size': 267, 'clip_norm': 8, 'learning_rate': 6.0933669882596665e-06, 'lr_pretraining': 1.9805881681612387e-06, 'pre_batch_size': 429}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 25:\n",
      "   [I 2025-03-05 03:12:06,432] Trial 25 finished with value: -inf and parameters: {'embed_dim': 223, 'hidden_dim_encoders': 180, 'fusion_dim': 700, 'dropout_encoders': 0.2290562064815021, 'pre_training_epochs': 35, 'hidden_dim': 1221, 'dropout': 0.3048314569510292, 'n_layers': 2, 'dropout_omics': 0.7463157566671972, 'dropout_omics_finetuning': 0.5683116326397234, 'batch_size': 183, 'clip_norm': 6, 'learning_rate': 2.1296562603799183e-06, 'lr_pretraining': 8.773442883752196e-06, 'pre_batch_size': 368}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 26:\n",
      "   [I 2025-03-05 03:12:37,875] Trial 26 finished with value: -inf and parameters: {'embed_dim': 294, 'hidden_dim_encoders': 355, 'fusion_dim': 238, 'dropout_encoders': 0.37136438795801896, 'pre_training_epochs': 153, 'hidden_dim': 259, 'dropout': 0.38263631409857324, 'n_layers': 9, 'dropout_omics': 0.40490101426444663, 'dropout_omics_finetuning': 0.33048071210952323, 'batch_size': 104, 'clip_norm': 9, 'learning_rate': 0.00014196355672541782, 'lr_pretraining': 0.0004525883794910186, 'pre_batch_size': 467}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 27:\n",
      "   [I 2025-03-05 03:12:50,318] Trial 27 finished with value: -inf and parameters: {'embed_dim': 259, 'hidden_dim_encoders': 441, 'fusion_dim': 572, 'dropout_encoders': 0.14301554688653625, 'pre_training_epochs': 110, 'hidden_dim': 1886, 'dropout': 0.47274226118993173, 'n_layers': 3, 'dropout_omics': 0.49560599012682227, 'dropout_omics_finetuning': 0.43781741157777704, 'batch_size': 412, 'clip_norm': 6, 'learning_rate': 8.794317545221396e-06, 'lr_pretraining': 4.370498826561321e-06, 'pre_batch_size': 275}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 28:\n",
      "   [I 2025-03-05 03:13:09,008] Trial 28 finished with value: -inf and parameters: {'embed_dim': 160, 'hidden_dim_encoders': 243, 'fusion_dim': 391, 'dropout_encoders': 0.2627102594168822, 'pre_training_epochs': 125, 'hidden_dim': 2041, 'dropout': 0.3946967877777124, 'n_layers': 4, 'dropout_omics': 0.6483189623805652, 'dropout_omics_finetuning': 0.6595032948673757, 'batch_size': 154, 'clip_norm': 14, 'learning_rate': 4.8406495974298846e-05, 'lr_pretraining': 1.7814088902206996e-05, 'pre_batch_size': 222}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 29:\n",
      "   [I 2025-03-05 03:13:28,252] Trial 29 finished with value: -inf and parameters: {'embed_dim': 207, 'hidden_dim_encoders': 541, 'fusion_dim': 501, 'dropout_encoders': 0.31062385158755035, 'pre_training_epochs': 73, 'hidden_dim': 1567, 'dropout': 0.3171646714256859, 'n_layers': 9, 'dropout_omics': 0.7700745636723854, 'dropout_omics_finetuning': 0.30594780589166926, 'batch_size': 225, 'clip_norm': 18, 'learning_rate': 2.439276398104085e-06, 'lr_pretraining': 0.001189090037852263, 'pre_batch_size': 444}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 30:\n",
      "   [I 2025-03-05 03:13:45,387] Trial 30 finished with value: -inf and parameters: {'embed_dim': 322, 'hidden_dim_encoders': 910, 'fusion_dim': 600, 'dropout_encoders': 0.2775526907500722, 'pre_training_epochs': 29, 'hidden_dim': 1601, 'dropout': 0.3558427177100909, 'n_layers': 10, 'dropout_omics': 0.16304470883264227, 'dropout_omics_finetuning': 0.1902347070940844, 'batch_size': 317, 'clip_norm': 17, 'learning_rate': 0.0019352641587353475, 'lr_pretraining': 2.1109862470137825e-06, 'pre_batch_size': 478}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 31:\n",
      "   [I 2025-03-05 03:14:04,242] Trial 31 finished with value: -inf and parameters: {'embed_dim': 452, 'hidden_dim_encoders': 506, 'fusion_dim': 652, 'dropout_encoders': 0.23168057744158366, 'pre_training_epochs': 15, 'hidden_dim': 1652, 'dropout': 0.27773557630032475, 'n_layers': 10, 'dropout_omics': 0.7672525474179089, 'dropout_omics_finetuning': 0.648988604102767, 'batch_size': 264, 'clip_norm': 17, 'learning_rate': 3.373684333905937e-06, 'lr_pretraining': 0.003188108592553291, 'pre_batch_size': 445}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 32:\n",
      "   [I 2025-03-05 03:14:21,272] Trial 32 finished with value: -inf and parameters: {'embed_dim': 490, 'hidden_dim_encoders': 350, 'fusion_dim': 712, 'dropout_encoders': 0.3390684038595553, 'pre_training_epochs': 51, 'hidden_dim': 1458, 'dropout': 0.4181503040889453, 'n_layers': 9, 'dropout_omics': 0.5579019453818782, 'dropout_omics_finetuning': 0.5467493883531913, 'batch_size': 299, 'clip_norm': 15, 'learning_rate': 5.253038929383197e-06, 'lr_pretraining': 0.00018880190151656927, 'pre_batch_size': 479}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 33:\n",
      "   [I 2025-03-05 03:14:37,951] Trial 33 finished with value: -inf and parameters: {'embed_dim': 90, 'hidden_dim_encoders': 704, 'fusion_dim': 891, 'dropout_encoders': 0.39388820474595176, 'pre_training_epochs': 23, 'hidden_dim': 1296, 'dropout': 0.25215593775301615, 'n_layers': 6, 'dropout_omics': 0.5920279915846782, 'dropout_omics_finetuning': 0.49394426168442224, 'batch_size': 241, 'clip_norm': 13, 'learning_rate': 1.8578662219735922e-06, 'lr_pretraining': 0.0006967538412907525, 'pre_batch_size': 251}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 34:\n",
      "   [I 2025-03-05 03:14:57,894] Trial 34 finished with value: -inf and parameters: {'embed_dim': 410, 'hidden_dim_encoders': 556, 'fusion_dim': 307, 'dropout_encoders': 0.21668535216653442, 'pre_training_epochs': 48, 'hidden_dim': 1854, 'dropout': 0.4644811773276021, 'n_layers': 8, 'dropout_omics': 0.6315316975675916, 'dropout_omics_finetuning': 0.7631788084919302, 'batch_size': 204, 'clip_norm': 16, 'learning_rate': 7.2701642068205905e-06, 'lr_pretraining': 0.00010192633425026667, 'pre_batch_size': 430}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 35:\n",
      "   [I 2025-03-05 03:15:23,752] Trial 35 finished with value: -inf and parameters: {'embed_dim': 259, 'hidden_dim_encoders': 133, 'fusion_dim': 642, 'dropout_encoders': 0.2703622788157637, 'pre_training_epochs': 62, 'hidden_dim': 1042, 'dropout': 0.33832604881848516, 'n_layers': 10, 'dropout_omics': 0.8913935572786473, 'dropout_omics_finetuning': 0.5965196128543663, 'batch_size': 151, 'clip_norm': 12, 'learning_rate': 1.0518604444641613e-06, 'lr_pretraining': 0.008840748598487597, 'pre_batch_size': 387}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 36:\n",
      "   [I 2025-03-05 03:15:38,699] Trial 36 finished with value: -inf and parameters: {'embed_dim': 368, 'hidden_dim_encoders': 661, 'fusion_dim': 560, 'dropout_encoders': 0.15383854187859913, 'pre_training_epochs': 104, 'hidden_dim': 1723, 'dropout': 0.3677264108167735, 'n_layers': 7, 'dropout_omics': 0.6993102586401955, 'dropout_omics_finetuning': 0.5158056788294323, 'batch_size': 355, 'clip_norm': 20, 'learning_rate': 3.3390725565037194e-06, 'lr_pretraining': 0.0016929801624206093, 'pre_batch_size': 453}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 37:\n",
      "   [I 2025-03-05 03:15:57,030] Trial 37 finished with value: -inf and parameters: {'embed_dim': 509, 'hidden_dim_encoders': 222, 'fusion_dim': 490, 'dropout_encoders': 0.4366508595018932, 'pre_training_epochs': 95, 'hidden_dim': 1473, 'dropout': 0.4076786232962559, 'n_layers': 2, 'dropout_omics': 0.43977085376979025, 'dropout_omics_finetuning': 0.6395911406583404, 'batch_size': 118, 'clip_norm': 14, 'learning_rate': 1.4705210576249714e-05, 'lr_pretraining': 0.0002777359656258143, 'pre_batch_size': 325}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 38:\n",
      "   [I 2025-03-05 03:16:11,858] Trial 38 finished with value: -inf and parameters: {'embed_dim': 471, 'hidden_dim_encoders': 426, 'fusion_dim': 803, 'dropout_encoders': 0.2454554367585648, 'pre_training_epochs': 132, 'hidden_dim': 1968, 'dropout': 0.24273795712090082, 'n_layers': 6, 'dropout_omics': 0.3235308625533675, 'dropout_omics_finetuning': 0.4520642845579339, 'batch_size': 288, 'clip_norm': 5, 'learning_rate': 0.00037685422568987264, 'lr_pretraining': 3.7707572151169726e-06, 'pre_batch_size': 244}. Best is trial 0 with value: -inf.\n",
      "\n",
      "🔹 Trial 39:\n",
      "   [I 2025-03-05 03:16:35,879] Trial 39 finished with value: -inf and parameters: {'embed_dim': 131, 'hidden_dim_encoders': 759, 'fusion_dim': 428, 'dropout_encoders': 0.32533743849934327, 'pre_training_epochs': 73, 'hidden_dim': 1825, 'dropout': 0.49993161850928136, 'n_layers': 9, 'dropout_omics': 0.5255490177787787, 'dropout_omics_finetuning': 0.6911248618812798, 'batch_size': 136, 'clip_norm': 19, 'learning_rate': 0.00011257996059322072, 'lr_pretraining': 8.066275067561858e-06, 'pre_batch_size': 492}. Best is trial 0 with value: -inf.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trial_configs = {}\n",
    "\n",
    "# 로그 파일 읽기\n",
    "with open(\"slurm-984849.out\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if \"Trial\" in line and \"finished with value\" in line:\n",
    "            try:\n",
    "                # Trial ID 추출\n",
    "                parts = line.split(\"Trial\")\n",
    "                trial_id = int(parts[1].split()[0])  # \"Trial X\"에서 X만 추출\n",
    "                trial_configs[trial_id] = line\n",
    "            except (IndexError, ValueError) as e:\n",
    "                print(f\"⚠️ Trial ID 추출 오류 발생: {line} | 오류: {e}\")\n",
    "\n",
    "# 가독성 좋게 출력\n",
    "print(\"\\n📌 오류 발생한 Trial 목록:\\n\")\n",
    "for trial_id, trial_info in sorted(trial_configs.items()):\n",
    "    print(f\"🔹 Trial {trial_id}:\")\n",
    "    print(f\"   {trial_info}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4373325f-07ed-4a08-938b-149682398b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features': {'fp_radius': 2,\n",
       "  'use_correlation_representation': True,\n",
       "  'num_modalities': 4},\n",
       " 'optimizer': {'batch_size': 136,\n",
       "  'clip_norm': 19,\n",
       "  'learning_rate': 0.00011257996059322072,\n",
       "  'stopping_patience': 15,\n",
       "  'pre_batch_size': 492,\n",
       "  'lr_pretraining': 8.066275067561858e-06},\n",
       " 'model': {'embed_dim': 131,\n",
       "  'hidden_dim': 1825,\n",
       "  'dropout': 0.49993161850928136,\n",
       "  'n_layers': 9,\n",
       "  'norm': 'batchnorm',\n",
       "  'hidden_dim_encoders': 759,\n",
       "  'fusion_dim': 428,\n",
       "  'dropout_encoders': 0.32533743849934327,\n",
       "  'dropout_omics': 0.5255490177787787,\n",
       "  'dropout_omics_finetuning': 0.6911248618812798,\n",
       "  'pre_training_epochs': 73},\n",
       " 'env': {'fold': 0,\n",
       "  'device': 'cuda:0',\n",
       "  'max_epochs': 100,\n",
       "  'search_hyperparameters': False}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3923d296-ab3b-4dbf-a6a7-d67b45daf8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
