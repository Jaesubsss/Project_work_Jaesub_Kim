{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ce831d-3d8b-4bdf-9d5c-8f96227a2b8c",
   "metadata": {},
   "source": [
    "# Package & Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9dff727-3bc9-4647-9d03-3aa3bdeb368e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim14/.conda/envs/jupyter/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "from torchmetrics import MeanSquaredError\n",
    "from sklearn.impute import KNNImputer\n",
    "import scripts\n",
    "from functools import lru_cache\n",
    "import optuna\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import warnings\n",
    "from torch.utils.data import Dataset\n",
    "from datetime import datetime\n",
    "import uuid \n",
    "# 경고 무시 설정\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6edd50e5-91f8-4df6-8d58-e5e712430d34",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dataset for sim_matrix\n",
    "class OmicsDataset_dict(Dataset): \n",
    "    def __init__(self, omic_dict, drug_dict, data): \n",
    "        self.omic_dict = omic_dict\n",
    "        self.drug_dict = drug_dict\n",
    "        self.cell_mapped_ids = {key:i for i, key in enumerate(self.omic_dict.keys())}\n",
    "        # omic_dict의 키를 고유한 인덱스로 매핑\n",
    "        # enumerate는 키들을 순서대로 열거하여 (인덱스, 키) 형태의 튜플로 반환\n",
    "        # 딕셔너리 컴프레헨션: 각 키를 key로, 각 키의 인덱스를 i로 사용하여 {key:i}형태로 매핑된 딕셔너리 만듬.\n",
    "        self.drug_mapped_ids = {key:i for i, key in enumerate(self.drug_dict.keys())}\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx): # idx = train_data\n",
    "        instance = self.data.iloc[idx] \n",
    "        cell_id = instance.iloc[0]\n",
    "        drug_id = instance.iloc[1]\n",
    "        target = instance.iloc[2]\n",
    "        \n",
    "        #omics_data = { # usage of dictionary here causes a problem or crash with collate_fn function in Dataloader \n",
    "        #    cell_id : {\n",
    "        #        data_type: self.omic_dict[cell_id][data_type] for data_type in self.omic_dict[cell_id].keys()\n",
    "        #    }\n",
    "        #}\n",
    "        \n",
    "        return (torch.cat([self.omic_dict[cell_id][modality] for modality in self.omic_dict[cell_id].keys()]), \n",
    "                self.drug_dict[drug_id],\n",
    "                torch.Tensor([target]),\n",
    "                torch.Tensor([self.cell_mapped_ids[cell_id]]),\n",
    "                torch.Tensor([self.drug_mapped_ids[drug_id]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deaf58b2-acde-4a0b-92c0-c32057378a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDA model\n",
    "class MultimodalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dims,  hidden_dim_encoders = 150, embed_dim = 75, fusion_dim = 150, dropout_encoders = 0.2):\n",
    "        # get input as a dictionary\n",
    "        super(MultimodalAutoencoder, self).__init__()\n",
    "        # EEEEEEEEEEncoder\n",
    "        self.input_dims = input_dims\n",
    "        self.num_modalities = len(input_dims)\n",
    "        self.do = nn.Dropout(dropout_encoders)\n",
    "\n",
    "        self.omics_encoder = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(mod_input_dim, hidden_dim_encoders), # input \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim_encoders, embed_dim) # encoder hidden layer: 150, 75 as the value from the paper. so we start from this \n",
    "            )                                 # I dont get why they used 150, 75 for dimension, but we can tune it later\n",
    "            for mod_input_dim in self.input_dims\n",
    "        ])\n",
    "        # fused latent feature \n",
    "        self.fusion_layer = nn.Sequential( # I think we need a fusion layer here, to combine the data modalities\n",
    "            nn.Linear(embed_dim * self.num_modalities, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fusion_dim, embed_dim) # This concatenate latent features of all omics data, and fusion them and make its dim final latent dim\n",
    "        )                                     # This is the only way I can think of to fuse omics data\n",
    "        # decoder\n",
    "        self.omics_decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim_encoders),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_encoders, sum(input_dims))\n",
    "        )\n",
    "        # I actually dont understand this step in paper. they said that decoder has symmetric structure as encoder,\n",
    "        # but the data after MDA they provided, has weird dimension(363x90) which makes no sense. this is the point that i cant understand\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_features = [] # get dictionary as an input \n",
    "        start_idx = 0\n",
    "        for i, encoder in enumerate(self.omics_encoder):\n",
    "            mod_input_dim = self.input_dims[i]\n",
    "            x_modality = x[:, start_idx:start_idx + mod_input_dim]\n",
    "            latent_features.append(encoder(self.do(x_modality)))\n",
    "            start_idx += mod_input_dim\n",
    "            \n",
    "        latent_fused = torch.cat(latent_features, dim=1)\n",
    "        latent_final = self.fusion_layer(latent_fused)\n",
    "        decoded = self.omics_decoder(latent_final)\n",
    "        return decoded, latent_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d90f4c86-c210-4d9e-a83b-a8cddf993ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main resnet model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, embed_dim=256, hidden_dim=1024, dropout=0.1, n_layers=6, norm=\"layernorm\"):\n",
    "        super().__init__()\n",
    "        self.mlps = nn.ModuleList()\n",
    "        \n",
    "        # Determine normalization layer\n",
    "        if norm == \"layernorm\":\n",
    "            norm_layer = nn.LayerNorm\n",
    "        elif norm == \"batchnorm\":\n",
    "            norm_layer = nn.BatchNorm1d\n",
    "        else:\n",
    "            norm_layer = nn.Identity\n",
    "        \n",
    "        # Create MLP layers\n",
    "        for _ in range(n_layers):\n",
    "            self.mlps.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(embed_dim, hidden_dim),\n",
    "                    norm_layer(hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(hidden_dim, embed_dim)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.lin = nn.Linear(embed_dim, 1)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.mlps:\n",
    "            \n",
    "            x = (layer(x) + x) / 2  # Residual connection\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "class Main_model(nn.Module):\n",
    "    def __init__(self, embed_dim=256, hidden_dim=1024, dropout=0.1, n_layers=6, norm=\"layernorm\", \n",
    "                 dropout_omics=0.4, dropout_omics_finetuning=0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ResNet as backbone\n",
    "        self.resnet = ResNet(embed_dim, hidden_dim, dropout, n_layers, norm)\n",
    "        \n",
    "        # Modified embed_d: Two-layer MLP with dropout after ReLU\n",
    "        self.embed_d = nn.Sequential(\n",
    "            nn.LazyLinear(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Modified embed_c: Two-layer MLP with dropout after ReLU and before first Linear Layer\n",
    "        self.embed_c = nn.Sequential(\n",
    "            nn.Dropout(dropout_omics_finetuning),\n",
    "            nn.LazyLinear(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_omics),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, c, d):\n",
    "        # Combine embedded inputs and pass through ResNet\n",
    "        return self.resnet(self.embed_d(d) + self.embed_c(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73df737-dd85-452a-9b4b-1ffd3dc25eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Data function\n",
    "@lru_cache(maxsize=None)\n",
    "def get_data_corr(n_fold = 0, fp_radius = 2, transform_into_corr = True, typ = [\"rnaseq\", \"mutations\", \"cnvs\"],\n",
    "                  #reconstructed = None\n",
    "                 ):\n",
    "    # drug\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    \n",
    "    # loading all datasets\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    \n",
    "    proteomics = pd.read_csv(\"data/proteomics.csv\", index_col=0)\n",
    "    \n",
    "    mutation = pd.read_csv(\"data/binary_mutations.csv\")\n",
    "    mutation.columns = mutation.iloc[0]\n",
    "    mutation = mutation.iloc[2:,:].set_index(\"gene_symbol\")\n",
    "    driver_columns = mutation.columns.isin(driver_genes)\n",
    "    filtered_mut = mutation.loc[:, driver_columns]\n",
    "    filtered_mut = filtered_mut.astype(float)\n",
    "\n",
    "    methylations = pd.read_csv(\"data/methylations.csv\",index_col = 0).sort_index(ascending = True)\n",
    "\n",
    "    cnvs = pd.read_csv(\"data/copy_number_variations.csv\",index_col= 0)\n",
    "\n",
    "    # concatenate all dataset \n",
    "    # inner join based on index: model_ids with NaN are automatically filtered out \n",
    "    data_concat = pd.concat([filtered_rna, proteomics, filtered_mut, methylations, cnvs], axis=1, join='inner')\n",
    "    \n",
    "    \n",
    "    # Filter data by common indices in all modalities\n",
    "    filtered_rna = filtered_rna[filtered_rna.index.isin(data_concat.index)]\n",
    "    proteomics = proteomics[proteomics.index.isin(data_concat.index)]\n",
    "    filtered_mut = filtered_mut[filtered_mut.index.isin(data_concat.index)]\n",
    "    methylations = methylations[methylations.index.isin(data_concat.index)]\n",
    "    cnvs = cnvs[cnvs.index.isin(data_concat.index)]\n",
    "    \n",
    "    # Initialize cell_dict\n",
    "    cell_dict = {}\n",
    "\n",
    "    if not transform_into_corr : #and reconstructed is None:\n",
    "\n",
    "        dims = []\n",
    "        if \"rnaseq\" in typ:\n",
    "            dims.append(filtered_rna.shape[1])\n",
    "        if \"proteomics\" in typ:\n",
    "            dims.append(proteomics.shape[1])\n",
    "        if \"mutations\" in typ:\n",
    "            dims.append(filtered_mut.shape[1])\n",
    "        if \"methylations\" in typ:\n",
    "            dims.append(methylations.shape[1])\n",
    "        if \"cnvs\" in typ:\n",
    "            dims.append(cnvs.shape[1])\n",
    "        \n",
    "        for cell in data_concat.index:\n",
    "            # Initialize a sub-dictionary for each cell\n",
    "            concatenated_data = []\n",
    "            \n",
    "            # Add data for each type specified in typ\n",
    "            if \"rnaseq\" in typ:\n",
    "                concatenated_data.append(filtered_rna.loc[cell].to_numpy())\n",
    "            if \"proteomics\" in typ:\n",
    "                concatenated_data.append(proteomics.loc[cell].to_numpy())\n",
    "            if \"mutations\" in typ:\n",
    "                concatenated_data.append(filtered_mut.loc[cell].to_numpy())\n",
    "            if \"methylations\" in typ:\n",
    "                concatenated_data.append(methylations.loc[cell].to_numpy())\n",
    "            if \"cnvs\" in typ:\n",
    "                concatenated_data.append(cnvs.loc[cell].to_numpy())\n",
    "\n",
    "            cell_dict[cell] = torch.Tensor(np.concatenate(concatenated_data))\n",
    "            \n",
    "#    if reconstructed is not None:\n",
    "#        for cell_idx, cell in enumerate(data_concat.index):\n",
    "#            # cell_dict에 reconstructed 텐서의 각 행(cell 데이터) 저장\n",
    "#            cell_dict[cell] = reconstructed[cell_idx]\n",
    "        \n",
    "\n",
    "    # GDSC\n",
    "    GDSC1 = pd.read_csv(\"data/GDSC1.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = GDSC1.query(\"SANGER_MODEL_ID in @data_concat.index & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "\n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    test_lines = folds[0] \n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold] \n",
    "\n",
    "        # no change needed, query works fine with some missing\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "    \n",
    "    if transform_into_corr: #and reconstructed is None:\n",
    "        # train, val, test among filtered data\n",
    "        # these are valid train_, val_ and test_data index\n",
    "        \n",
    "        \n",
    "        n_train = len(train_lines)  \n",
    "        n_val = len(validation_lines)      \n",
    "        n_test = len(test_lines)\n",
    "        \n",
    "        # Precompute similarity matrices for each data type\n",
    "        similarity_matrices = {}\n",
    "        dims = []\n",
    "        \n",
    "        if \"rnaseq\" in typ:\n",
    "            exp_com = np.corrcoef(np.vstack([filtered_rna.loc[train_lines], \n",
    "                                             filtered_rna.loc[validation_lines], \n",
    "                                             filtered_rna.loc[test_lines]]), rowvar=True)\n",
    "            train = exp_com[:n_train, :n_train]\n",
    "            val = exp_com[n_train:n_train+n_val, :n_train]\n",
    "            test = exp_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"rnaseq\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"rnaseq\"][0]))\n",
    "        \n",
    "        if \"proteomics\" in typ:\n",
    "            prot_com = np.corrcoef(np.vstack([proteomics.loc[train_lines], \n",
    "                                              proteomics.loc[validation_lines], \n",
    "                                              proteomics.loc[test_lines]]), rowvar=True)\n",
    "            train = prot_com[:n_train, :n_train]\n",
    "            val = prot_com[n_train:n_train+n_val, :n_train]\n",
    "            test = prot_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"proteomics\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"proteomics\"][0]))\n",
    "        \n",
    "        if \"mutations\" in typ:\n",
    "            train_snp = filtered_mut.loc[train_lines].astype(bool)\n",
    "            val_snp = filtered_mut.loc[validation_lines].astype(bool)\n",
    "            test_snp = filtered_mut.loc[test_lines].astype(bool)\n",
    "            \n",
    "            train = 1 - pairwise_distances(train_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            val = 1 - pairwise_distances(val_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "            test = 1 - pairwise_distances(test_snp.values, train_snp.values, metric=\"jaccard\")\n",
    "    \n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"mutations\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"mutations\"][0]))\n",
    "        \n",
    "        if \"methylations\" in typ:\n",
    "            methyl_com = np.nan_to_num(np.corrcoef(np.vstack([methylations.loc[train_lines], \n",
    "                                                methylations.loc[validation_lines], \n",
    "                                                methylations.loc[test_lines]]), rowvar=True))\n",
    "            train = methyl_com[:n_train, :n_train]\n",
    "            val = methyl_com[n_train:n_train+n_val, :n_train]\n",
    "            test = methyl_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"methylations\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"methylations\"][0]))\n",
    "        \n",
    "        if \"cnvs\" in typ:\n",
    "            cnv_com = np.nan_to_num(np.corrcoef(np.vstack([cnvs.loc[train_lines], # nan-generation problem fixed \n",
    "                                             cnvs.loc[validation_lines], \n",
    "                                             cnvs.loc[test_lines]]), rowvar=True))\n",
    "            train= cnv_com[:n_train, :n_train]\n",
    "            val= cnv_com[n_train:n_train+n_val, :n_train]\n",
    "            test= cnv_com[n_train+n_val:, :n_train]\n",
    "            sim_combined = np.vstack([train, val, test])\n",
    "            similarity_matrices[\"cnvs\"] = sim_combined\n",
    "            dims.append(len(similarity_matrices[\"cnvs\"][0]))\n",
    "            \n",
    "        cell_dict = {}\n",
    "\n",
    "        # \n",
    "        for cell in unique_cell_lines:\n",
    "            cell_dict[cell] = {}\n",
    "            for data_type in typ:\n",
    "                sim_matrices = similarity_matrices[data_type]\n",
    "                sim_tensor = torch.Tensor(sim_matrices)\n",
    "                cell_idx = np.where(unique_cell_lines == cell)[0][0]\n",
    "                cell_dict[cell][data_type] = sim_tensor[cell_idx]\n",
    "    \n",
    "        return (OmicsDataset_dict(cell_dict, drug_dict, train_data),\n",
    "        OmicsDataset_dict(cell_dict, drug_dict, validation_data),\n",
    "        OmicsDataset_dict(cell_dict, drug_dict, test_data),\n",
    "        dims)\n",
    "\n",
    "    return (scripts.OmicsDataset(cell_dict, drug_dict, train_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, validation_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, test_data),\n",
    "    dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0cc9d82-bcb7-4ee8-b710-3dfa2640517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder training step\n",
    "\n",
    "def autoencoder_train_step(config, train_dataset, input_dims):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"optimizer\"][\"pre_batch_size\"],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    # Autoencoder 모델 초기화\n",
    "    autoencoder = MultimodalAutoencoder(\n",
    "        input_dims=input_dims,\n",
    "        hidden_dim_encoders=config[\"model\"][\"hidden_dim_encoders\"],\n",
    "        embed_dim=config[\"model\"][\"embed_dim\"],\n",
    "        fusion_dim=config[\"model\"][\"fusion_dim\"],\n",
    "        dropout_encoders=config[\"model\"][\"dropout_encoders\"]\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=config[\"optimizer\"][\"lr_pretraining\"])\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "    device = torch.device(config[\"env\"][\"device\"])\n",
    "    \n",
    "    autoencoder.to(device)\n",
    "    autoencoder.train()\n",
    "\n",
    "    pre_training_epochs = config[\"model\"][\"pre_training_epochs\"]\n",
    "    total_losses = []\n",
    "\n",
    "    for epoch in range(pre_training_epochs):\n",
    "        total_loss = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            inputs = batch[0].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed, latent_features = autoencoder(inputs)  # Latent Feature 생성\n",
    "\n",
    "            loss = criterion(reconstructed, inputs)  # MSE Loss 계산\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(total_loss)\n",
    "        total_losses.append(train_loss)\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{pre_training_epochs}, Train Loss: {train_loss:.5f}')\n",
    "\n",
    "    print(\"Pre-training complete!\")\n",
    "\n",
    "    return autoencoder, total_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d52915fa-a8d4-4968-8cde-f1912dfa7d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training function\n",
    "def train_step(model, autoencoder, optimizer, loader, config, device):\n",
    "    loss = nn.MSELoss()\n",
    "    ls = []\n",
    "    model.train()\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():  # Autoencoder는 업데이트되지 않도록 설정\n",
    "            _, latent_features = autoencoder(batch[0].to(device))  # (batch_size, latent_dim)\n",
    "        out = model(latent_features, batch[1].to(device))\n",
    "        l = loss(out.squeeze(), batch[2].to(device).squeeze())\n",
    "        l.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"optimizer\"][\"clip_norm\"])\n",
    "        ls += [l.item()]\n",
    "        optimizer.step()\n",
    "    return np.mean(ls)\n",
    "\n",
    "\n",
    "# 여기서, autoencoder는 pre-trained 되어있고, frozen 상태이다. 즉, 어떤 데이터를 어떤 조합으로 넣던간에 항상 동일한 latent feature를 생성한다.\n",
    "# 따라서, 학습 전에, 모든 데이터에 대해 한번만 autoencoder를 사용하더라도 문제가 없을..줄 알았는데!\n",
    "# autoencoder 과정이 dropout을 포함하고 있기 때문에, 데이터의 분포에 따라 latent feature가 달라질 수 있다. \n",
    "# 따라서, 매 train_step마다 데이터를 autoencoder를 거치도록 하겠다. \n",
    "def latent_resnet_training(config, train_dataset, autoencoder, validation_dataset=None, use_momentum=True, callback_epoch=None):\n",
    "    device = torch.device(config[\"env\"][\"device\"])\n",
    "    autoencoder.eval()\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "        drop_last=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    if validation_dataset is not None:\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            validation_dataset,\n",
    "            batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "            drop_last=False,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    ### Fine-tuning 모델 생성 ###\n",
    "    model = Main_model(\n",
    "        embed_dim=config[\"model\"][\"embed_dim\"],\n",
    "        hidden_dim=config[\"model\"][\"hidden_dim\"], \n",
    "        dropout=config[\"model\"][\"dropout\"], \n",
    "        n_layers=config[\"model\"][\"n_layers\"],  \n",
    "        dropout_omics=config[\"model\"][\"dropout_omics\"], \n",
    "        dropout_omics_finetuning=config[\"model\"][\"dropout_omics_finetuning\"],\n",
    "        norm=config[\"model\"][\"norm\"]\n",
    "    )\n",
    "    #----\n",
    "    \n",
    "    #with torch.no_grad():\n",
    "    #    ae_fusion_weight = autoencoder.omics_encoder[-1][-1].weight  # Autoencoder Encoder 마지막 Linear Layer 가중치\n",
    "    #    model.embed_c[-1].weight.copy_(ae_fusion_weight)  # ResNet의 embed_c 마지막 Linear Layer 가중치로 복사\n",
    "    \n",
    "    #----\n",
    "    optimizer = torch.optim.Adam(model.parameters(), config[\"optimizer\"][\"learning_rate\"])\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)\n",
    "    early_stop = scripts.EarlyStop(config[\"optimizer\"][\"stopping_patience\"])\n",
    "    model.to(device)\n",
    "\n",
    "    metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection({\n",
    "        \"R_cellwise_residuals\": scripts.GroupwiseMetric(\n",
    "            metric=torchmetrics.functional.pearson_corrcoef,\n",
    "            grouping=\"drugs\",\n",
    "            average=\"macro\",\n",
    "            residualize=True\n",
    "        ),\n",
    "        \"R_cellwise\": scripts.GroupwiseMetric(\n",
    "            metric=torchmetrics.functional.pearson_corrcoef,\n",
    "            grouping=\"cell_lines\",\n",
    "            average=\"macro\",\n",
    "            residualize=False\n",
    "        ),\n",
    "        \"MSE\": torchmetrics.MeanSquaredError()\n",
    "    }))\n",
    "    metrics.to(device)\n",
    "\n",
    "    ### Fine-tuning Loop ###\n",
    "    for epoch in range(config[\"env\"][\"max_epochs\"]):\n",
    "        train_loss = train_step(model, autoencoder, optimizer, train_loader, config, device)\n",
    "\n",
    "        if epoch == 0:  \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=config[\"optimizer\"][\"learning_rate\"])\n",
    "        \n",
    "        lr_scheduler.step(train_loss)\n",
    "        \n",
    "        if validation_dataset is not None:\n",
    "            validation_metrics = evaluate_step(model, autoencoder, val_loader, metrics, device)\n",
    "            if epoch > 0 and use_momentum:\n",
    "                val_target = 0.2 * val_target + 0.8 * validation_metrics['R_cellwise_residuals']\n",
    "            else:\n",
    "                val_target = validation_metrics['R_cellwise_residuals']\n",
    "        else:\n",
    "            val_target = None\n",
    "\n",
    "        if callback_epoch is None:\n",
    "            print(f\"epoch : {epoch}: train loss: {train_loss} Smoothed R interaction (validation) {val_target}\")\n",
    "        else:\n",
    "            callback_epoch(epoch, val_target)\n",
    "\n",
    "        if early_stop(train_loss):\n",
    "            break\n",
    "\n",
    "    return val_target, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423a3b62-5c29-4360-8eab-265db337cd55",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# evaluation step\n",
    "\n",
    "def evaluate_step(model, autoencoder, loader, metrics, device, save_predictions=False, model_name = \"model\", dataset_name = \"dataset\"):\n",
    "    metrics.increment()\n",
    "    autoencoder.to(device)\n",
    "    model.to(device) # ensure model is on the correct device\n",
    "    autoencoder.eval()\n",
    "    model.eval()\n",
    "    \n",
    "\n",
    "    # Storage for predictions if saving is enabled\n",
    "    predictions = {\"cell_line\": [], \"drug_id\": [], \"prediction\": [], \"target\": []}\n",
    "\n",
    "    for x in loader:\n",
    "        with torch.no_grad():\n",
    "            _, latent_features = autoencoder(x[0].to(device))\n",
    "            out = model(latent_features, x[1].to(device))\n",
    "            metrics.update(out.squeeze(),\n",
    "                           x[2].to(device).squeeze(),\n",
    "                           cell_lines=x[3].to(device).squeeze(),\n",
    "                           drugs=x[4].to(device).squeeze())\n",
    "            \n",
    "            # Save predictions if required\n",
    "            if save_predictions:\n",
    "                predictions[\"cell_line\"].extend(x[3].squeeze().tolist())  \n",
    "                predictions[\"drug_id\"].extend(x[4].squeeze().cpu().tolist())    \n",
    "                predictions[\"prediction\"].extend(out.squeeze().tolist()) \n",
    "                predictions[\"target\"].extend(x[2].squeeze().cpu() .tolist())    \n",
    "\n",
    "    # Compute and return metrics\n",
    "    metrics_dict = {it[0]: it[1].item() for it in metrics.compute().items()}\n",
    "\n",
    "    # Save predictions to a CSV file if required\n",
    "    if save_predictions:\n",
    "        df = pd.DataFrame(predictions)\n",
    "        filename = generate_filename(model_name, dataset_name, extension=\"csv\")\n",
    "        df.to_csv(\"results/\" + filename, index=False)\n",
    "        print(f\"Predictions saved to: results/{filename}\")\n",
    "\n",
    "    return metrics_dict\n",
    "        \n",
    "def generate_filename(model_name=\"model1\", dataset_name=\"dataset\", extension=\"csv\"):\n",
    "    time = datetime.now().strftime(\"%Y%m%d_%H:%M:%S\")\n",
    "    unique_id = uuid.uuid4()\n",
    "    filename = f\"pred_{model_name}_{dataset_name}_{time}_{unique_id}.{extension}\"\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f0d3fae-2e79-4c4f-b99f-31088038ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "config = {\"features\" : {\"fp_radius\":2,\n",
    "                        \"use_correlation_representation\": True,\n",
    "                        \"num_modalities\": 4},\n",
    "          \"optimizer\": {\"batch_size\": 220,\n",
    "                        \"clip_norm\":19,\n",
    "                        \"learning_rate\": 0.0004592646200179472,\n",
    "                        \"stopping_patience\":15,\n",
    "                        \"pre_batch_size\": 200,\n",
    "                        \"lr_pretraining\": 0.0004592646200179472},\n",
    "          \"model\":{\"embed_dim\":485, # shared\n",
    "                 \"hidden_dim\":696, \n",
    "                 \"dropout\":0.48541242824674574, \n",
    "                 \"n_layers\": 4, \n",
    "                 \"norm\": \"batchnorm\", \n",
    "                 \"hidden_dim_encoders\": 256, # ENCODER, 원래 256\n",
    "                 \"fusion_dim\": 700, # ENCODER\n",
    "                 \"dropout_encoders\": 0.2,\n",
    "                 \"dropout_omics\": 0.4, # second\n",
    "                 \"dropout_omics_finetuning\": 0.4, # first\n",
    "                 \"pre_training_epochs\": 100}, \n",
    "         \"env\": {\"fold\": 0,  \n",
    "                \"device\":\"cuda:0\", \n",
    "                 \"max_epochs\": 100, \n",
    "                 \"search_hyperparameters\":False}} \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40550cfe-32b5-4780-82d7-8a890facb06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new best hyperparameter\n",
    "config = {\"features\" : {\"fp_radius\":2,\n",
    "                        \"use_correlation_representation\": True,\n",
    "                        \"num_modalities\": 4},\n",
    "          \"optimizer\": {\"batch_size\": 469,\n",
    "                        \"clip_norm\":19,\n",
    "                        \"learning_rate\": 0.0006230634247016341,\n",
    "                        \"stopping_patience\":15,\n",
    "                        \"pre_batch_size\": 474,\n",
    "                        \"lr_pretraining\": 3.9761097548681355e-06},\n",
    "          \"model\":{\"embed_dim\":215, # shared\n",
    "                 \"hidden_dim\":696, \n",
    "                 \"dropout\":0.48541242824674574, \n",
    "                 \"n_layers\": 4, \n",
    "                 \"norm\": \"batchnorm\", \n",
    "                 \"hidden_dim_encoders\": 696, # ENCODER, 원래 256\n",
    "                 \"fusion_dim\": 505, # ENCODER\n",
    "                 \"dropout_encoders\": 0.3626415306459327,\n",
    "                 \"dropout_omics\": 0.5398956377457799, # second\n",
    "                 \"dropout_omics_finetuning\": 0.42563415722955866, # first\n",
    "                 \"pre_training_epochs\": 184}, \n",
    "         \"env\": {\"fold\": 0,  \n",
    "                \"device\":\"cuda:0\", \n",
    "                 \"max_epochs\": 100, \n",
    "                 \"search_hyperparameters\":False}} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a2735-570b-4331-a27b-616ca1182b72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fbb24a9-529a-42db-bc7e-c03f7029833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_optuna(trial, config):\n",
    "    \"\"\"\n",
    "    Optuna를 활용한 하이퍼파라미터 최적화 함수\n",
    "    - Pre-training (Autoencoder)\n",
    "    - Fine-tuning (ResNet)\n",
    "    \"\"\"\n",
    "\n",
    "    ### 1️⃣ Pre-training 하이퍼파라미터 샘플링 ###\n",
    "    config[\"model\"][\"embed_dim\"] = trial.suggest_int(\"embed_dim\", 64, 512)\n",
    "    config[\"model\"][\"hidden_dim_encoders\"] = trial.suggest_int(\"hidden_dim_encoders\", 64, 2048)\n",
    "    config[\"model\"][\"fusion_dim\"] = trial.suggest_int(\"fusion_dim\", 64, 2048)\n",
    "    config[\"model\"][\"dropout_encoders\"] = trial.suggest_float(\"dropout_encoders\", 0.0, 0.5)\n",
    "    config[\"model\"][\"pre_training_epochs\"] = trial.suggest_int(\"pre_training_epochs\", 1, 500)\n",
    "    config[\"optimizer\"][\"pre_batch_size\"] = trial.suggest_int(\"pre_batch_size\", 128, 512)\n",
    "    config[\"optimizer\"][\"lr_pretraining\"] = trial.suggest_float(\"lr_pretraining\", 1e-6, 1e-1, log=True)\n",
    "\n",
    "    ### 2️⃣ Pre-training 실행 ###\n",
    "    try:\n",
    "        # Train + Validation 데이터 합쳐서 Pre-training 진행\n",
    "        pre_trained_autoencoder, pre_training_losses = autoencoder_train_step(\n",
    "            config, train_dataset, input_dims=input_dims\n",
    "        )\n",
    "\n",
    "        # Pre-training 동안의 최소 loss 반환 (낮을수록 좋음)\n",
    "        pre_training_score = min(pre_training_losses)\n",
    "        print(f\"Trial {trial.number}: Pre-training Loss = {pre_training_score:.5f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Pre-training Error: {e}\")\n",
    "        return float(\"inf\")  # Pre-training 실패 시, 최대 loss 반환\n",
    "\n",
    "    ### 3️⃣ Fine-tuning 하이퍼파라미터 샘플링 ###\n",
    "    config[\"model\"][\"dropout_omics\"] = trial.suggest_float(\"dropout_omics\", 0.0, 0.9)\n",
    "    config[\"model\"][\"dropout_omics_finetuning\"] = trial.suggest_float(\"dropout_omics_finetuning\", 0.0, 0.9)\n",
    "    config[\"optimizer\"][\"learning_rate\"] = trial.suggest_float(\"learning_rate\", 1e-6, 1e-1, log=True)\n",
    "    config[\"optimizer\"][\"batch_size\"] = trial.suggest_int(\"batch_size\", 64, 512)\n",
    "\n",
    "    ### 4️⃣ Fine-tuning 실행 (Latent Representation 생성 포함) ###\n",
    "    try:\n",
    "        val_target, fine_tuned_resnet = latent_resnet_training(\n",
    "            config, train_dataset, pre_trained_autoencoder, val_dataset\n",
    "        )\n",
    "\n",
    "        print(f\"Trial {trial.number}: Fine-tuning R_cellwise_residuals = {val_target:.5f}\")\n",
    "        return val_target  # Fine-tuning 성능 반환 (높을수록 좋음)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fine-tuning Error: {e}\")\n",
    "        return -float(\"inf\")  # Fine-tuning 실패 시, 최소 성능 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f8e12-d0e9-4395-90a2-bd181ae56cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"env\"][\"search_hyperparameters\"]:\n",
    "    study_name = \"optimized_model\"\n",
    "    storage_name = f\"sqlite:///studies/{study_name}.db\"\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=storage_name,\n",
    "        direction='maximize',  # Fine-tuning 성능을 최적화 (R_cellwise_residuals 최대화)\n",
    "        load_if_exists=True,\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=30, n_warmup_steps=5, interval_steps=5)\n",
    "    )\n",
    "\n",
    "    study.optimize(lambda trial: train_model_optuna(trial, config), n_trials=40)\n",
    "\n",
    "    best_config = study.best_params\n",
    "    print(\"Best Hyperparameters:\", best_config)\n",
    "\n",
    "    # 최적의 하이퍼파라미터를 config에 반영\n",
    "    config[\"model\"][\"embed_dim\"] = best_config[\"embed_dim\"]\n",
    "    config[\"model\"][\"hidden_dim_encoders\"] = best_config[\"hidden_dim_encoders\"]\n",
    "    config[\"model\"][\"fusion_dim\"] = best_config[\"fusion_dim\"]\n",
    "    config[\"model\"][\"dropout_encoders\"] = best_config[\"dropout_encoders\"]\n",
    "    config[\"model\"][\"pre_training_epochs\"] = best_config[\"pre_training_epochs\"]\n",
    "    config[\"optimizer\"][\"pre_batch_size\"] = best_config[\"pre_batch_size\"]\n",
    "    config[\"optimizer\"][\"lr_pretraining\"] = best_config[\"lr_pretraining\"]\n",
    "    config[\"model\"][\"dropout_omics\"] = best_config[\"dropout_omics\"]\n",
    "    config[\"model\"][\"dropout_omics_finetuning\"] = best_config[\"dropout_omics_finetuning\"]\n",
    "    config[\"optimizer\"][\"learning_rate\"] = best_config[\"learning_rate\"]\n",
    "    config[\"optimizer\"][\"batch_size\"] = best_config[\"batch_size\"]\n",
    "\n",
    "# 2️⃣ 최적 하이퍼파라미터를 적용한 후, Training (Train + Validation 합쳐서 사용)\n",
    "train_val_dataset = torch.utils.data.ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "# Pre-training 실행\n",
    "pre_trained_autoencoder, pre_training_losses = autoencoder_train_step(config, train_val_dataset, input_dims=input_dims)\n",
    "\n",
    "# Fine-tuning 실행 (이제 Train + Validation을 합쳐서 사용)\n",
    "fine_tuned_resnet, final_target = latent_resnet_training(\n",
    "    config, train_val_dataset, autoencoder, pre_trained_autoencoder, validation_dataset=None  # ✅ Validation 없이 학습\n",
    ")\n",
    "\n",
    "print(f\"Final Training Complete! Final R_cellwise_residuals: {final_target:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62307b7-6612-43b1-a2b6-8cc7d48c9f02",
   "metadata": {},
   "source": [
    "# Model training \n",
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af043824-988a-437e-8b6b-df4e9133ce0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[20:53:47] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "# data loading with raw data\n",
    "train_dataset, val_dataset, test_dataset, input_dims= get_data_corr(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"], \n",
    "                                                           transform_into_corr = True,\n",
    "                                                           typ = (\"rnaseq\", \"proteomics\", \"mutations\", \"methylations\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f544e44c-523b-4ee0-99a5-ea8ec63e789f",
   "metadata": {},
   "source": [
    "여기서 Autoencoder의 evaluation은 수행하지 않는다. Autoencoder를 단독으로 평가하는 것이 Test distribution과 맞지 않을 수 있기 때문이다. 여기서 이걸 먼저 평가하게 되면 Test set의 정보가 노출되어 편향된 결과가 나타날 수 있다. \n",
    "\n",
    "또한, 단순 Reconstruction Loss로 이를 중간에 평가하는 것이 애초에 의미가 없을 가능성이 높다. \n",
    "\n",
    "그렇다면, 두 모델을 따로 트레이닝 할 것인지, 한번에 트레이닝 할 것인지 정해야겠다. \n",
    "\n",
    "1. Autoencoder pretraining -> encoder frozen, used as feature extractor -> Resnet training\n",
    "   - Autoencoder가 단순 Reconstruction Loss로 학습되므로, ResNet에 필요한 Feature를 충분히 학습하지 못할 가능성이 있음.\n",
    "3. Autoencoder -> latent feature -> Resnet\n",
    "   - 설계가 복잡해짐.\n",
    "   - 두 모델을 한번에 트레이닝하므로, 학습이 불안정할 수 있음. \n",
    "\n",
    "\n",
    "둘다 해볼건데, 여기서는 1번을 할거다. 간략적인 플로우는 다음과 같다.\n",
    "\n",
    "- Train + Val 데이터로 Autoencoder 학습\n",
    "- Decoder는 사용하지만, 나중에는 Encoder만 저장\n",
    "- MSE Loss로 Reconstruction 학습\n",
    "- Autoencoder를 저장해서 이후 ResNet에서 사용 가능하도록 함. latent feature 생성 목적\n",
    "- Autoencoder를 따로 평가하지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84584108-f676-4d60-98c1-adccbc220f0a",
   "metadata": {},
   "source": [
    "## Autoencoder training\n",
    "\n",
    "여기서는 Autoencoder와 Resnet을 한 시스템으로 묶어버리도록 하겠다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c552b7a5-c3ad-4c21-b221-88d772f67552",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.00676\n",
      "Epoch 2/100, Train Loss: 0.00160\n",
      "Epoch 3/100, Train Loss: 0.00123\n",
      "Epoch 4/100, Train Loss: 0.00102\n",
      "Epoch 5/100, Train Loss: 0.00090\n",
      "Epoch 6/100, Train Loss: 0.00082\n",
      "Epoch 7/100, Train Loss: 0.00075\n",
      "Epoch 8/100, Train Loss: 0.00070\n",
      "Epoch 9/100, Train Loss: 0.00065\n",
      "Epoch 10/100, Train Loss: 0.00061\n",
      "Epoch 11/100, Train Loss: 0.00057\n",
      "Epoch 12/100, Train Loss: 0.00054\n",
      "Epoch 13/100, Train Loss: 0.00051\n",
      "Epoch 14/100, Train Loss: 0.00049\n",
      "Epoch 15/100, Train Loss: 0.00047\n",
      "Epoch 16/100, Train Loss: 0.00045\n",
      "Epoch 17/100, Train Loss: 0.00043\n",
      "Epoch 18/100, Train Loss: 0.00042\n",
      "Epoch 19/100, Train Loss: 0.00041\n",
      "Epoch 20/100, Train Loss: 0.00039\n",
      "Epoch 21/100, Train Loss: 0.00039\n",
      "Epoch 22/100, Train Loss: 0.00038\n",
      "Epoch 23/100, Train Loss: 0.00037\n",
      "Epoch 24/100, Train Loss: 0.00037\n",
      "Epoch 25/100, Train Loss: 0.00036\n",
      "Epoch 26/100, Train Loss: 0.00036\n",
      "Epoch 27/100, Train Loss: 0.00035\n",
      "Epoch 28/100, Train Loss: 0.00035\n",
      "Epoch 29/100, Train Loss: 0.00035\n",
      "Epoch 30/100, Train Loss: 0.00035\n",
      "Epoch 31/100, Train Loss: 0.00034\n",
      "Epoch 32/100, Train Loss: 0.00034\n",
      "Epoch 33/100, Train Loss: 0.00034\n",
      "Epoch 34/100, Train Loss: 0.00034\n",
      "Epoch 35/100, Train Loss: 0.00034\n",
      "Epoch 36/100, Train Loss: 0.00033\n",
      "Epoch 37/100, Train Loss: 0.00033\n",
      "Epoch 38/100, Train Loss: 0.00033\n",
      "Epoch 39/100, Train Loss: 0.00033\n",
      "Epoch 40/100, Train Loss: 0.00033\n",
      "Epoch 41/100, Train Loss: 0.00033\n",
      "Epoch 42/100, Train Loss: 0.00033\n",
      "Epoch 43/100, Train Loss: 0.00032\n",
      "Epoch 44/100, Train Loss: 0.00032\n",
      "Epoch 45/100, Train Loss: 0.00032\n",
      "Epoch 46/100, Train Loss: 0.00032\n",
      "Epoch 47/100, Train Loss: 0.00032\n",
      "Epoch 48/100, Train Loss: 0.00032\n",
      "Epoch 49/100, Train Loss: 0.00032\n",
      "Epoch 50/100, Train Loss: 0.00031\n",
      "Epoch 51/100, Train Loss: 0.00031\n",
      "Epoch 52/100, Train Loss: 0.00031\n",
      "Epoch 53/100, Train Loss: 0.00031\n",
      "Epoch 54/100, Train Loss: 0.00031\n",
      "Epoch 55/100, Train Loss: 0.00031\n",
      "Epoch 56/100, Train Loss: 0.00031\n",
      "Epoch 57/100, Train Loss: 0.00030\n",
      "Epoch 58/100, Train Loss: 0.00030\n",
      "Epoch 59/100, Train Loss: 0.00030\n",
      "Epoch 60/100, Train Loss: 0.00030\n",
      "Epoch 61/100, Train Loss: 0.00030\n",
      "Epoch 62/100, Train Loss: 0.00030\n",
      "Epoch 63/100, Train Loss: 0.00030\n",
      "Epoch 64/100, Train Loss: 0.00030\n",
      "Epoch 65/100, Train Loss: 0.00029\n",
      "Epoch 66/100, Train Loss: 0.00029\n",
      "Epoch 67/100, Train Loss: 0.00029\n",
      "Epoch 68/100, Train Loss: 0.00029\n",
      "Epoch 69/100, Train Loss: 0.00029\n",
      "Epoch 70/100, Train Loss: 0.00029\n",
      "Epoch 71/100, Train Loss: 0.00029\n",
      "Epoch 72/100, Train Loss: 0.00029\n",
      "Epoch 73/100, Train Loss: 0.00028\n",
      "Epoch 74/100, Train Loss: 0.00028\n",
      "Epoch 75/100, Train Loss: 0.00028\n",
      "Epoch 76/100, Train Loss: 0.00028\n",
      "Epoch 77/100, Train Loss: 0.00028\n",
      "Epoch 78/100, Train Loss: 0.00028\n",
      "Epoch 79/100, Train Loss: 0.00028\n",
      "Epoch 80/100, Train Loss: 0.00028\n",
      "Epoch 81/100, Train Loss: 0.00027\n",
      "Epoch 82/100, Train Loss: 0.00027\n",
      "Epoch 83/100, Train Loss: 0.00027\n",
      "Epoch 84/100, Train Loss: 0.00027\n",
      "Epoch 85/100, Train Loss: 0.00027\n",
      "Epoch 86/100, Train Loss: 0.00027\n",
      "Epoch 87/100, Train Loss: 0.00027\n",
      "Epoch 88/100, Train Loss: 0.00026\n",
      "Epoch 89/100, Train Loss: 0.00026\n",
      "Epoch 90/100, Train Loss: 0.00026\n",
      "Epoch 91/100, Train Loss: 0.00026\n",
      "Epoch 92/100, Train Loss: 0.00026\n",
      "Epoch 93/100, Train Loss: 0.00026\n",
      "Epoch 94/100, Train Loss: 0.00026\n",
      "Epoch 95/100, Train Loss: 0.00026\n",
      "Epoch 96/100, Train Loss: 0.00026\n",
      "Epoch 97/100, Train Loss: 0.00026\n",
      "Epoch 98/100, Train Loss: 0.00026\n",
      "Epoch 99/100, Train Loss: 0.00025\n",
      "Epoch 100/100, Train Loss: 0.00025\n",
      "Pre-training complete!\n"
     ]
    }
   ],
   "source": [
    "# pre training with raw data! final version. \n",
    "autoencoder, autoencoder_losses = autoencoder_train_step(config, torch.utils.data.ConcatDataset([train_dataset, val_dataset]), input_dims = input_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f757810-67d7-4662-9fed-39f7ef3f5a9a",
   "metadata": {},
   "source": [
    "## Resnet training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48c261b3-0155-49d6-9486-ecb626421232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lesgo\n",
      "epoch : 0: train loss: 2.183289214471362 Smoothed R interaction (validation) None\n",
      "epoch : 1: train loss: 1.8301144769076807 Smoothed R interaction (validation) None\n",
      "epoch : 2: train loss: 1.7184799908118136 Smoothed R interaction (validation) None\n",
      "epoch : 3: train loss: 1.6683552845741527 Smoothed R interaction (validation) None\n",
      "epoch : 4: train loss: 1.6316558051905026 Smoothed R interaction (validation) None\n",
      "epoch : 5: train loss: 1.6059475608160823 Smoothed R interaction (validation) None\n",
      "epoch : 6: train loss: 1.5791467571140663 Smoothed R interaction (validation) None\n",
      "epoch : 7: train loss: 1.5496696347948646 Smoothed R interaction (validation) None\n",
      "epoch : 8: train loss: 1.523333319406427 Smoothed R interaction (validation) None\n",
      "epoch : 9: train loss: 1.5053276323122795 Smoothed R interaction (validation) None\n",
      "epoch : 10: train loss: 1.4783500055005436 Smoothed R interaction (validation) None\n",
      "epoch : 11: train loss: 1.466653089588151 Smoothed R interaction (validation) None\n",
      "epoch : 12: train loss: 1.4556489752603254 Smoothed R interaction (validation) None\n",
      "epoch : 13: train loss: 1.4458623770877663 Smoothed R interaction (validation) None\n",
      "epoch : 14: train loss: 1.4366260395651254 Smoothed R interaction (validation) None\n",
      "epoch : 15: train loss: 1.4270715909481637 Smoothed R interaction (validation) None\n",
      "epoch : 16: train loss: 1.4185381878733783 Smoothed R interaction (validation) None\n",
      "epoch : 17: train loss: 1.4103690956078 Smoothed R interaction (validation) None\n",
      "epoch : 18: train loss: 1.4025575270316804 Smoothed R interaction (validation) None\n",
      "epoch : 19: train loss: 1.403130123582847 Smoothed R interaction (validation) None\n",
      "epoch : 20: train loss: 1.3929543384662812 Smoothed R interaction (validation) None\n",
      "epoch : 21: train loss: 1.3843723949602125 Smoothed R interaction (validation) None\n",
      "epoch : 22: train loss: 1.3768855337748864 Smoothed R interaction (validation) None\n",
      "epoch : 23: train loss: 1.3723289027939032 Smoothed R interaction (validation) None\n",
      "epoch : 24: train loss: 1.367770897222243 Smoothed R interaction (validation) None\n",
      "epoch : 25: train loss: 1.3592506921924974 Smoothed R interaction (validation) None\n",
      "epoch : 26: train loss: 1.349388907218009 Smoothed R interaction (validation) None\n",
      "epoch : 27: train loss: 1.3502413850042818 Smoothed R interaction (validation) None\n",
      "epoch : 28: train loss: 1.3427880546365887 Smoothed R interaction (validation) None\n",
      "epoch : 29: train loss: 1.3352127055302538 Smoothed R interaction (validation) None\n",
      "epoch : 30: train loss: 1.326186543489416 Smoothed R interaction (validation) None\n",
      "epoch : 31: train loss: 1.3234608574172917 Smoothed R interaction (validation) None\n",
      "epoch : 32: train loss: 1.3193261593911791 Smoothed R interaction (validation) None\n",
      "epoch : 33: train loss: 1.308489282582098 Smoothed R interaction (validation) None\n",
      "epoch : 34: train loss: 1.3039433788162815 Smoothed R interaction (validation) None\n",
      "epoch : 35: train loss: 1.2988460658507235 Smoothed R interaction (validation) None\n",
      "epoch : 36: train loss: 1.2972193992476824 Smoothed R interaction (validation) None\n",
      "epoch : 37: train loss: 1.2903745199458119 Smoothed R interaction (validation) None\n",
      "epoch : 38: train loss: 1.282873005890581 Smoothed R interaction (validation) None\n",
      "epoch : 39: train loss: 1.278977984845712 Smoothed R interaction (validation) None\n",
      "epoch : 40: train loss: 1.273663030419273 Smoothed R interaction (validation) None\n",
      "epoch : 41: train loss: 1.2699498956371444 Smoothed R interaction (validation) None\n",
      "epoch : 42: train loss: 1.2618377557938414 Smoothed R interaction (validation) None\n",
      "epoch : 43: train loss: 1.2617686863145958 Smoothed R interaction (validation) None\n",
      "epoch : 44: train loss: 1.2570045751016425 Smoothed R interaction (validation) None\n",
      "epoch : 45: train loss: 1.2500971970511012 Smoothed R interaction (validation) None\n",
      "epoch : 46: train loss: 1.246846031567959 Smoothed R interaction (validation) None\n",
      "epoch : 47: train loss: 1.243267052989955 Smoothed R interaction (validation) None\n",
      "epoch : 48: train loss: 1.2401338864315867 Smoothed R interaction (validation) None\n",
      "epoch : 49: train loss: 1.2339074439996547 Smoothed R interaction (validation) None\n",
      "epoch : 50: train loss: 1.2322448878117338 Smoothed R interaction (validation) None\n",
      "epoch : 51: train loss: 1.2305807544215501 Smoothed R interaction (validation) None\n",
      "epoch : 52: train loss: 1.2220490213082953 Smoothed R interaction (validation) None\n",
      "epoch : 53: train loss: 1.2181038857243116 Smoothed R interaction (validation) None\n",
      "epoch : 54: train loss: 1.2128207002051534 Smoothed R interaction (validation) None\n",
      "epoch : 55: train loss: 1.214096785844183 Smoothed R interaction (validation) None\n",
      "epoch : 56: train loss: 1.207687819770299 Smoothed R interaction (validation) None\n",
      "epoch : 57: train loss: 1.208907359904795 Smoothed R interaction (validation) None\n",
      "epoch : 58: train loss: 1.197946592889109 Smoothed R interaction (validation) None\n",
      "epoch : 59: train loss: 1.193052493671257 Smoothed R interaction (validation) None\n",
      "epoch : 60: train loss: 1.1892024629342983 Smoothed R interaction (validation) None\n",
      "epoch : 61: train loss: 1.1868651640429928 Smoothed R interaction (validation) None\n",
      "epoch : 62: train loss: 1.1825757169605629 Smoothed R interaction (validation) None\n",
      "epoch : 63: train loss: 1.1792899543334292 Smoothed R interaction (validation) None\n",
      "epoch : 64: train loss: 1.1729799148031455 Smoothed R interaction (validation) None\n",
      "epoch : 65: train loss: 1.1711199088355961 Smoothed R interaction (validation) None\n",
      "epoch : 66: train loss: 1.1652611655416831 Smoothed R interaction (validation) None\n",
      "epoch : 67: train loss: 1.1634713156408962 Smoothed R interaction (validation) None\n",
      "epoch : 68: train loss: 1.1591797636819563 Smoothed R interaction (validation) None\n",
      "epoch : 69: train loss: 1.1546981337633357 Smoothed R interaction (validation) None\n",
      "epoch : 70: train loss: 1.1525072759691952 Smoothed R interaction (validation) None\n",
      "epoch : 71: train loss: 1.1500948646159932 Smoothed R interaction (validation) None\n",
      "epoch : 72: train loss: 1.1431587385600812 Smoothed R interaction (validation) None\n",
      "epoch : 73: train loss: 1.1405860437598305 Smoothed R interaction (validation) None\n",
      "epoch : 74: train loss: 1.1388952689353555 Smoothed R interaction (validation) None\n",
      "epoch : 75: train loss: 1.1352622105253052 Smoothed R interaction (validation) None\n",
      "epoch : 76: train loss: 1.1359682026398639 Smoothed R interaction (validation) None\n",
      "epoch : 77: train loss: 1.128546334978087 Smoothed R interaction (validation) None\n",
      "epoch : 78: train loss: 1.126283148457889 Smoothed R interaction (validation) None\n",
      "epoch : 79: train loss: 1.1214291490199981 Smoothed R interaction (validation) None\n",
      "epoch : 80: train loss: 1.1156560262879276 Smoothed R interaction (validation) None\n",
      "epoch : 81: train loss: 1.1162466678690115 Smoothed R interaction (validation) None\n",
      "epoch : 82: train loss: 1.118887640858168 Smoothed R interaction (validation) None\n",
      "epoch : 83: train loss: 1.1079052671219127 Smoothed R interaction (validation) None\n",
      "epoch : 84: train loss: 1.1052681836857932 Smoothed R interaction (validation) None\n",
      "epoch : 85: train loss: 1.1088874736439311 Smoothed R interaction (validation) None\n",
      "epoch : 86: train loss: 1.1036952626425198 Smoothed R interaction (validation) None\n",
      "epoch : 87: train loss: 1.102098022977855 Smoothed R interaction (validation) None\n",
      "epoch : 88: train loss: 1.1017715776217145 Smoothed R interaction (validation) None\n",
      "epoch : 89: train loss: 1.103007692931167 Smoothed R interaction (validation) None\n",
      "epoch : 90: train loss: 1.0919622447346284 Smoothed R interaction (validation) None\n",
      "epoch : 91: train loss: 1.093901166515091 Smoothed R interaction (validation) None\n",
      "epoch : 92: train loss: 1.0895167299637247 Smoothed R interaction (validation) None\n",
      "epoch : 93: train loss: 1.0817557235878095 Smoothed R interaction (validation) None\n",
      "epoch : 94: train loss: 1.0840682013073133 Smoothed R interaction (validation) None\n",
      "epoch : 95: train loss: 1.083293436263194 Smoothed R interaction (validation) None\n",
      "epoch : 96: train loss: 1.0811615485343415 Smoothed R interaction (validation) None\n",
      "epoch : 97: train loss: 1.0800713358172969 Smoothed R interaction (validation) None\n",
      "epoch : 98: train loss: 1.0766774865399185 Smoothed R interaction (validation) None\n",
      "epoch : 99: train loss: 1.070740673097014 Smoothed R interaction (validation) None\n"
     ]
    }
   ],
   "source": [
    "#weight_path_raw = \"trained_models/pretrained_raw_omics.pth\"\n",
    "_, Resnet = latent_resnet_training(\n",
    "                             config,\n",
    "                             torch.utils.data.ConcatDataset([train_dataset, val_dataset]), \n",
    "                             autoencoder = autoencoder,\n",
    "                             use_momentum=False)\n",
    "metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(\n",
    "    {\"R_cellwise_residuals\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"drugs\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=True),\n",
    "    \"R_cellwise\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"cell_lines\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=False),\n",
    "    \"MSE\":torchmetrics.MeanSquaredError()}))\n",
    "device = torch.device(config[\"env\"][\"device\"])\n",
    "metrics.to(device)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                       batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                       drop_last=False,\n",
    "                                      shuffle=False,\n",
    "                                      pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2949c0b-2267-4732-a146-4ea178117fdf",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "트레이닝된 Autoencoder의 encoder와, Resnet을 하나의 시스템으로 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f5d20ff-ce7c-414f-8aa2-f36ab25ced43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to: results/pred_autoencoder_resnet_septrain_raw_data_20250310_23:02:57_3b363c2f-e5bc-44cb-94b1-52d32211b7ce.csv\n",
      "main model final metrics: {'MSE': 2.2382700443267822, 'R_cellwise': 0.8708129525184631, 'R_cellwise_residuals': -0.02552889473736286}\n"
     ]
    }
   ],
   "source": [
    "autoencoder_resnet_septrain = evaluate_step(Resnet, autoencoder, test_dataloader, metrics, device, save_predictions = True, model_name = \"autoencoder_resnet_septrain\", dataset_name = \"raw_data\")\n",
    "print(f\"main model final metrics: {autoencoder_resnet_septrain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c23cc-7ddc-43fc-9fb3-449a376726c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
